# Estimation basics {#eb}

This chapter deals with estimation basics.

The students are expected to acquire the following knowledge:

- Biased and unbiased estimators.
- Consistent estimators.
- Empirical cumulative distribution function.

<style>
.fold-btn { 
  float: right; 
  margin: 5px 5px 0 0;
}
.fold { 
  border: 1px solid black;
  min-height: 40px;
}
</style>

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold" ? "Unfold" : "Fold");  // if the text equals "Fold", change it to "Unfold"or else to "Fold" 
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>

```{r, echo = FALSE, warning = FALSE, message = FALSE}
togs <- T
library(ggplot2)
library(dplyr)
library(reshape2)
library(tidyr)
# togs <- FALSE
```

## ECDF
```{exercise, name = "ECDF intuition"}


a. Take any univariate continuous distribution that is readily available in R and plot its CDF ($F$).
b. Draw one sample ($n = 1$) from the chosen distribution and draw the ECDF ($F_n$) of that one sample. Use the definition of the ECDF, not an existing function in R. Implementation hint: ECDFs are always piecewise constant - they only jump at the sampled values and by $1/n$. 
c. Repeat (b) for $n = 5, 10, 100, 1000...$ Theory says that $F_n$ should converge to $F$. Can you observe that?
d. For $n = 100$ repeat the process $m = 20$ times and plot every $F_n^{(m)}$. Theory says that $F_n$ will converge to $F$ the slowest where $F$ is close to 0.5 (where the variance is largest). Can you observe that?


```
<div class = "fold">
```{r, echo = togs, eval = togs, message = FALSE, warning=FALSE}
library(ggplot2)
set.seed(1)
ggplot(data = data.frame(x = seq(-5, 5, by = 0.01))) +
  # stat_function(aes(x = x), fun = pbeta, args = list(shape1 = 1, shape2 = 2))
  stat_function(aes(x = x), fun = pnorm, args = list(mean = 0, sd = 1))

one_samp <- rnorm(1)
X        <- data.frame(x = c(-5, one_samp, 5), y = c(0,1,1))
ggplot(data = data.frame(x = seq(-5, 5, by = 0.01))) +
  # stat_function(aes(x = x), fun = pbeta, args = list(shape1 = 1, shape2 = 2))
  stat_function(aes(x = x), fun = pnorm, args = list(mean = 0, sd = 1)) +
  geom_step(data = X, aes(x = x, y = y))

N <- c(5, 10, 100, 1000)
X <- NULL
for (n in N) {
  tmp   <- rnorm(n)
  tmp_X <- data.frame(x = c(-5, sort(tmp), 5), y = c(0, seq(1/n, 1, by = 1/n), 1), n = n)
  X     <- rbind(X, tmp_X)
}
ggplot(data = data.frame(x = seq(-5, 5, by = 0.01))) +
  # stat_function(aes(x = x), fun = pbeta, args = list(shape1 = 1, shape2 = 2))
  stat_function(aes(x = x), fun = pnorm, args = list(mean = 0, sd = 1)) +
  geom_step(data = X, aes(x = x, y = y, color = as.factor(n))) +
  labs(color = "N")
```
</div>


## Properties of estimators
```{exercise}
Show that the sample average is, as an estimator of the mean:

a. unbiased,
b. consistent,
c. asymptotically normal.



```
<div class="fold">
```{solution, echo = togs}


a.
\begin{align*}
E[\frac{1}{n} \sum_{i=1}^n X_i] &= \frac{1}{n} \sum_{i=i}^n E[X_i] \\
                                &= E[X].
\end{align*}
b.
\begin{align*}
  \lim_{n \rightarrow \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - E[X]| > \epsilon) &= \lim_{n \rightarrow \infty} P((\frac{1}{n} \sum_{i=1}^n X_i - E[X])^2 > \epsilon^2) \\
  & \leq \lim_{n \rightarrow \infty} \frac{E[(\frac{1}{n} \sum_{i=1}^n X_i - E[X])^2]}{\epsilon^2} & \text{Markov inequality} \\
  & = \lim_{n \rightarrow \infty} \frac{E[(\frac{1}{n} \sum_{i=1}^n X_i)^2 - 2 \frac{1}{n} \sum_{i=1}^n X_i E[X] + E[X]^2]}{\epsilon^2} \\
  & = \lim_{n \rightarrow \infty} \frac{E[(\frac{1}{n} \sum_{i=1}^n X_i)^2] - 2 E[X] + E[X]^2}{\epsilon^2} \\
  &= 0
\end{align*}
For the last equality see the solution to \@ref(exr:vardecomp).

c. Follows directly from the CLT.

```
</div>



```{exercise, name = "Consistent but biased estimator", label = "cbest"}


a. Show that sample variance (the plug-in estimator of variance) is a biased estimator of variance.
b. Show that sample variance is a consistent estimator of variance.
c. Show that the estimator with ($N-1$) (Bessel correction) is unbiased.




```
<div class="fold">
```{solution, echo = togs}


a.
\begin{align*}
  E[\frac{1}{n} \sum_{i=1}^n (Y_i - \bar{Y})^2] &= \frac{1}{n} \sum_{i=1}^n E[(Y_i - \bar{Y})^2] \\
  &= \frac{1}{n} \sum_{i=1}^n E[Y_i^2] - 2 E[Y_i \bar{Y}] + \bar{Y}^2)] \\
  &= \frac{1}{n} \sum_{i=1}^n E[Y_i^2 - 2 Y_i \bar{Y} + \bar{Y}^2] \\
  &= \frac{1}{n} \sum_{i=1}^n E[Y_i^2 - \frac{2}{n} Y_i^2 - \frac{2}{n} \sum_{i \neq j} Y_i Y_j  + \frac{1}{n^2}\sum_j \sum_{k \neq j} Y_j Y_k + \frac{1}{n^2} \sum_j Y_j^2] \\
  &= \frac{1}{n} \sum_{i=1}^n \frac{n - 2}{n} (\sigma^2 - \mu^2) - \frac{2}{n} (n - 1) \mu^2 + \frac{1}{n^2}n(n-1)\mu^2 + \frac{1}{n^2}(\sigma^2 + \mu^2) \\
  &= \frac{n-1}{n}\sigma^2 \\
  < \sigma^2.
\end{align*}
b. Let $S_n$ denote the sample variance. Then we can write it as
\begin{align*}
  S_n &= \frac{1}{n} \sum_{i=1}^n (X_i - \bar{X})^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 + 2(X_i - \mu)(\mu - \bar{X}) + (\mu - \bar{X})^2 \\
  &= \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 - (\bar{X} - \mu)^2.
\end{align*}
Now $\bar{X}$ converges in probability (by WLLN) to $\mu$ therefore the right term converges in probability to zero. The left term converges in probability to $\sigma^2$, also by WLLN. Therefore the sample variance is a consistent estimatior of the variance.

c. The denominator changes in the second-to-last line of a., therefore the last line is now equality.

```
</div>


```{exercise, name = "Estimating the median"}


a. Show that the sample median is an unbiased estimator of the median for N$(\mu, \sigma^2)$.
b. Show that the sample median is an unbiased estimator of the mean for any distribution with symmetric density.





```
<div class="fold">
```{solution, echo = togs}


b. Let $Z_i$, $i = 1,...,n$ be i.i.d. variables with a symmetric distribution and let $Z_{k:n}$ denote the $k$-th order statistic. We will distinguish two cases, when $n$ is odd and when $n$ is even. Let first $n = 2m + 1$ be odd. Then the sample median is $M = Z_{m+1:2m+1}$. Its PDF is
\begin{align*}
  f_M(x) = (m+1)\binom{2m + 1}{m}f_Z(x)\Big(F_Z(x)^m (1 - F_Z(x)^m) \Big).
\end{align*}
For every symmetric distribution, it holds that $F_X(x) = 1 - F(2a - x)$ (CHECK THIS). Let $a = \mu$, the population mean. Plugging this into the PDF, we get that $f_M(x) = f_M(2\mu -x)$. It follows that
\begin{align*}
  E[M] &= E[2\mu - M] \\
  2E[M] &= 2\mu \\
  E[M] &= \mu.
\end{align*}
Now let $n = 2m$ be even. Then the sample median is $M = \frac{Z_{m:2m} + Z_{m+1:2m}}{2}$. It can be shown, that the joint PDF of these terms is also symmetric. Therefore, similar to the above
\begin{align*}
  E[M] &= E[\frac{Z_{m:2m} + Z_{m+1:2m}}{2}] \\
       &= E[\frac{2\mu - Z_{m:2m} + 2\mu - Z_{m+1:2m}}{2}] \\
       &= E[2\mu - M].
\end{align*}
The above also proves point a. as the median and the mean are the same in normal distribution.

```
</div>
