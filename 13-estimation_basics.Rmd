# Estimation basics {#eb}

This chapter deals with estimation basics.

The students are expected to acquire the following knowledge:

<style>
.fold-btn { 
  float: right; 
  margin: 5px 5px 0 0;
}
.fold { 
  border: 1px solid black;
  min-height: 40px;
}
</style>

<script type="text/javascript">
$(document).ready(function() {
  $folds = $(".fold");
  $folds.wrapInner("<div class=\"fold-blck\">"); // wrap a div container around content
  $folds.prepend("<button class=\"fold-btn\">Unfold</button>");  // add a button
  $(".fold-blck").toggle();  // fold all blocks
  $(".fold-btn").on("click", function() {  // add onClick event
    $(this).text($(this).text() === "Fold" ? "Unfold" : "Fold");  // if the text equals "Fold", change it to "Unfold"or else to "Fold" 
    $(this).next(".fold-blck").toggle("linear");  // "swing" is the default easing function. This can be further customized in its speed or the overall animation itself.
  })
});
</script>

```{r, echo = FALSE, warning = FALSE, message = FALSE}
togs <- T
library(ggplot2)
library(dplyr)
library(reshape2)
library(tidyr)
# togs <- FALSE
```

## ECDF
```{exercise, name = "ECDF intuition"}


a. Take any univariate continuous distribution that is readily available in R and plot its CDF ($F$).
b. Draw one sample ($n = 1$) from the chosen distribution and draw the ECDF ($F_n$) of that one sample. Use the definition of the ECDF, not an existing function in R. Implementation hint: ECDFs are always piecewise constant - they only jump at the sampled values and by $1/n$. 
c. Repeat (b) for $n = 5, 10, 100, 1000...$ Theory says that $F_n$ should converge to $F$. Can you observe that?
d. For $n = 100$ repeat the process $m = 20$ times and plot every $F_n^{(m)}$. Theory says that $F_n$ will converge to $F$ the slowest where $F$ is close to 0.5 (where the variance is largest). Can you observe that?


```
<div class = "fold">
```{r, echo = togs, message = FALSE, warning=FALSE}
library(ggplot2)
set.seed(1)
ggplot(data = data.frame(x = seq(-5, 5, by = 0.01))) +
  # stat_function(aes(x = x), fun = pbeta, args = list(shape1 = 1, shape2 = 2))
  stat_function(aes(x = x), fun = pnorm, args = list(mean = 0, sd = 1))

one_samp <- rnorm(1)
X        <- data.frame(x = c(-5, one_samp, 5), y = c(0,1,1))
ggplot(data = data.frame(x = seq(-5, 5, by = 0.01))) +
  # stat_function(aes(x = x), fun = pbeta, args = list(shape1 = 1, shape2 = 2))
  stat_function(aes(x = x), fun = pnorm, args = list(mean = 0, sd = 1)) +
  geom_step(data = X, aes(x = x, y = y))

N <- c(5, 10, 100, 1000)
X <- NULL
for (n in N) {
  tmp   <- rnorm(n)
  tmp_X <- data.frame(x = c(-5, sort(tmp), 5), y = c(0, seq(1/n, 1, by = 1/n), 1), n = n)
  X     <- rbind(X, tmp_X)
}
ggplot(data = data.frame(x = seq(-5, 5, by = 0.01))) +
  # stat_function(aes(x = x), fun = pbeta, args = list(shape1 = 1, shape2 = 2))
  stat_function(aes(x = x), fun = pnorm, args = list(mean = 0, sd = 1)) +
  geom_step(data = X, aes(x = x, y = y, color = as.factor(n))) +
  labs(color = "N")
```
</div>


## Properties of estimators
```{exercise}
Show that the sample average is, as an estimator of the mean:

a. unbiased,
b. consistent,
c. asymptotically normal.



```
<div class="fold">
```{solution, echo = togs}


a.
\begin{align*}
E[\frac{1}{n} \sum_{i=1}^n X_i] &= \frac{1}{n} \sum_{i=i}^n E[X_i] \\
                                &= E[X].
\end{align*}
b.
\begin{align*}
  \lim_{n \rightarrow \infty} P(|\frac{1}{n} \sum_{i=1}^n X_i - E[X]| > \epsilon) &= \lim_{n \rightarrow \infty} P((\frac{1}{n} \sum_{i=1}^n X_i - E[X])^2 > \epsilon^2) \\
  & \leq \lim_{n \rightarrow \infty} \frac{E[(\frac{1}{n} \sum_{i=1}^n X_i - E[X])^2]}{\epsilon^2} & \text{Markov inequality} \\
  & = \lim_{n \rightarrow \infty} \frac{E[(\frac{1}{n} \sum_{i=1}^n X_i)^2 - 2 \frac{1}{n} \sum_{i=1}^n X_i E[X] + E[X]^2]}{\epsilon^2} \\
  & = \lim_{n \rightarrow \infty} \frac{E[(\frac{1}{n} \sum_{i=1}^n X_i)^2] - 2 E[X] + E[X]^2}{\epsilon^2} \\
  &= 0
\end{align*}
For the last equality see the solution to \@ref(exr:vardecomp).

c. Follows directly from the CLT.

```
</div>



```{exercise, name = "Consistent but biased estimator", label = "cbest"}


a. Show that sample variance (the plug-in estimator of variance) is a biased estimator of variance.
b. Show that sample variance is a consistent estimator of variance.
c. Show that the estimator with ($N-1$) (Bessel correction) is unbiased.




```
<div class="fold">
```{solution, echo = togs}


a.
\begin{align*}
  E[\frac{1}{n} \sum_{i=1}^n (Y_i - \bar{Y})^2] &= \frac{1}{n} \sum_{i=1}^n E[(Y_i - \bar{Y})^2] \\
  &= \frac{1}{n} \sum_{i=1}^n E[Y_i^2] - 2 E[Y_i \bar{Y}] + \bar{Y}^2)] \\
  &= \frac{1}{n} \sum_{i=1}^n E[Y_i^2 - 2 Y_i \bar{Y} + \bar{Y}^2] \\
  &= \frac{1}{n} \sum_{i=1}^n E[Y_i^2 - \frac{2}{n} Y_i^2 - \frac{2}{n} \sum_{i \neq j} Y_i Y_j  + \frac{1}{n^2}\sum_j \sum_{k \neq j} Y_j Y_k + \frac{1}{n^2} \sum_j Y_j^2] \\
  &= \frac{1}{n} \sum_{i=1}^n \frac{n - 2}{n} (\sigma^2 - \mu^2) - \frac{2}{n} (n - 1) \mu^2 + \frac{1}{n^2}n(n-1)\mu^2 + \frac{1}{n^2}(\sigma^2 + \mu^2) \\
  &= \frac{n-1}{n}\sigma^2 \\
  < \sigma^2.
\end{align*}
b. 
\begin{align*}
  \lim_{n \rightarrow \infty} P(|\frac{1}{n} \sum_{i=1}^n (Y_i - \bar{Y})^2 - \sigma^2| > \epsilon) &\leq \lim_{n \rightarrow \infty} \frac{E[(\frac{1}{n} \sum_{i=1}^n (Y_i - \bar{Y})^2 - \sigma^2)^2]}{\epsilon^2} \\
  &= \lim_{n \rightarrow \infty} \frac{E[((\frac{1}{n} \sum_{i=1}^n (Y_i - \bar{Y})^2)^2 - 2 \frac{1}{n} \sum_{i=1}^n (Y_i - \bar{Y})^2 \sigma^2 + \sigma^4]}{\epsilon^2}
\end{align*}
c. The denominator changes in the second-to-last line of a., therefore the last line is now equality.

```
</div>
