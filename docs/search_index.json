[
["index.html", "Principles of Uncertainty – exercises Preface", " Principles of Uncertainty – exercises Gregor Pirš and Erik Štrumbelj 2019-12-16 Preface These are the exercises for the Principles of Uncertainty course of the Data Science Master’s at University of Ljubljana, Faculty of Computer and Information Science. This document will be extended each week as the course progresses. At the end of each exercise session, we will post the solutions to the exercises worked in class and select exercises for homework. Students are also encouraged to solve the remaining exercises to further extend their knowledge. Some exercises require the use of R. Those exercises (or parts of) are coloured blue. Students that are not familiar with R programming language should study ?? to learn the basics. As the course progresses, we will cover more relevant uses of R for data science. "],
["introduction.html", "1 Probability spaces 1.1 Measure and probability spaces 1.2 Properties of probability measures 1.3 Discrete probability spaces", " 1 Probability spaces This chapter deals with measures and probability spaces. At the end of the chapter, we look more closely at discrete probability spaces. The students are expected to acquire the following knowledge: Theoretical Use properties of probability to calculate probabilities. Combinatorics. Understanding of continuity of probability. R Vectors and vector operations. For loop. Estimating probability with simulation. sample function. Matrices and matrix operations. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 1.1 Measure and probability spaces Exercise 1 (Completing a set to a sigma algebra) Let \\(\\Omega = \\{1,2,...,10\\}\\) and let \\(A = \\{\\emptyset, \\{1\\}, \\{2\\}, \\Omega \\}\\). Show that \\(A\\) is not a sigma algebra of \\(\\Omega\\). Find the minimum number of elements to complete A to a sigma algebra of \\(\\Omega\\). Solution. \\(1^c = \\{2,3,...,10\\} \\notin A \\implies\\) \\(A\\) is not sigma algebra. First we need the complements of all elements, so we need to add sets \\(\\{2,3,...,10\\}\\) and \\(\\{1,3,4,...,10\\}\\). Next we need unions of all sets – we add the set \\(\\{1,2\\}\\). Again we need the complement of this set, so we add \\(\\{3,4,...,10\\}\\). So the minimum number of elements we need to add is 4. Exercise 2 (Diversity of sigma algebras) Let \\(\\Omega\\) be a set. Find the smallest sigma algebra of \\(\\Omega\\). Find the largest sigma algebra of \\(\\Omega\\). Solution. \\(A = \\{\\emptyset, \\Omega\\}\\) \\(2^{\\Omega}\\) Exercise 3 Find all sigma algebras for \\(\\Omega = \\{0, 1, 2\\}\\). Solution. \\(A = \\{\\emptyset, \\Omega\\}\\) \\(A = 2^{\\Omega}\\) \\(A = \\{\\emptyset, \\{0\\}, \\{1,2\\}, \\Omega\\}\\) \\(A = \\{\\emptyset, \\{1\\}, \\{0,2\\}, \\Omega\\}\\) \\(A = \\{\\emptyset, \\{2\\}, \\{0,1\\}, \\Omega\\}\\) Exercise 4 (Difference between algebra and sigma algebra) Let \\(\\Omega = \\mathbb{N}\\) and \\(\\mathcal{A} = \\{A \\subseteq \\mathbb{N}: A \\text{ is finite or } A^c \\text{ is finite.} \\}\\). Show that \\(\\mathcal{A}\\) is an algebra but not a sigma algebra. Solution. \\(\\emptyset\\) is finite so \\(\\emptyset \\in \\mathcal{A}\\). Let \\(A \\in \\mathcal{A}\\) and \\(B \\in \\mathcal{A}\\). If both are finite, then their union is also finite and therefore in \\(\\mathcal{A}\\). Let at least one of them not be finite. Then their union is not finite. But \\((A \\cup B)^c = A^c \\cap B^c\\). And since at least one is infinite, then its complement is finite and the intersection is too. So finite unions are in \\(\\mathcal{A}\\). Let us look at numbers \\(2n\\). For any \\(n\\), \\(2n \\in \\mathcal{A}\\) as it is finite. But \\(\\bigcup_{k = 1}^{\\infty} 2n \\notin \\mathcal{A}\\). Exercise 5 (Intro to measure) Take the measurable space \\(\\Omega = \\{1,2\\}\\), \\(F = 2^{\\Omega}\\). Which of the following is a measure? Which is a probability measure? \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 5\\), \\(\\mu(\\{2\\}) = 6\\), \\(\\mu(\\{1,2\\}) = 11\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 0\\), \\(\\mu(\\{1,2\\}) = 1\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 0\\), \\(\\mu(\\{1,2\\}) = 0\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 1\\), \\(\\mu(\\{1,2\\}) = 1\\) \\(\\mu(\\emptyset)=0\\), \\(\\mu(\\{1\\})=0\\), \\(\\mu(\\{2\\})=\\infty\\), \\(\\mu(\\{1,2\\})=\\infty\\) Solution. Measure. Not probability measure since \\(\\mu(\\Omega) &gt; 1\\). Neither due to countable additivity. Measure. Not probability measure since \\(\\mu(\\Omega) = 0\\). Probability measure. Measure. Not probability measure since \\(\\mu(\\Omega) &gt; 1\\). Exercise 6 Define a probability space that could be used to model the outcome of throwing two fair 6-sided dice. Solution. \\(\\Omega = \\{\\{i,j\\}, i = 1,...,6, j = 1,...,6\\}\\) \\(F = 2^{\\Omega}\\) \\(\\forall \\omega \\in \\Omega\\), \\(P(\\omega) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}\\) 1.2 Properties of probability measures Exercise 7 A standard deck (52 cards) is distributed to two persons: 26 cards to each person. All partitions are equally likely. Find the probability that: The first person gets 4 Queens. The first person gets at least 2 Queens. R: Use simulation (sample) to check the above answers. Solution. \\(\\frac{\\binom{48}{22}}{\\binom{52}{26}}\\) 1 - \\(\\frac{\\binom{48}{26} + 4 \\times \\binom{48}{25}}{\\binom{52}{26}}\\) For the simulation, let us represent cards with numbers from 1 to 52, and let 1 through 4 represent Queens. set.seed(1) cards &lt;- 1:52 n &lt;- 10000 q4 &lt;- vector(mode = &quot;logical&quot;, length = n) q2 &lt;- vector(mode = &quot;logical&quot;, length = n) tmp &lt;- vector(mode = &quot;logical&quot;, length = n) for (i in 1:n) { p1 &lt;- sample(1:52, 26) q4[i] &lt;- sum(1:4 %in% p1) == 4 q2[i] &lt;- sum(1:4 %in% p1) &gt;= 2 } sum(q4) / n ## [1] 0.0556 sum(q2) / n ## [1] 0.6995 Exercise 8 Let \\(A\\) and \\(B\\) be events with probabilities \\(P(A) = \\frac{2}{3}\\) and \\(P(B) = \\frac{1}{2}\\). Show that \\(\\frac{1}{6} \\leq P(A\\cap B) \\leq \\frac{1}{2}\\), and give examples to show that both extremes are possible. Find corresponding bounds for \\(P(A\\cup B)\\). R: Draw samples from the examples and show the probability bounds of \\(P(A \\cap B)\\) . Solution. - From the properties of probability we have \\[\\begin{equation} P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\leq 1. \\end{equation}\\] From this follows \\[\\begin{align} P(A \\cap B) &amp;\\geq P(A) + P(B) - 1 \\\\ &amp;= \\frac{2}{3} + \\frac{1}{2} - 1 \\\\ &amp;= \\frac{1}{6}, \\end{align}\\] which is the lower bound for the intersection. Conversely, we have \\[\\begin{equation} P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\geq P(A). \\end{equation}\\] From this follows \\[\\begin{align} P(A \\cap B) &amp;\\leq P(B) \\\\ &amp;= \\frac{1}{2}, \\end{align}\\] which is the upper bound for the intersection. For an example take a fair die. To achieve the lower bound let \\(A = \\{1,2,3\\}\\) and \\(B = \\{3,4,5,6\\}\\), then their intersection is \\(A \\cap B = \\{3\\}\\). To achieve the upper bound take \\(A = \\{1,2,3\\}\\) and \\(B = \\{1,2,3,4\\}\\). For the bounds of the union we will use the results from the first part. Again from the properties of probability we have \\[\\begin{align} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;\\geq P(A) + P(B) - \\frac{1}{2} \\\\ &amp;= \\frac{2}{3}. \\end{align}\\] Conversely \\[\\begin{align} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;\\leq P(A) + P(B) - \\frac{1}{6} \\\\ &amp;= 1. \\end{align}\\] Therefore \\(\\frac{2}{3} \\leq P(A \\cup B) \\leq 1\\). We use sample in R: set.seed(1) n &lt;- 10000 samps &lt;- sample(1:6, n, replace = TRUE) # lower bound lb &lt;- vector(mode = &quot;logical&quot;, length = n) A &lt;- c(1,2,3) B &lt;- c(3,4,5,6) for (i in 1:n) { lb[i] &lt;- samps[i] %in% A &amp; samps[i] %in% B } sum(lb) / n ## [1] 0.1724 # upper bound ub &lt;- vector(mode = &quot;logical&quot;, length = n) A &lt;- c(1,2,3) B &lt;- c(1,2,3,4) for (i in 1:n) { ub[i] &lt;- samps[i] %in% A &amp; samps[i] %in% B } sum(ub) / n ## [1] 0.5047 Exercise 9 A fair coin is tossed repeatedly. Show that, with probability one, a head turns up sooner or later. Show similarly that any given finite sequence of heads and tails occurs eventually with probability one. Solution. \\[\\begin{align} P(\\text{no heads}) &amp;= \\lim_{n \\rightarrow \\infty} P(\\text{no heads in first }n \\text{ tosses}) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\frac{1}{2^n} \\\\ &amp;= 0. \\end{align}\\] For the second part, let us fix the given sequence of heads and tails of length \\(k\\) as \\(s\\). A probability that this happens in \\(k\\) tosses is \\(\\frac{1}{2^k}\\). \\[\\begin{align} P(s \\text{ occurs}) &amp;= \\lim_{n \\rightarrow \\infty} P(s \\text{ occurs in first } nk \\text{ tosses}) \\end{align}\\] The right part of the upper equation is greater than if \\(s\\) occurs either in the first \\(k\\) tosses, second \\(k\\) tosses,…, \\(n\\)-th \\(k\\) tosses. Therefore \\[\\begin{align} P(s \\text{ occurs}) &amp;\\geq \\lim_{n \\rightarrow \\infty} P(s \\text{ occurs in first } n \\text{ disjoint sequences of length } k) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} (1 - P(s \\text{ does not occur in first } n \\text{ disjoint sequences})) \\\\ &amp;= 1 - \\lim_{n \\rightarrow \\infty} P(s \\text{ does not occur in first } n \\text{ disjoint sequences}) \\\\ &amp;= 1 - \\lim_{n \\rightarrow \\infty} (1 - \\frac{1}{2^k})^n \\\\ &amp;= 1. \\end{align}\\] Exercise 10 An Erdos-Renyi random graph \\(G(n,p)\\) is a model with \\(n\\) nodes, where each pair of nodes is connected with probability \\(p\\). Calculate the probability that there exists a node that is not connected to any other node in \\(G(4,0.6)\\). Show that the upper bound for the probability that there exist 2 nodes that are not connected to any other node for an arbitrary \\(G(n,p)\\) is \\(\\binom{n}{2} (1-p)^{2n - 3}\\). R: Estimate the probability from the first point using simulation. Solution. Let \\(A_i\\) be the event that the \\(i\\)-th node is not connected to any other node. Then our goal is to calculate \\(P(\\cup_{i=1}^n A_i)\\). Using the inclusion-exclusion principle, we get \\[\\begin{align} P(\\cup_{i=1}^n A_i) &amp;= \\sum_i A_i - \\sum_{i&lt;j} P(A_i \\cap A_j) + \\sum_{i&lt;j&lt;k} P(A_i \\cap A_j \\cap A_k) - P(A_1 \\cap A_2 \\cap A_3 \\cap A_4) \\\\ &amp;=4 (1 - p)^3 - \\binom{4}{2} (1 - p)^5 + \\binom{4}{3} (1 - p)^6 - (1 - p)^6 \\\\ &amp;\\approx 0.21. \\end{align}\\] Let \\(A_{ij}\\) be the event that nodes \\(i\\) and \\(j\\) are not connected to any other node. We are interested in \\(P(\\cup_{i&lt;j}A_{ij})\\). By using Boole`s inequality, we get \\[\\begin{align} P(\\cup_{i&lt;j}A_{ij}) \\leq \\sum_{i&lt;j} P(A_{ij}). \\end{align}\\] What is the probability of \\(A_{ij}\\)? There need to be no connections to the \\(i\\)-th node to the remaining nodes (excluding \\(j\\)), the same for the \\(j\\)-th node, and there can be no connection between them. Therefore \\[\\begin{align} P(\\cup_{i&lt;j}A_{ij}) &amp;\\leq \\sum_{i&lt;j} (1 - p)^{2(n-2) + 1} \\\\ &amp;= \\binom{n}{2} (1 - p)^{2n - 3}. \\end{align}\\] set.seed(1) n_samp &lt;- 100000 n &lt;- 4 p &lt;- 0.6 conn_samp &lt;- vector(mode = &quot;logical&quot;, length = n_samp) for (i in 1:n_samp) { tmp_mat &lt;- matrix(data = 0, nrow = n, ncol = n) samp_conn &lt;- sample(c(0,1), choose(4,2), replace = TRUE, prob = c(1 - p, p)) tmp_mat[lower.tri(tmp_mat)] &lt;- samp_conn tmp_mat[upper.tri(tmp_mat)] &lt;- t(tmp_mat)[upper.tri(t(tmp_mat))] not_conn &lt;- apply(tmp_mat, 1, sum) if (any(not_conn == 0)) { conn_samp[i] &lt;- TRUE } else { conn_samp[i] &lt;- FALSE } } sum(conn_samp) / n_samp ## [1] 0.20565 1.3 Discrete probability spaces Exercise 11 Show that the standard measurable space on \\(\\Omega = \\{0,1,...,n\\}\\) equipped with binomial measure is a discrete probability space. Define another probability measure on this measurable space. Show that for \\(n=1\\) the binomial measure is the same as the Bernoulli measure. R: Draw 1000 samples from the binomial distribution \\(p=0.5\\), \\(n=20\\) (rbinom) and compare relative frequencies with theoretical probability measure. Solution. We need to show that the terms of \\(\\sum_{k=0}^n \\binom{n}{k} p^k (1 - p)^{n - k}\\) sum to 1. For that we use the binomial theorem \\(\\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} = (x + y)^n\\). So \\[\\begin{equation} \\sum_{k=0}^n \\binom{n}{k} p^k (1 - p)^{n - k} = (p + 1 - p)^n = 1. \\end{equation}\\] \\(P(\\{k\\}) = \\frac{1}{n + 1}\\). When \\(n=1\\) then \\(k \\in \\{0,1\\}\\). Inserting \\(n=1\\) into the binomial measure, we get \\(\\binom{1}{k}p^k (1-p)^{1 - k}\\). Now \\(\\binom{1}{1} = \\binom{1}{0} = 1\\), so the measure is \\(p^k (1-p)^{1 - k}\\), which is the Bernoulli measure. set.seed(1) library(ggplot2) library(dplyr) bin_samp &lt;- rbinom(n = 1000, size = 20, prob = 0.5) bin_samp &lt;- data.frame(x = bin_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dbinom(0:20, size = 20, prob = 0.5), type = &quot;theoretical_measure&quot;)) bin_plot &lt;- ggplot(data = bin_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(bin_plot) Exercise 12 Show that the standard measurable space on \\(\\Omega = \\{0,1,...,\\infty\\}\\) equipped with geometric measure is a discrete probability space, equipped with Poisson measure is a discrete probability space. Define another probability measure on this measurable space. R: Draw 1000 samples from the Poisson distribution \\(\\lambda = 10\\) (rpois) and compare relative frequencies with theoretical probability measure. Solution. \\(\\sum_{k = 0}^{\\infty} p(1 - p)^k = p \\sum_{k = 0}^{\\infty} (1 - p)^k = p \\frac{1}{1 - 1 + p} = 1\\). We used the formula for geometric series. \\(\\sum_{k = 0}^{\\infty} \\frac{\\lambda^k e^{-\\lambda}}{k!} = e^{-\\lambda} \\sum_{k = 0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{-\\lambda} e^{\\lambda} = 1.\\) We used the Taylor expansion of the exponential function. Since we only have to define a probability measure, we could only assign probabilities that sum to one to a finite number of events in \\(\\Omega\\), and probability zero to the other infinite number of events. However to make this solution more educational, we will try to find a measure that assigns a non-zero probability to all events in \\(\\Omega\\). A good start for this would be to find a converging infinite series, as the probabilities will have to sum to one. One simple converging series is the geometric series \\(\\sum_{k=0}^{\\infty} p^k\\) for \\(|p| &lt; 1\\). Let us choose an arbitrary \\(p = 0.5\\). Then \\(\\sum_{k=0}^{\\infty} p^k = \\frac{1}{1 - 0.5} = 2\\). To complete the measure, we have to normalize it, so it sums to one, therefore \\(P(\\{k\\}) = \\frac{0.5^k}{2}\\) is a probability measure on \\(\\Omega\\). We could make it even more difficult by making this measure dependent on some parameter \\(\\alpha\\), but this is out of the scope of this introductory chapter. set.seed(1) pois_samp &lt;- rpois(n = 1000, lambda = 10) pois_samp &lt;- data.frame(x = pois_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:25, n = dpois(0:25, lambda = 10), type = &quot;theoretical_measure&quot;)) pois_plot &lt;- ggplot(data = pois_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(pois_plot) Exercise 13 Define a probability measure on \\((\\Omega = \\mathbb{Z}, 2^{\\mathbb{Z}})\\). Define a probability measure such that \\(P(\\omega) &gt; 0, \\forall \\omega \\in \\Omega\\). R: Implement a random generator that will generate samples with the relative frequency that corresponds to your probability measure. Compare relative frequencies with theoretical probability measure . Solution. \\(P(0) = 1, P(\\omega) = 0, \\forall \\omega \\neq 0\\). \\(P(\\{k\\}) = \\sum_{k = -\\infty}^{\\infty} \\frac{p(1 - p)^{|k|}}{2^{1 - 1_0(k)}}\\), where \\(1_0(k)\\) is the indicator function, which equals to one if \\(k\\) is 0, and equals to zero in every other case. n &lt;- 1000 geom_samps &lt;- rgeom(n, prob = 0.5) sign_samps &lt;- sample(c(FALSE, TRUE), size = n, replace = TRUE) geom_samps[sign_samps] &lt;- -geom_samps[sign_samps] my_pmf &lt;- function (k, p) { indic &lt;- rep(1, length(k)) indic[k == 0] &lt;- 0 return ((p * (1 - p)^(abs(k))) / 2^indic) } geom_samps &lt;- data.frame(x = geom_samps) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = -10:10, n = my_pmf(-10:10, 0.5), type = &quot;theoretical_measure&quot;)) geom_plot &lt;- ggplot(data = geom_samps, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geom_plot) Exercise 14 Define a probability measure on \\(\\Omega = \\{1,2,3,4,5,6\\}\\) with parameter \\(m \\in \\{1,2,3,4,5,6\\}\\), so that the probability of outcome at distance \\(1\\) from \\(m\\) is half of the probability at distance \\(0\\), at distance \\(2\\) is half of the probability at distance \\(1\\), etc. R: Implement a random generator that will generate samples with the relative frequency that corresponds to your probability measure. Compare relative frequencies with theoretical probability measure . Solution. \\(P(\\{k\\}) = \\frac{\\frac{1}{2}^{|m - k|}}{\\sum_{i=1}^6 \\frac{1}{2}^{|m - i|}}\\) n &lt;- 10000 m &lt;- 4 my_pmf &lt;- function (k, m) { denom &lt;- sum(0.5^abs(m - 1:6)) return (0.5^abs(m - k) / denom) } samps &lt;- c() for (i in 1:n) { a &lt;- sample(1:6, 1) a_val &lt;- my_pmf(a, m) prob &lt;- runif(1) if (prob &lt; a_val) { samps &lt;- c(samps, a) } } samps &lt;- data.frame(x = samps) %&gt;% count(x) %&gt;% mutate(n = n / length(samps), type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 1:6, n = my_pmf(1:6, m), type = &quot;theoretical_measure&quot;)) my_plot &lt;- ggplot(data = samps, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(my_plot) "],
["uprobspaces.html", "2 Uncountable probability spaces 2.1 Borel sets 2.2 Lebesgue measure", " 2 Uncountable probability spaces This chapter deals with uncountable probability spaces. The students are expected to acquire the following knowledge: Theoretical Understand Borel sets and identify them. Estimate Lebesgue measure for different sets. Know when sets are Borel-measurable. Understanding of countable and uncountable sets. R Uniform sampling. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 2.1 Borel sets Exercise 15 Prove that the intersection of two sigma algebras on \\(\\Omega\\) is a sigma algebra. Prove that the collection of all open subsets \\((a,b)\\) on \\((0,1]\\) is not a sigma algebra of \\((0,1]\\). Solution. Empty set: \\[\\begin{equation} \\emptyset \\in \\mathcal{A} \\wedge \\emptyset \\in \\mathcal{B} \\Rightarrow \\emptyset \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Complement: \\[\\begin{equation} \\text{Let } A \\in \\mathcal{A} \\cap \\mathcal{B} \\Rightarrow A \\in \\mathcal{A} \\wedge A \\in \\mathcal{B} \\Rightarrow A^c \\in \\mathcal{A} \\wedge A^c \\in \\mathcal{B} \\Rightarrow A^c \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Countable additivity: Let \\(\\{A_i\\}\\) be a countable sequence of subsets in \\(\\mathcal{A} \\cap \\mathcal{B}\\). \\[\\begin{equation} \\forall i: A_i \\in \\mathcal{A} \\cap \\mathcal{B} \\Rightarrow A_i \\in \\mathcal{A} \\wedge A_i \\in \\mathcal{B} \\Rightarrow \\cup A_i \\in \\mathcal{A} \\wedge \\cup A_i \\in \\mathcal{B} \\Rightarrow \\cup A_i \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Let \\(A\\) denote the collection of all open subsets \\((a,b)\\) on \\((0,1]\\). Then \\((0,1) \\in A\\). But \\((0,1)^c = 1 \\notin A\\). Exercise 16 Show that \\(\\mathcal{C} = \\sigma(\\mathcal{C})\\) if and only if \\(\\mathcal{C}\\) is a sigma algebra. Solution. “\\(\\Rightarrow\\)” This follows from the definition of a generated sigma algebra. “\\(\\Leftarrow\\)” Let \\(\\mathcal{F} = \\cap_i F_i\\) be the intersection of all sigma algebras that contain \\(\\mathcal{C}\\). Then \\(\\sigma(\\mathcal{C}) = \\mathcal{F}\\). Additionally, \\(\\forall i: \\mathcal{C} \\in F_i\\). So each \\(F_i\\) can be written as \\(F_i = \\mathcal{C} \\cup D\\), where \\(D\\) are the rest of the elements in the sigma algebra. In other words, each sigma algebra in the collection contains at least \\(\\mathcal{C}\\), but can contain other elements. Now for some \\(j\\), \\(F_j = \\mathcal{C}\\) as \\(\\{F_i\\}\\) contains all sigma algebras that contain \\(\\mathcal{C}\\) and \\(\\mathcal{C}\\) is such a sigma algebra. Since this is the smallest subset in the intersection it follows that \\(\\sigma(\\mathcal{C}) = \\mathcal{F} = \\mathcal{C}\\). Exercise 17 Let \\(\\mathcal{C}\\) and \\(\\mathcal{D}\\) be two collections of subsets on \\(\\Omega\\) such that \\(\\mathcal{C} \\subset \\mathcal{D}\\). Prove that \\(\\sigma(\\mathcal{C}) \\subseteq \\sigma(\\mathcal{D})\\). Solution. \\(\\sigma(\\mathcal{D})\\) is a sigma algebra that contains \\(\\mathcal{D}\\). It follows that \\(\\sigma(\\mathcal{D})\\) is a sigma algebra that contains \\(\\mathcal{C}\\). Let us write \\(\\sigma(\\mathcal{C}) = \\cap_i F_i\\), where \\(\\{F_i\\}\\) is the collection of all sigma algebras that contain \\(\\mathcal{C}\\). Since \\(\\sigma(\\mathcal{D})\\) is such a sigma algebra, there exists an index \\(j\\), so that \\(F_j = \\sigma(\\mathcal{D})\\). Then we can write \\[\\begin{align} \\sigma(\\mathcal{C}) &amp;= (\\cap_{i \\neq j} F_i) \\cap \\sigma(\\mathcal{D}) \\\\ &amp;\\subset \\sigma(\\mathcal{D}). \\end{align}\\] Exercise 18 Prove that the following subsets of \\((0,1]\\) are Borel-measurable by finding their measure. Any countable set. The set of numbers in (0,1] whose decimal expansion does not contain 7. Solution. This follows directly from the fact that every countable set is a union of singletons, whose measure is 0. Let us first look at numbers which have a 7 as the first decimal numbers. Their measure is 0.1. Then we take all the numbers with a 7 as the second decimal number (excluding those who already have it as the first). These have the measure 0.01, and there are 9 of them, so their total measure is 0.09. We can continue to do so infinitely many times. At each \\(n\\), we have the measure of the intervals which is \\(10^n\\) and the number of those intervals is \\(9^{n-1}\\). Now \\[\\begin{align} \\lambda(A) &amp;= 1 - \\sum_{n = 0}^{\\infty} \\frac{9^n}{10^{n+1}} \\\\ &amp;= 1 - \\frac{1}{10} \\sum_{n = 0}^{\\infty} (\\frac{9}{10})^n \\\\ &amp;= 1 - \\frac{1}{10} \\frac{10}{1} \\\\ &amp;= 0. \\end{align}\\] Since we have shown that the measure of the set is \\(0\\), we have also shown that the set is measurable. Exercise 19 Let \\(\\Omega = [0,1]\\), and let \\(\\mathcal{F}_3\\) consist of all countable subsets of \\(\\Omega\\), and all subsets of \\(\\Omega\\) having a countable complement. Show that \\(\\mathcal{F}_3\\) is a sigma algebra. Let us define \\(P(A)=0\\) if \\(A\\) is countable, and \\(P(A) = 1\\) if \\(A\\) has a countable complement. Is \\((\\Omega, \\mathcal{F}_3, P)\\) a legitimate probability space? Solution. The empty set is countable, therefore it is in \\(\\mathcal{F}_3\\). For any \\(A \\in \\mathcal{F}_3\\). If \\(A\\) is countable, then \\(A^c\\) has a countable complement and is in \\(\\mathcal{F}_3\\). If \\(A\\) is uncountable, then it has a countable complement \\(A^c\\) which is therefore also in \\(\\mathcal{F}_3\\). We are left with showing countable additivity. Let \\(\\{A_i\\}\\) be an arbitrary collection of sets in \\(\\mathcal{F}_3\\). We will look at two possibilities. First let all \\(A_i\\) be countable. A countable union of countable sets is countable, and therefore in \\(\\mathcal{F}_3\\). Second, let at least one \\(A_i\\) be uncountable. It follows that it has a countable complement. We can write \\[\\begin{equation} (\\cup_{i=1}^{\\infty} A_i)^c = \\cap_{i=1}^{\\infty} A_i^c. \\end{equation}\\] Since at least one \\(A_i^c\\) on the right side is countable, the whole intersection is countable, and therefore the union has a countable complement. It follows that the union is in \\(\\mathcal{F}_3\\). The tuple \\((\\Omega, \\mathcal{F}_3)\\) is a measurable space. Therefore, we only need to check whether \\(P\\) is a probability measure. The measure of the empty set is zero as it is countable. We have to check for countable additivity. Let us look at three situations. Let \\(A_i\\) be disjoint sets. First, let all \\(A_i\\) be countable. \\[\\begin{equation} P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty}P( A_i)) = 0. \\end{equation}\\] Since the union is countable, the above equation holds. Second, let exactly one \\(A_i\\) be uncountable. W.L.O.G. let that be \\(A_1\\). Then \\[\\begin{equation} P(\\cup_{i=1}^{\\infty} A_i) = 1 + \\sum_{i=2}^{\\infty}P( A_i)) = 1. \\end{equation}\\] Since the union is uncountable, the above equation holds. Third, let at least two \\(A_i\\) be uncountable. We have to check whether it is possible for two uncountable sets in \\(\\mathcal{F}_3\\) to be disjoint. If that is possible, then their measures would sum to more than one and \\(P\\) would not be a probability measure. W.L.O.G. let \\(A_1\\) and \\(A_2\\) be uncountable. Then we have \\[\\begin{equation} A_1 \\cap A_2 = (A_1^c \\cup A_2^c)^c. \\end{equation}\\] Now \\(A_1^c\\) and \\(A_2^c\\) are countable and their union is therefore countable. Let \\(B = A_1^c \\cup A_2^c\\). So the intersection of \\(A_1\\) and \\(A_2\\) equals the complement of \\(B\\), which is countable. For the intersection to be the empty set, \\(B\\) would have to equal to \\(\\Omega\\). But \\(\\Omega\\) is uncountable and therefore \\(B\\) can not equal to \\(\\Omega\\). It follows that two uncountable sets in \\(\\mathcal{F}_3\\) can not have an empty intersection. Therefore the tuple is a legitimate probability space. 2.2 Lebesgue measure Exercise 20 Show that the Lebesgue measure of rational numbers on \\([0,1]\\) is 0. R: Implement a random number generator, which generates uniform samples of irrational numbers in \\([0,1]\\) by uniformly sampling from \\([0,1]\\) and rejecting a sample if it is rational. Solution. There are a countable number of rational numbers. Therefore, we can write \\[\\begin{align} \\lambda(\\mathbb{Q}) &amp;= \\lambda(\\cup_{i = 1}^{\\infty} q_i) &amp;\\\\ &amp;= \\sum_{i = 1}^{\\infty} \\lambda(q_i) &amp;\\text{ (countable additivity)} \\\\ &amp;= \\sum_{i = 1}^{\\infty} 0 &amp;\\text{ (Lebesgue measure of a singleton)} \\\\ &amp;= 0. \\end{align}\\] Exercise 21 Prove that the Lebesgue measure of \\(\\mathbb{R}\\) is infinity. Paradox. Show that the cardinality of \\(\\mathbb{R}\\) and \\((0,1)\\) is the same, while their Lebesgue measures are infinity and one respectably. Solution. Let \\(a_i\\) be the \\(i\\)-th integer for \\(i \\in \\mathbb{Z}\\). We can write \\(\\mathbb{R} = \\cup_{-\\infty}^{\\infty} (a_i, a_{i + 1}]\\). \\[\\begin{align} \\lambda(\\mathbb{R}) &amp;= \\lambda(\\cup_{i = -\\infty}^{\\infty} (a_i, a_{i + 1}]) \\\\ &amp;= \\lambda(\\lim_{n \\rightarrow \\infty} \\cup_{i = -n}^{n} (a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\lambda(\\cup_{i = -n}^{n} (a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\sum_{i = -n}^{n} \\lambda((a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\sum_{i = -n}^{n} 1 \\\\ &amp;= \\lim_{n \\rightarrow \\infty} 2n \\\\ &amp;= \\infty. \\end{align}\\] We need to find a bijection between \\(\\mathbb{R}\\) and \\((0,1)\\). A well-known function that maps from a bounded interval to \\(\\mathbb{R}\\) is the tangent. To make the bijection easier to achieve, we will take the inverse, which maps from \\(\\mathbb{R}\\) to \\((-\\frac{\\pi}{2}, \\frac{\\pi}{2})\\). However, we need to change the function so it maps to \\((0,1)\\). First we add \\(\\frac{\\pi}{2}\\), so that we move the function above zero. Then we only have to divide by the max value, which in this case is \\(\\pi\\). So our bijection is \\[\\begin{equation} f(x) = \\frac{\\tan^{-1}(x) + \\frac{\\pi}{2}}{\\pi}. \\end{equation}\\] Exercise 22 Take the measure space \\((\\Omega_1 = (0,1], B_{(0,1], \\lambda})\\) (we know that this is a probability space on \\((0,1]\\)). Define a map (function) from \\(\\Omega_1\\) to \\(\\Omega_2 = \\{1,2,3,4,5,6\\}\\) such that the measure space \\((\\Omega_2, 2^{\\Omega_2}, \\lambda(f^{-1}()))\\) will be a discrete probability space with uniform probabilities (\\(P(\\omega) = \\frac{1}{6}, \\forall \\omega \\in \\Omega_2)\\). Is the map that you defined in (a) the only such map? How would you in the same fashion define a map that would result in a probability space that can be interpreted as a coin toss with probability \\(p\\) of heads? R: Use the map in (a) as a basis for a random generator for this fair die. Solution. In other words, we have to assign disjunct intervals of the same size to each element of \\(\\Omega_2\\). Therefore \\[\\begin{equation} f(x) = \\lceil 6x \\rceil. \\end{equation}\\] No, we could for example rearrange the order in which the intervals are mapped to integers. Additionally, we could have several disjoint intervals that mapped to the same integer, as long as the Lebesgue measure of their union would be \\(\\frac{1}{6}\\) and the function would remain injective. We have \\(\\Omega_3 = \\{0,1\\}\\), where zero represents heads and one represents tails. Then \\[\\begin{equation} f(x) = 0^{I_{A}(x)}, \\end{equation}\\] where \\(A = \\{y \\in (0,1] : y &lt; p\\}\\). set.seed(1) unif_s &lt;- runif(1000) die_s &lt;- ceiling(6 * unif_s) summary(as.factor(die_s)) ## 1 2 3 4 5 6 ## 166 154 200 146 166 168 "],
["condprob.html", "3 Conditional probability 3.1 Calculating conditional probabilities 3.2 Conditional independence 3.3 Monty Hall problem", " 3 Conditional probability This chapter deals with conditional probability. The students are expected to acquire the following knowledge: Theoretical Identify whether variables are independent. Calculation of conditional probabilities. Understanding of conditional dependence and independence. How to apply Bayes’ theorem to solve difficult probabilistic questions. R Simulating conditional probabilities. cumsum. apply. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 3.1 Calculating conditional probabilities Exercise 23 A military officer is in charge of identifying enemy aircraft and shooting them down. He is able to positively identify an enemy airplane 95% of the time and positively identify a friendly airplane 90% of the time. Furthermore, 99% of the airplanes are friendly. When the officer identifies an airplane as an enemy airplane, what is the probability that it is not and they will shoot at a friendly airplane? Solution. Let \\(E = 0\\) denote that the observed plane is friendly and \\(E=1\\) that it is an enemy. Let \\(I = 0\\) denote that the officer identified it as friendly and \\(I = 1\\) as enemy. Then \\[\\begin{align} P(E = 0 | I = 1) &amp;= \\frac{P(I = 1 | E = 0)P(E = 0)}{P(I = 1)} \\\\ &amp;= \\frac{P(I = 1 | E = 0)P(E = 0)}{P(I = 1 | E = 0)P(E = 0) + P(I = 1 | E = 1)P(E = 1)} \\\\ &amp;= \\frac{0.1 \\times 0.99}{0.1 \\times 0.99 + 0.95 \\times 0.01} \\\\ &amp;= 0.91. \\end{align}\\] Exercise 24 R: Consider tossing a fair die. Let \\(A = \\{2,4,6\\}\\) and \\(B = \\{1,2,3,4\\}\\). Then \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{2}{3}\\) and \\(P(AB) = \\frac{1}{3}\\). Since \\(P(AB) = P(A)P(B)\\), the events \\(A\\) and \\(B\\) are independent. Simulate draws from the sample space and verify that the proportions are the same. Then find two events \\(C\\) and \\(D\\) that are not independent and repeat the simulation. set.seed(1) nsamps &lt;- 10000 tosses &lt;- sample(1:6, nsamps, replace = TRUE) PA &lt;- sum(tosses %in% c(2,4,6)) / nsamps PB &lt;- sum(tosses %in% c(1,2,3,4)) / nsamps PA * PB ## [1] 0.3283998 sum(tosses %in% c(2,4)) / nsamps ## [1] 0.3217 # Let C = {1,2} and D = {2,3} PC &lt;- sum(tosses %in% c(1,2)) / nsamps PD &lt;- sum(tosses %in% c(2,3)) / nsamps PC * PD ## [1] 0.1114867 sum(tosses %in% c(2)) / nsamps ## [1] 0.1631 Exercise 25 A machine reports the true value of a thrown 12-sided die 5 out of 6 times. If the machine reports a 1 has been tossed, what is the probability that it is actually a 1? Now let the machine only report whether a 1 has been tossed or not. Does the probability change? R: Use simulation to check your answers to a) and b). Solution. Let \\(T = 1\\) denote that the toss is 1 and \\(M = 1\\) that the machine reports a 1. \\[\\begin{align} P(T = 1 | M = 1) &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{P(M = 1)} \\\\ &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{\\sum_{k=1}^{12} P(M = 1 | T = k)P(T = k)} \\\\ &amp;= \\frac{\\frac{5}{6}\\frac{1}{12}}{\\frac{5}{6}\\frac{1}{12} + 11 \\frac{1}{6} \\frac{1}{11} \\frac{1}{12}} \\\\ &amp;= \\frac{5}{6}. \\end{align}\\] Yes. \\[\\begin{align} P(T = 1 | M = 1) &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{P(M = 1)} \\\\ &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{\\sum_{k=1}^{12} P(M = 1 | T = k)P(T = k)} \\\\ &amp;= \\frac{\\frac{5}{6}\\frac{1}{12}}{\\frac{5}{6}\\frac{1}{12} + 11 \\frac{1}{6} \\frac{1}{12}} \\\\ &amp;= \\frac{5}{16}. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 report_a &lt;- vector(mode = &quot;numeric&quot;, length = nsamps) report_b &lt;- vector(mode = &quot;logical&quot;, length = nsamps) truths &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:10000) { toss &lt;- sample(1:12, size = 1) truth &lt;- sample(c(TRUE, FALSE), size = 1, prob = c(5/6, 1/6)) truths[i] &lt;- truth if (truth) { report_a[i] &lt;- toss report_b[i] &lt;- toss == 1 } else { remaining &lt;- (1:12)[1:12 != toss] report_a[i] &lt;- sample(remaining, size = 1) report_b[i] &lt;- toss != 1 } } truth_a1 &lt;- truths[report_a == 1] sum(truth_a1) / length(truth_a1) ## [1] 0.8384075 truth_b1 &lt;- truths[report_b] sum(truth_b1) / length(truth_b1) ## [1] 0.3110339 Exercise 26 A coin is tossed independently \\(n\\) times. The probability of heads at each toss is \\(p\\). At each time \\(k\\), \\((k = 2,3,...,n)\\) we get a reward at time \\(k+1\\) if \\(k\\)-th toss was a head and the previous toss was a tail. Let \\(A_k\\) be the evebt that a reward is obtained at time \\(k\\). Are events \\(A_k\\) and \\(A_{k+1}\\) independent? Are events \\(A_k\\) and \\(A_{k+2}\\) independent? R: simulate 10 tosses 10000 times, where \\(p = 0.7\\). Check your answers to a) and b) by counting the frequencies of the events \\(A_5\\), \\(A_6\\), and \\(A_7\\). Solution. For \\(A_k\\) to happen, we need the tosses \\(k-2\\) and \\(k-1\\) be tails and heads respectively. For \\(A_{k+1}\\) to happen, we need tosses \\(k-1\\) and \\(k\\) be tails and heads respectively. As the toss \\(k-1\\) need to be heads for one and tails for the other, these two events can not happen simultaneously. Therefore the probability of their intersection is 0. But the probability of each of them separately is \\(p(1-p) &gt; 0\\). Therefore, they are not independent. For \\(A_k\\) to happen, we need the tosses \\(k-2\\) and \\(k-1\\) be tails and heads respectively. For \\(A_{k+2}\\) to happen, we need tosses \\(k\\) and \\(k+1\\) be tails and heads respectively. So the probability of intersection is \\(p^2(1-p)^2\\). And the probability of each separately is again \\(p(1-p)\\). Therefore, they are independent. set.seed(1) nsamps &lt;- 10000 p &lt;- 0.7 rewardA_5 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_6 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_7 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_56 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_57 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:nsamps) { samps &lt;- sample(c(0,1), size = 10, replace = TRUE, prob = c(0.7, 0.3)) rewardA_5[i] &lt;- (samps[4] == 0 &amp; samps[3] == 1) rewardA_6[i] &lt;- (samps[5] == 0 &amp; samps[4] == 1) rewardA_7[i] &lt;- (samps[6] == 0 &amp; samps[5] == 1) rewardA_56[i] &lt;- (rewardA_5[i] &amp; rewardA_6[i]) rewardA_57[i] &lt;- (rewardA_5[i] &amp; rewardA_7[i]) } sum(rewardA_5) / nsamps ## [1] 0.2141 sum(rewardA_6) / nsamps ## [1] 0.2122 sum(rewardA_7) / nsamps ## [1] 0.2107 sum(rewardA_56) / nsamps ## [1] 0 sum(rewardA_57) / nsamps ## [1] 0.0454 Exercise 27 A drawer contains two coins. One is an unbiased coin, the other is a biased coin, which will turn up heads with probability \\(p\\) and tails with probability \\(1-p\\). One coin is selected uniformly at random. The selected coin is tossed \\(n\\) times. The coin turns up heads \\(k\\) times and tails \\(n-k\\) times. What is the probability that the coin is biased? The selected coin is tossed repeatedly until it turns up heads \\(k\\) times. Given that it is tossed \\(n\\) times in total, what is the probability that the coin is biased? Solution. Let \\(B = 1\\) denote that the coin is biased and let \\(H = k\\) denote that we’ve seen \\(k\\) heads. \\[\\begin{align} P(B = 1 | H = k) &amp;= \\frac{P(H = k | B = 1)P(B = 1)}{P(H = k)} \\\\ &amp;= \\frac{P(H = k | B = 1)P(B = 1)}{P(H = k | B = 1)P(B = 1) + P(H = k | B = 0)P(B = 0)} \\\\ &amp;= \\frac{p^k(1-p)^{n-k} 0.5}{p^k(1-p)^{n-k} 0.5 + 0.5^{n+1}} \\\\ &amp;= \\frac{p^k(1-p)^{n-k}}{p^k(1-p)^{n-k} + 0.5^n}. \\end{align}\\] The same results as in a). The only difference between these two scenarios is that in b) the last throw must be heads. However, this holds for the biased and the unbiased coin and therefore does not affect the probability of the coin being biased. Exercise 28 Judy goes around the company for Women’s day and shares flowers. In every office she leaves a flower, if there is at least one woman inside. The probability that there’s a woman in the office is \\(\\frac{3}{5}\\). What is the probability that Judy leaves her first flower in the fourth office? Given that she has given away exactly three flowers in the first four offices, what is the probability that she gives her fourth flower in the eighth office? What is the probability that she leaves the second flower in the fifth office? What is the probability that she leaves the second flower in the fifth office, given that she did not leave the second flower in the second office? Judy needs a new supply of flowers immediately after the office, where she gives away her last flower. What is the probability that she visits at least five offices, if she starts with two flowers? R: simulate Judy’s walk 10000 times to check your answers a) - e). Solution. Let \\(X_i = k\\) denote the event that … \\(i\\)-th sample on the \\(k\\)-th run. Since the events are independent, we can multiply their probabilities to get \\[\\begin{equation} P(X_1 = 4) = 0.4^3 \\times 0.6 = 0.0384. \\end{equation}\\] Same as in a) as we have a fresh start after first four offices. For this to be possible, she had to leave the first flower in one of the first four offices. Therefore there are four possibilities, and for each of those the probability is \\(0.4^3 \\times 0.6\\). Additionally, the probability that she leaves a flower in the fifth office is \\(0.6\\). So \\[\\begin{equation} P(X_2 = 5) = \\binom{4}{1} \\times 0.4^3 \\times 0.6^2 = 0.09216. \\end{equation}\\] We use Bayes’ theorem. \\[\\begin{align} P(X_2 = 5 | X_2 \\neq 2) &amp;= \\frac{P(X_2 \\neq 2 | X_2 = 5)P(X_2 = 5)}{P(X_2 \\neq 2)} \\\\ &amp;= \\frac{0.09216}{0.64} \\\\ &amp;= 0.144. \\end{align}\\] The denominator in the second equation can be calculated as follows. One of three things has to happen for the second not to be dealt in the second round. First, both are zero, so \\(0.4^2\\). Second, first is zero, and second is one, so \\(0.4 \\times 0.6\\). Third, the first is one and the second one zero, so \\(0.6 \\times 0.4\\). Summing these values we get \\(0.64\\). We will look at the complement, so the events that she gave away exactly two flowers after two, three and four offices. \\[\\begin{equation} P(X_2 \\geq 5) = 1 - 0.6^2 - 2 \\times 0.4 \\times 0.6^2 - 3 \\times 0.4^2 \\times 0.6^2 = 0.1792. \\end{equation}\\] The multiplying parts represent the possibilities of the first flower. set.seed(1) nsamps &lt;- 100000 Judyswalks &lt;- matrix(data = NA, nrow = nsamps, ncol = 8) for (i in 1:nsamps) { thiswalk &lt;- sample(c(0,1), size = 8, replace = TRUE, prob = c(0.4, 0.6)) Judyswalks[i, ] &lt;- thiswalk } csJudy &lt;- t(apply(Judyswalks, 1, cumsum)) # a sum(csJudy[ ,4] == 1 &amp; csJudy[ ,3] == 0) / nsamps ## [1] 0.03848 # b csJsubset &lt;- csJudy[csJudy[ ,4] == 3 &amp; csJudy[ ,3] == 2, ] sum(csJsubset[ ,8] == 4 &amp; csJsubset[ ,7] == 3) / nrow(csJsubset) ## [1] 0.03665893 # c sum(csJudy[ ,5] == 2 &amp; csJudy[ ,4] == 1) / nsamps ## [1] 0.09117 # d sum(csJudy[ ,5] == 2 &amp; csJudy[ ,4] == 1) / sum(csJudy[ ,2] != 2) ## [1] 0.1422398 # e sum(csJudy[ ,4] &lt; 2) / nsamps ## [1] 0.17818 3.2 Conditional independence Exercise 29 Describe: A real-world example of two events \\(A\\) and \\(B\\) that are dependent but become conditionally independent if conditioned on a third event \\(C\\). A real-world example of two events \\(A\\) and \\(B\\) that are independent, but become dependent if conditioned on some third event \\(C\\). Solution. Let \\(A\\) be the height of a person and let \\(B\\) be the person’s knowledge of the Dutch language. These events are dependent since the Dutch are known to be taller than average. However if \\(C\\) is the nationality of the person, then \\(A\\) and \\(B\\) are independent given \\(C\\). Let \\(A\\) be the event that Mary passes the exam and let \\(B\\) be the event that John passes the exam. These events are independent. However, if the event \\(C\\) is that Mary and John studied together, then \\(A\\) and \\(B\\) are conditionally dependent given \\(C\\). Exercise 30 We have two coins of identical appearance. We know that one is a fair coin and the other flips heads 80% of the time. We choose one of the two coins uniformly at random. We discard the coin that was not chosen. We now flip the chosen coin independently 10 times, producing a sequence \\(Y_1 = y_1\\), \\(Y_2 = y_2\\), …, \\(Y_{10} = y_{10}\\). Intuitively, without doing and computation, are these random variables independent? Compute the probability \\(P(Y_1 = 1)\\). Compute the probabilities \\(P(Y_2 = 1 | Y_1 = 1)\\) and P(Y_{10} = 1 | Y_1 = 1,…,Y_9 = 1)$. Given your answers to b) and c), would you now change your answer to a)? If so, discuss why your intuition had failed. Solution. \\(P(Y_1 = 1) = 0.5 * 0.8 + 0.5 * 0.5 = 0.65\\). Since we know that \\(Y_1 = 1\\) this should change our view of the probability of the coin being biased or not. Let \\(B = 1\\) denote the event that the coin is biased and let \\(B = 0\\) denote that the coin is unbiased. By using marginal probability, we can write \\[\\begin{align} P(Y_2 = 1 | Y_1 = 1) &amp;= P(Y_2 = 1, B = 1 | Y_1 = 1) + P(Y_2 = 1, B = 0 | Y_1 = 1) \\\\ &amp;= \\sum_{k=1}^2 P(Y_2 = 1 | B = k, Y_1 = 1)P(B = k | Y_1 = 1) \\\\ &amp;= 0.8 \\frac{P(Y_1 = 1 | B = 1)P(B = 1)}{P(Y_1 = 1)} + 0.5 \\frac{P(Y_1 = 1 | B = 0)P(B = 0)}{P(Y_1 = 1)} \\\\ &amp;= 0.8 \\frac{0.8 \\times 0.5}{0.65} + 0.5 \\frac{0.5 \\times 0.5}{0.65} \\\\ &amp;\\approx 0.68. \\end{align}\\] For the other calculation we follow the same procedure. Let \\(X = 1\\) denote that first nine tosses are all heads (equivalent to \\(Y_1 = 1\\),…, \\(Y_9 = 1\\)). \\[\\begin{align} P(Y_{10} = 1 | X = 1) &amp;= P(Y_2 = 1, B = 1 | X = 1) + P(Y_2 = 1, B = 0 | X = 1) \\\\ &amp;= \\sum_{k=1}^2 P(Y_2 = 1 | B = k, X = 1)P(B = k | X = 1) \\\\ &amp;= 0.8 \\frac{P(X = 1 | B = 1)P(B = 1)}{P(X = 1)} + 0.5 \\frac{P(X = 1 | B = 0)P(B = 0)}{P(X = 1)} \\\\ &amp;= 0.8 \\frac{0.8^9 \\times 0.5}{0.5 \\times 0.8^9 + 0.5 \\times 0.5^9} + 0.5 \\frac{0.5^9 \\times 0.5}{0.5 \\times 0.8^9 + 0.5 \\times 0.5^9} \\\\ &amp;\\approx 0.8. \\end{align}\\] 3.3 Monty Hall problem The Monty Hall problem is a famous probability puzzle with non-intuitive outcome. Many established mathematicians and statisticians had problems solving it and many even disregarded the correct solution until they’ve seen the proof by simulation. Here we will show how it can be solved relatively simply with the use of Bayes’ theorem if we select the variables in a smart way. Exercise 31 (Monty Hall problem) A prize is placed at random behind one of three doors. You pick a door. Now Monty Hall chooses one of the other two doors, opens it and shows you that it is empty. He then gives you the opportunity to keep your door or switch to the other unopened door. Should you stay or switch? Use Bayes’ theorem to calculate the probability of winning if you switch and if you do not. R: Check your answers in R. Solution. W.L.O.G. assume we always pick the first door. The host can only open door 2 or door 3, as he can not open the door we picked. Let \\(k \\in \\{2,3\\}\\). Let us first look at what happens if we do not change. Then we have \\[\\begin{align} P(\\text{car in 1} | \\text{open $k$}) &amp;= \\frac{P(\\text{open $k$} | \\text{car in 1})P(\\text{car in 1})}{P(\\text{open $k$})} \\\\ &amp;= \\frac{P(\\text{open $k$} | \\text{car in 1})P(\\text{car in 1})}{\\sum_{n=1}^3 P(\\text{open $k$} | \\text{car in $n$})P(\\text{car in $n$)}}. \\end{align}\\] The probability that he opened \\(k\\) if the car is in 1 is \\(\\frac{1}{2}\\), as he can choose between door 2 and 3 as both have a goat behind it. Let us look at the normalization constant. When \\(n = 1\\) we get the value in the nominator. When \\(n=k\\), we get 0, as he will not open the door if there’s a prize behind. The remaining option is that we select 1, the car is behind \\(k\\) and he opens the only door left. Since he can’t open 1 due to it being our pick and \\(k\\) due to having the prize, the probability of opening the remaining door is 1, and the prior probability of the car being behind this door is \\(\\frac{1}{3}\\). So we have \\[\\begin{align} P(\\text{car in 1} | \\text{open $k$}) &amp;= \\frac{\\frac{1}{2}\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} \\\\ &amp;= \\frac{1}{3}. \\end{align}\\] Now let us look at what happens if we do change. Let \\(k&#39; \\in \\{2,3\\}\\) be the door that is not opened. If we change, we select this door, so we have \\[\\begin{align} P(\\text{car in $k&#39;$} | \\text{open $k$}) &amp;= \\frac{P(\\text{open $k$} | \\text{car in $k&#39;$})P(\\text{car in $k&#39;$})}{P(\\text{open $k$})} \\\\ &amp;= \\frac{P(\\text{open $k$} | \\text{car in $k&#39;$})P(\\text{car in $k&#39;$})}{\\sum_{n=1}^3 P(\\text{open $k$} | \\text{car in $n$})P(\\text{car in $n$)}}. \\end{align}\\] The denominator stays the same, the only thing that is different from before is \\(P(\\text{open $k$} | \\text{car in $k&#39;$})\\). We have a situation where we initially selected door 1 and the car is in door \\(k&#39;\\). The probability that the host will open door \\(k\\) is then 1, as he can not pick any other door. So we have \\[\\begin{align} P(\\text{car in $k&#39;$} | \\text{open $k$}) &amp;= \\frac{\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} \\\\ &amp;= \\frac{2}{3}. \\end{align}\\] Therefore it makes sense to change the door. set.seed(1) nsamps &lt;- 1000 ifchange &lt;- vector(mode = &quot;logical&quot;, length = nsamps) ifstay &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:nsamps) { where_car &lt;- sample(c(1:3), 1) where_player &lt;- sample(c(1:3), 1) open_samp &lt;- (1:3)[where_car != (1:3) &amp; where_player != (1:3)] if (length(open_samp) == 1) { where_open &lt;- open_samp } else { where_open &lt;- sample(open_samp, 1) } ifstay[i] &lt;- where_car == where_player where_ifchange &lt;- (1:3)[where_open != (1:3) &amp; where_player != (1:3)] ifchange[i] &lt;- where_ifchange == where_car } sum(ifstay) / nsamps ## [1] 0.333 sum(ifchange) / nsamps ## [1] 0.667 "],
["rvs.html", "4 Random variables 4.1 General properties and calculations 4.2 Discrete random variables 4.3 Continuous random variables 4.4 Singular random variables 4.5 Transformations", " 4 Random variables This chapter deals with random variables and their distributions. The students are expected to acquire the following knowledge: Theoretical Identification of random variables. Convolutions of random variables. Derivation of PDF, PMF, CDF, and quantile function. Definitions and properties of common discrete random variables. Definitions and properties of common continuous random variables. Transforming univariate random variables. R Familiarize with PDF, PMF, CDF, and quantile functions for several distributions. Visual inspection of probability distributions. Analytical and empirical calculation of probabilities based on distributions. New R functions for plotting (for example, facet_wrap). Creating random number generators based on the Uniform distribution. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 4.1 General properties and calculations Exercise 32 Which of the functions below are valid CDFs? Find their respective densities. R: Plot the three functions. \\[\\begin{equation} F(x) = \\begin{cases} 1 - e^{-x^2} &amp; x \\geq 0 \\\\ 0 &amp; x &lt; 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} e^{-\\frac{1}{x}} &amp; x &gt; 0 \\\\ 0 &amp; x \\leq 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x \\leq 0 \\\\ \\frac{1}{3} &amp; 0 &lt; x \\leq \\frac{1}{2} \\\\ 1 &amp; x &gt; \\frac{1}{2}. \\end{cases} \\end{equation}\\] Solution. Yes. First, let us check the limits. \\(\\lim_{x \\rightarrow -\\infty} (0) = 0\\). \\(\\lim_{x \\rightarrow \\infty} (1 - e^{-x^2}) = 1 - \\lim_{x \\rightarrow \\infty} e^{-x^2} = 1 - 0 = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(1 - e^{-x^2} \\geq 1 - e^{-y^2}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} 1 - e^{-\\epsilon^2} = 1 - \\lim_{\\epsilon \\downarrow 0} e^{-\\epsilon^2} = 1 - 1 = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} 1 - e^{-x^2} = 2xe^{-x^2}.\\) Students are encouraged to check that this is a proper PDF. Yes. First, let us check the limits. $_{x -} (0) = 0 and \\(\\lim_{x \\rightarrow \\infty} (e^{-\\frac{1}{x}}) = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(e^{-\\frac{1}{x}} \\geq e^{-\\frac{1}{y}}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} e^{-\\frac{1}{\\epsilon}} = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} e^{-\\frac{1}{x}} = \\frac{1}{x^2}e^{-\\frac{1}{x}}.\\) Students are encouraged to check that this is a proper PDF. No. The function is not right continuous as \\(F(\\frac{1}{2}) = \\frac{1}{3}\\), but \\(\\lim_{\\epsilon \\downarrow 0} F(\\frac{1}{2} + \\epsilon) = 1\\). f1 &lt;- function (x) { tmp &lt;- 1 - exp(-x^2) tmp[x &lt; 0] &lt;- 0 return(tmp) } f2 &lt;- function (x) { tmp &lt;- exp(-(1 / x)) tmp[x &lt;= 0] &lt;- 0 return(tmp) } f3 &lt;- function (x) { tmp &lt;- x tmp[x == x] &lt;- 1 tmp[x &lt;= 0.5] &lt;- 1/3 tmp[x &lt;= 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 20, by = 0.001), f1 = f1(x), f2 = f2(x), f3 = f3(x)) %&gt;% melt(id.vars = &quot;x&quot;) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = value, color = variable)) + geom_hline(yintercept = 1) + geom_line() plot(cdf_plot) Exercise 33 Let \\(X\\) be a random variable with CDF \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\frac{x^2}{2} &amp; 0 \\leq x &lt; 1 \\\\ \\frac{1}{2} + \\frac{p}{2} &amp; 1 \\leq x &lt; 2 \\\\ \\frac{1}{2} + \\frac{p}{2} + \\frac{1 - p}{2} &amp; x \\geq 2 \\end{cases} \\end{equation}\\] R: Plot this CDF for \\(p = 0.3\\). Is it a discrete, continuous, or mixed random varible? Find the probability density/mass of \\(X\\). f1 &lt;- function (x, p) { tmp &lt;- x tmp[x &gt;= 2] &lt;- 0.5 + (p * 0.5) + ((1-p) * 0.5) tmp[x &lt; 2] &lt;- 0.5 + (p * 0.5) tmp[x &lt; 1] &lt;- (x[x &lt; 1])^2 / 2 tmp[x &lt; 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 5, by = 0.001), y = f1(x, 0.3)) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = y)) + geom_hline(yintercept = 1) + geom_line(color = &quot;blue&quot;) plot(cdf_plot) Solution. \\(X\\) is a mixed random variable. Since \\(X\\) is a mixed random variable, we have to find the PDF of the continuous part and the PMF of the discrete part. We get the continuous part by differentiating the corresponding CDF, \\(\\frac{d}{dx}\\frac{x^2}{2} = x\\). So the PDF, when \\(0 \\leq x &lt; 1\\), is \\(p(x) = x\\). Let us look at the discrete part now. It has two steps, so this is a discrete distribution with two outcomes – numbers 1 and 2. The first happens with probability \\(\\frac{p}{2}\\), and the second with probability \\(\\frac{1 - p}{2}\\). This reminds us of the Bernoulli distribution. The PMF for the discrete part is \\(P(X = x) = (\\frac{p}{2})^{2 - x} (\\frac{1 - p}{2})^{x - 1}\\). Exercise 34 (Convolutions) Convolutions are probability distributions that correspond to sums of independent random variables. Let \\(X\\) and \\(Y\\) be independent discrete variables. Find the PMF of \\(Z = X + Y\\). Hint: Use the law of total probability. Let \\(X\\) and \\(Y\\) be independent continuous variables. Find the PDF of \\(Z = X + Y\\). Hint: Start with the CDF. Solution. \\[\\begin{align} P(Z = z) &amp;= P(X + Y = z) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + Y = z | Y = k) P(Y = k) &amp; \\text{ (law of total probability)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z | Y = k) P(Y = k) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z) P(Y = k) &amp; \\text{ (independence of $X$ and $Y$)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X = z - k) P(Y = k). &amp; \\end{align}\\] Let \\(f\\) and \\(g\\) be the PDFs of \\(X\\) and \\(Y\\) respectively. \\[\\begin{align} F(z) &amp;= P(Z &lt; z) \\\\ &amp;= P(X + Y &lt; z) \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + Y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X &lt; z - y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy \\end{align}\\] Now \\[\\begin{align} p(z) &amp;= \\frac{d}{dz} F(z) &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\frac{d}{dz}\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} f(z - y) g(y) dy &amp; \\text{ (fundamental theorem of calculus)}. \\end{align}\\] 4.2 Discrete random variables Exercise 35 (Binomial random variable) Let \\(X_k\\), \\(k = 1,...,n\\), be random variables with the Bernoulli measure as the PMF. Let \\(X = \\sum_{k=1}^n X_k\\). We call \\(X_k\\) a Bernoulli random variable with parameter \\(p \\in (0,1)\\). Find the CDF of \\(X_k\\). Find PMF of \\(X\\). This is a Binomial random variable with support in \\(\\{0,1,2,...,n\\}\\) and parameters \\(p \\in (0,1)\\) and \\(n \\in \\mathbb{N}_0\\). We denote \\[\\begin{equation} X | n,p \\sim \\text{binomial}(n,p). \\end{equation}\\] Find CDF of \\(X\\). R: Simulate from the binomial distribution with \\(n = 10\\) and \\(p = 0.5\\), and from \\(n\\) Bernoulli distributions with \\(p = 0.5\\). Visually compare the sum of Bernoullis and the binomial. Hint: there is no standard function like rpois for a Bernoulli random variable. Check exercise 11 to find out how to sample from a Bernoulli distribution. Solution. There are two outcomes – zero and one. Zero happens with probability \\(1 - p\\). Therefore \\[\\begin{equation} F(k) = \\begin{cases} 0 &amp; k &lt; 0 \\\\ 1 - p &amp; 0 \\leq k &lt; 1 \\\\ 1 &amp; k \\geq 1. \\end{cases} \\end{equation}\\] For the probability of \\(X\\) to be equal to some \\(k \\leq n\\), exactly \\(k\\) Bernoulli variables need to be one, and the others zero. So \\(p^k(1-p)^{n-k}\\). There are \\(\\binom{n}{k}\\) such possible arrangements. Therefore \\[\\begin{align} P(X = k) = \\binom{n}{k} p^k (1 - p)^{n-k}. \\end{align}\\] \\[\\begin{equation} F(k) = \\sum_{i = 0}^{\\lfloor k \\rfloor} \\binom{n}{i} p^i (1 - p)^{n - i} \\end{equation}\\] set.seed(1) nsamps &lt;- 10000 binom_samp &lt;- rbinom(nsamps, size = 10, prob = 0.5) bernoulli_mat &lt;- matrix(data = NA, nrow = nsamps, ncol = 10) for (i in 1:nsamps) { bernoulli_mat[i, ] &lt;- rbinom(10, size = 1, prob = 0.5) } bern_samp &lt;- apply(bernoulli_mat, 1, sum) b_data &lt;- tibble(x = c(binom_samp, bern_samp), type = c(rep(&quot;binomial&quot;, 10000), rep(&quot;Bernoulli_sum&quot;, 10000))) b_plot &lt;- ggplot(data = b_data, aes(x = x, fill = type)) + geom_bar(position = &quot;dodge&quot;) plot(b_plot) Exercise 36 (Geometric random variable) A variable with PMF \\[\\begin{equation} P(k) = p(1-p)^k \\end{equation}\\] is a geometric random variable with support in non-negative integers. It has one parameter \\(p \\in (0,1]\\). We denote \\[\\begin{equation} X | p \\sim \\text{geometric}(p) \\end{equation}\\] Derive the CDF of a geometric random variable. R: Draw 1000 samples from the geometric distribution with \\(p = 0.3\\) and compare their frequencies to theoretical values. Solution. \\[\\begin{align} P(X \\leq k) &amp;= \\sum_{i = 0}^k p(1-p)^i \\\\ &amp;= p \\sum_{i = 0}^k (1-p)^i \\\\ &amp;= p \\frac{1 - (1-p)^{k+1}}{1 - (1 - p)} \\\\ &amp;= 1 - (1-p)^{k + 1} \\end{align}\\] set.seed(1) geo_samp &lt;- rgeom(n = 1000, prob = 0.3) geo_samp &lt;- data.frame(x = geo_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dgeom(0:20, prob = 0.3), type = &quot;theoretical_measure&quot;)) geo_plot &lt;- ggplot(data = geo_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geo_plot) Exercise 37 (Poisson random variable) A variable with PMF \\[\\begin{equation} P(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\end{equation}\\] is a Poisson random variable with support in non-negative integers. It has one positive parameter \\(\\lambda\\), which also represents its mean value and variance (a measure of the deviation of the values from the mean – more on mean and variance in the next chapter). We denote \\[\\begin{equation} X | \\lambda \\sim \\text{Poisson}(\\lambda). \\end{equation}\\] This distribution is usually the default choice for modeling counts. We have already encountered a Poisson random variable in exercise 12, where we also sampled from this distribution. The CDF of a Poisson random variable is \\(P(X &lt;= x) = e^{-\\lambda} \\sum_{i=0}^x \\frac{\\lambda^{i}}{i!}\\). R: Draw 1000 samples from the Poisson distribution with \\(\\lambda = 5\\) and compare their empirical cumulative distribution function with the theoretical CDF. set.seed(1) pois_samp &lt;- rpois(n = 1000, lambda = 5) pois_samp &lt;- data.frame(x = pois_samp) pois_plot &lt;- ggplot(data = pois_samp, aes(x = x, colour = &quot;ECDF&quot;)) + stat_ecdf(geom = &quot;step&quot;) + geom_step(data = tibble(x = 0:17, y = ppois(x, 5)), aes(x = x, y = y, colour = &quot;CDF&quot;)) + scale_colour_manual(&quot;Lgend title&quot;, values = c(&quot;black&quot;, &quot;red&quot;)) plot(pois_plot) Exercise 38 (Negative binomial random variable) A variable with PMF \\[\\begin{equation} p(k) = \\binom{k + r - 1}{k}(1-p)^r p^k \\end{equation}\\] is a negative binomial random variable with support in non-negative integers. It has two parameters \\(r &gt; 0\\) and \\(p \\in (0,1)\\). We denote \\[\\begin{equation} X | r,p \\sim \\text{NB}(r,p). \\end{equation}\\] Let us reparameterize the negative binomial distribution with \\(q = 1 - p\\). Find the PMF of \\(X \\sim \\text{NB}(1, q)\\). Do you recognize this distribution? Show that the sum of two negative binomial random variables with the same \\(p\\) is also a negative binomial random variable. Hint: Use the fact that the number of ways to place \\(n\\) indistinct balls into \\(k\\) boxes is \\(\\binom{n + k - 1}{n}\\). R: Draw samples from \\(X \\sim \\text{NB}(5, 0.4)\\) and \\(Y \\sim \\text{NB}(3, 0.4)\\). Draw samples from \\(Z = X + Y\\), where you use the parameters calculated in b). Plot both distributions, their sum, and \\(Z\\) using facet_wrap. Be careful, as R uses a different parameterization size=\\(r\\) and prob=\\(1 - p\\). Solution. \\[\\begin{align} P(X = k) &amp;= \\binom{k + 1 - 1}{k}q^1 (1-q)^k \\\\ &amp;= q(1-q)^k. \\end{align}\\] This is the geometric distribution. Let \\(X \\sim \\text{NB}(r_1, p)\\) and \\(Y \\sim \\text{NB}(r_2, p)\\). Let \\(Z = X + Y\\). \\[\\begin{align} P(Z = z) &amp;= \\sum_{k = 0}^{\\infty} P(X = z - k)P(Y = k), \\text{ if k &lt; 0, then the probabilities are 0} \\\\ &amp;= \\sum_{k = 0}^{z} P(X = z - k)P(Y = k), \\text{ if k &gt; z, then the probabilities are 0} \\\\ &amp;= \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k}(1 - p)^{r_1} p^{z - k} \\binom{k + r_2 - 1}{k}(1 - p)^{r_2} p^{k} &amp; \\\\ &amp;= \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}(1 - p)^{r_1 + r_2} p^{z} &amp; \\\\ &amp;= (1 - p)^{r_1 + r_2} p^{z} \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}&amp; \\end{align}\\] The part before the sum reminds us of the negative binomial distribution with parameters \\(r_1 + r_2\\) and \\(p\\). To complete this term to the negative binomial PMF we need \\(\\binom{z + r_1 + r_2 -1}{z}\\). So the only thing we need to prove is that the sum equals this term. Both terms in the sum can be interpreted as numbers of ways to place a number of balls into boxes. For the left term it is \\(z-k\\) balls into \\(r_1\\) boxes, and for the right \\(k\\) balls into \\(r_2\\) boxes. For each \\(k\\) we are distributing \\(z\\) balls in total. By summing over all \\(k\\), we actually get all the possible placements of \\(z\\) balls into \\(r_1 + r_2\\) boxes. Therefore \\[\\begin{align} P(Z = z) &amp;= (1 - p)^{r_1 + r_2} p^{z} \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}&amp; \\\\ &amp;= \\binom{z + r_1 + r_2 -1}{z} (1 - p)^{r_1 + r_2} p^{z}. \\end{align}\\] From this it also follows that the sum of geometric distributions with the same parameter is a negative binomial distribution. \\(Z \\sim \\text{NB}(8, 0.4)\\). set.seed(1) nsamps &lt;- 10000 x &lt;- rnbinom(nsamps, size = 5, prob = 0.6) y &lt;- rnbinom(nsamps, size = 3, prob = 0.6) xpy &lt;- x + y z &lt;- rnbinom(nsamps, size = 8, prob = 0.6) samps &lt;- tibble(x, y, xpy, z) samps &lt;- melt(samps) ggplot(data = samps, aes(x = value)) + geom_bar() + facet_wrap(~ variable) 4.3 Continuous random variables Exercise 39 (Exponential random variable) A variable \\(X\\) with PDF \\(\\lambda e^{-\\lambda x}\\) is an exponential random variable with support in non-negative real numbers. It has one positive parameter \\(\\lambda\\). We denote \\[\\begin{equation} X | \\lambda \\sim \\text{Exp}(\\lambda). \\end{equation}\\] Find the CDF of an exponential random variable. Find the quantile function of an exponential random variable. Calculate the probability \\(P(1 \\leq X \\leq 3)\\), where \\(X \\sim \\text{Exp(1.5)}\\). R: Check your answer to c) with a simulation (rexp). Plot the probability in a meaningful way. R: Implement PDF, CDF, and the quantile function and compare their values with corresponding R functions visually. Hint: use the size parameter to make one of the curves wider. Solution. \\[\\begin{align} F(x) &amp;= \\int_{0}^{x} \\lambda e^{-\\lambda t} dt \\\\ &amp;= \\lambda \\int_{0}^{x} e^{-\\lambda t} dt \\\\ &amp;= \\lambda (\\frac{1}{-\\lambda}e^{-\\lambda t} |_{0}^{x}) \\\\ &amp;= \\lambda(\\frac{1}{\\lambda} - \\frac{1}{\\lambda} e^{-\\lambda x}) \\\\ &amp;= 1 - e^{-\\lambda x}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(x)) &amp;= x \\\\ 1 - e^{-\\lambda F^{-1}(x)} &amp;= x \\\\ e^{-\\lambda F^{-1}(x)} &amp;= 1 - x \\\\ -\\lambda F^{-1}(x) &amp;= \\ln(1 - x) \\\\ F^{-1}(x) &amp;= - \\frac{ln(1 - x)}{\\lambda}. \\end{align}\\] \\[\\begin{align} P(1 \\leq X \\leq 3) &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= 1 - e^{-1.5 \\times 3} - 1 + e^{-1.5 \\times 1} \\\\ &amp;\\approx 0.212. \\end{align}\\] set.seed(1) nsamps &lt;- 1000 samps &lt;- rexp(nsamps, rate = 1.5) sum(samps &gt;= 1 &amp; samps &lt;= 3) / nsamps ## [1] 0.212 exp_plot &lt;- ggplot(data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5)) + stat_function(fun = dexp, args = list(rate = 1.5), xlim = c(1,3), geom = &quot;area&quot;, fill = &quot;red&quot;) plot(exp_plot) exp_pdf &lt;- function(x, lambda) { return (lambda * exp(-lambda * x)) } exp_cdf &lt;- function(x, lambda) { return (1 - exp(-lambda * x)) } exp_quant &lt;- function(q, lambda) { return (-(log(1 - q) / lambda)) } ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_pdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = pexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_cdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = qexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_quant, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) Exercise 40 (Uniform random variable) Continuous uniform random variable with parameters \\(a\\) and \\(b\\) has the PDF \\[\\begin{equation} p(x) = \\begin{cases} \\frac{1}{b - a} &amp; x \\in [a,b] \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation}\\] Find the CDF of the uniform random variable. Find the quantile function of the uniform random variable. Let \\(X \\sim \\text{Uniform}(a,b)\\). Find the CDF of the variable \\(Y = \\frac{X - a}{b - a}\\). This is the standard uniform random variable. Let \\(X \\sim \\text{Uniform}(-1, 3)\\). Find such \\(z\\) that \\(P(X &lt; z + \\mu_x) = \\frac{1}{5}\\). R: Check your result from d) using simulation. Solution. a. \\[\\begin{align} F(x) &amp;= \\int_{a}^x \\frac{1}{b - a} dt \\\\ &amp;= \\frac{1}{b - a} \\int_{a}^x dt \\\\ &amp;= \\frac{x - a}{b - a}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(p)) &amp;= p \\\\ \\frac{F^{-1}(p) - a}{b - a} &amp;= p \\\\ F^{-1}(p) &amp;= p(b - a) + a. \\end{align}\\] \\[\\begin{align} F_Y(y) &amp;= P(Y &lt; y) \\\\ &amp;= P(\\frac{X - a}{b - a} &lt; y) \\\\ &amp;= P(X &lt; y(b - a) + a) \\\\ &amp;= F_X(y(b - a) + a) \\\\ &amp;= \\frac{(y(b - a) + a) - a}{b - a} \\\\ &amp;= y. \\end{align}\\] \\[\\begin{align} P(X &lt; z + 1) &amp;= \\frac{1}{5} \\\\ F(z + 1) &amp;= \\frac{1}{5} \\\\ z + 1 &amp;= F^{-1}(\\frac{1}{5}) \\\\ z &amp;= \\frac{1}{5}4 - 1 - 1 \\\\ z &amp;= -1.2. \\end{align}\\] set.seed(1) a &lt;- -1 b &lt;- 3 nsamps &lt;- 10000 unif_samp &lt;- runif(nsamps, a, b) mu_x &lt;- mean(unif_samp) new_samp &lt;- unif_samp - mu_x quantile(new_samp, probs = 1/5) ## 20% ## -1.203192 punif(-0.2, -1, 3) ## [1] 0.2 Exercise 41 (Beta random variable) A variable \\(X\\) with PDF \\[\\begin{equation} p(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}, \\end{equation}\\] where \\(\\text{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) and \\(\\Gamma(x) = \\int_0^{\\infty} x^{z - 1} e^{-x} dx\\) is a Beta random variable with support on \\([0,1]\\). It has two positive parameters \\(\\alpha\\) and \\(\\beta\\). Notation: \\[\\begin{equation} X | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta) \\end{equation}\\] It is often used in modeling rates. Calculate the PDF for \\(\\alpha = 1\\) and \\(\\beta = 1\\). What do you notice? R: Plot densities of the beta distribution for parameter pairs (2, 2), (4, 1), (1, 4), (2, 5), and (0.1, 0.1). R: Sample from \\(X \\sim \\text{Beta}(2, 5)\\) and compare the histogram with Beta PDF. Solution. \\[\\begin{equation} p(x) = \\frac{x^{1 - 1} (1 - x)^{1 - 1}}{\\text{B}(1, 1)} = 1. \\end{equation}\\] This is the standard uniform distribution. set.seed(1) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), aes(color = &quot;alpha = 0.5&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 4, shape2 = 1), aes(color = &quot;alpha = 4&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 4), aes(color = &quot;alpha = 1&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5), aes(color = &quot;alpha = 25&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 0.1, shape2 = 0.1), aes(color = &quot;alpha = 0.1&quot;)) set.seed(1) nsamps &lt;- 1000 samps &lt;- rbeta(nsamps, 2, 5) ggplot(data = data.frame(x = samps), aes(x = x)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;) + stat_function(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x), fun = dbeta, args = list(shape1 = 2, shape2 = 5), color = &quot;red&quot;, size = 1.2) Exercise 42 (Gamma random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\beta x} \\end{equation}\\] is a Gamma random variable with support on the positive numbers and parameters shape \\(\\alpha &gt; 0\\) and rate \\(\\beta &gt; 0\\). We write \\[\\begin{equation} X | \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta) \\end{equation}\\] and it’s CDF is \\[\\begin{equation} \\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}, \\end{equation}\\] where \\(\\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt\\). It is usually used in modeling positive phenomena (for example insurance claims and rainfalls). Let \\(X \\sim \\text{Gamma}(1, \\beta)\\). Find the PDF of \\(X\\). Do you recognize this PDF? Let \\(k = \\alpha\\) and \\(\\theta = \\frac{1}{\\beta}\\). Find the PDF of \\(X | k, \\theta \\sim \\text{Gamma}(k, \\theta)\\). Random variables can be reparameterized, and sometimes a reparameterized distribution is more suitable for certain calculations. The first parameterization is for example usually used in Bayesian statistics, while this parameterization is more common in econometrics and some other applied fields. Note that you also need to pay attention to the parameters in statistical software, so diligently read the help files when using functions like rgamma to see how the function is parameterized. R: Plot gamma CDF for random variables with shape and rate parameters (1,1), (10,1), (1,10). Solution. \\[\\begin{align} p(x) &amp;= \\frac{\\beta^1}{\\Gamma(1)} x^{1 - 1}e^{-\\beta x} \\\\ &amp;= \\beta e^{-\\beta x} \\end{align}\\] This is the PDF of the exponential distribution with parameter \\(\\beta\\). \\[\\begin{align} p(x) &amp;= \\frac{1}{\\Gamma(k)\\beta^k} x^{k - 1}e^{-\\frac{x}{\\theta}}. \\end{align}\\] set.seed(1) ggplot(data = data.frame(x = seq(0, 25, by = 0.01)), aes(x = x)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 1), aes(color = &quot;Gamma(1,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 10, rate = 1), aes(color = &quot;Gamma(10,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 10), aes(color = &quot;Gamma(1,10)&quot;)) Exercise 43 (Normal random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\end{equation}\\] is a normal random variable with support on the real axis and parameters \\(\\mu\\) in reals and \\(\\sigma^2 &gt; 0\\). The first is the mean parameter and the second is the variance parameter. Many statistical methods assume a normal distribution. We denote \\[\\begin{equation} X | \\mu, \\sigma \\sim \\text{N}(\\mu, \\sigma^2), \\end{equation}\\] and it’s CDF is \\[\\begin{equation} F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(t - \\mu)^2}{2 \\sigma^2}} dt, \\end{equation}\\] which is intractable and is usually approximated. Due to its flexibility it is also one of the most researched distributions. For that reason statisticians often use transformations of variables or approximate distributions with the normal distribution. Show that a variable \\(\\frac{X - \\mu}{\\sigma} \\sim \\text{N}(0,1)\\). This transformation is called standardization, and \\(\\text{N}(0,1)\\) is a standard normal distribution. R: Plot the normal distribution with \\(\\mu = 0\\) and different values for the \\(\\sigma\\) parameter. R: The normal distribution provides a good approximation for the Poisson distribution with a large \\(\\lambda\\). Let \\(X \\sim \\text{Poisson}(50)\\). Approximate \\(X\\) with the normal distribution and compare its density with the Poisson histogram. What are the values of \\(\\mu\\) and \\(\\sigma^2\\) that should provide the best approximation? Note that R function rnorm takes standard deviation (\\(\\sigma\\)) as a parameter and not variance. Solution. \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= P(X &lt; \\sigma x + \\mu) \\\\ &amp;= F(\\sigma x + \\mu) \\\\ &amp;= \\int_{-\\infty}^{\\sigma x + \\mu} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(t - \\mu)^2}{2\\sigma^2}} dt \\end{align}\\] Now let \\(s = f(t) = \\frac{t - \\mu}{\\sigma}\\), then \\(ds = \\frac{dt}{\\sigma}\\) and \\(f(\\sigma x + \\mu) = x\\), so \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{s^2}{2}} ds. \\end{align}\\] There is no need to evaluate this integral, as we recognize it as the CDF of a normal distribution with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). set.seed(1) # b ggplot(data = data.frame(x = seq(-15, 15, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = &quot;sd = 1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 0.4), aes(color = &quot;sd = 0.1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;sd = 2&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 5), aes(color = &quot;sd = 5&quot;)) # c mean_par &lt;- 50 nsamps &lt;- 100000 pois_samps &lt;- rpois(nsamps, lambda = mean_par) norm_samps &lt;- rnorm(nsamps, mean = mean_par, sd = sqrt(mean_par)) my_plot &lt;- ggplot() + geom_bar(data = tibble(x = pois_samps), aes(x = x, y = (..count..)/sum(..count..))) + geom_density(data = tibble(x = norm_samps), aes(x = x), color = &quot;red&quot;) plot(my_plot) Exercise 44 (Logistic random variable) A logistic random variable has CDF \\[\\begin{equation} F(x) = \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}}, \\end{equation}\\] where \\(\\mu\\) is real and \\(s &gt; 0\\). The support is on the real axis. We denote \\[\\begin{equation} X | \\mu, s \\sim \\text{Logistic}(\\mu, s). \\end{equation}\\] The distribution of the logistic random variable resembles a normal random variable, however it has heavier tails. Find the PDF of a logistic random variable. R: Implement logistic PDF and CDF and visually compare both for \\(X \\sim \\text{N}(0, 1)\\) and \\(Y \\sim \\text{logit}(0, \\sqrt{\\frac{3}{\\pi^2}})\\). These distributions have the same mean and variance. Additionally, plot the same plot on the interval [5,10], to better see the difference in the tails. R: For the distributions in b) find the probability \\(P(|X| &gt; 4)\\) and interpret the result. Solution. \\[\\begin{align} p(x) &amp;= \\frac{d}{dx} \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}} \\\\ &amp;= \\frac{- \\frac{d}{dx} (1 + e^{-\\frac{x - \\mu}{s}})}{(1 + e{-\\frac{x - \\mu}{s}})^2} \\\\ &amp;= \\frac{e^{-\\frac{x - \\mu}{s}}}{(1 + e{-\\frac{x - \\mu}{s}})^2}. \\end{align}\\] # b set.seed(1) logit_pdf &lt;- function (x, mu, s) { return ((exp(-(x - mu)/(s))) / (s * (1 + exp(-(x - mu)/(s)))^2)) } nl_plot &lt;- ggplot(data = data.frame(x = seq(-12, 12, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(12/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) nl_plot &lt;- ggplot(data = data.frame(x = seq(5, 10, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(12/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) # c logit_cdf &lt;- function (x, mu, s) { return (1 / (1 + exp(-(x - mu) / s))) } p_logistic &lt;- 1 - logit_cdf(4, 0, sqrt(12/pi^2)) + logit_cdf(-4, 0, sqrt(12/pi^2)) p_norm &lt;- 1 - pnorm(4, 0, 2) + pnorm(-4, 0, 2) p_logistic ## [1] 0.05178347 p_norm ## [1] 0.04550026 # Logistic distribution has wider tails, therefore the probability of larger # absolute values is higher. 4.4 Singular random variables Exercise 45 (Cantor distribution) The Cantor set is a subset of \\([0,1]\\), which we create by iteratively deleting the middle third of the interval. For example, in the first iteration, we get the sets \\([0,\\frac{1}{3}]\\) and \\([\\frac{2}{3},1]\\). In the second iteration, we get \\([0,\\frac{1}{9}]\\), \\([\\frac{2}{9},\\frac{1}{3}]\\), \\([\\frac{2}{3}, \\frac{7}{9}]\\), and \\([\\frac{8}{9}, 1]\\). On the \\(n\\)-th iteration, we have \\[\\begin{equation} C_n = \\frac{C_{n-1}}{3} \\cup \\bigg(\\frac{2}{3} + \\frac{C_{n-1}}{3} \\bigg), \\end{equation}\\] where \\(C_0 = [0,1]\\). The Cantor set is then defined as the intersection of these sets \\[\\begin{equation} C = \\cap_{n=1}^{\\infty} C_n. \\end{equation}\\] It has the same cardinality as \\([0,1]\\). Another way to define the Cantor set is the set of all numbers on \\([0,1]\\), that do not have a 1 in the ternary representation \\(x = \\sum_{n=1}^\\infty \\frac{x_i}{3^i}, x_i \\in \\{0,1,2\\}\\). A random variable follows the Cantor distribution, if its CDF is the Cantor function (below). You can find the implementations of random number generator, CDF, and quantile functions for the Cantor distributions at https://github.com/Henrygb/CantorDist.R. Show that the Lebesgue measure of the Cantor set is 0. (Jagannathan) Let us look at an infinite sequence of independent fair-coin tosses. If the outcome is heads, let \\(x_i = 2\\) and \\(x_i = 0\\), when tails. Then use these to create \\(x = \\sum_{n=1}^\\infty \\frac{x_i}{3^i}\\). This is a random variable with the Cantor distribution. Show that \\(X\\) has a singular distribution. Solution. \\[\\begin{align} \\lambda(C) &amp;= 1 - \\lambda(C^c) \\\\ &amp;= 1 - \\frac{1}{3}\\sum_{k = 0}^\\infty (\\frac{2}{3})^k \\\\ &amp;= 1 - \\frac{\\frac{1}{3}}{1 - \\frac{2}{3}} \\\\ &amp;= 0. \\end{align}\\] First, for every \\(x\\), the probability of observing it is \\(\\lim_{n \\rightarrow \\infty} \\frac{1}{2^n} = 0\\). Second, the probability that we observe one of all the possible sequences is 1. Therefore \\(P(C) = 1\\). So this is a singular variable. The CDF only increments on the elements of the Cantor set. 4.5 Transformations Exercise 46 Let \\(X\\) be a random variable that is uniformly distributed on \\(\\{-2, -1, 0, 1, 2\\}\\). Find the PMF of \\(Y = X^2\\). Solution. \\[\\begin{align} P_Y(y) = \\sum_{x \\in \\sqrt(y)} P_X(x) = \\begin{cases} 0 &amp; y \\notin \\{0,1,4\\} \\\\ \\frac{1}{5} &amp; y = 0 \\\\ \\frac{2}{5} &amp; y \\in \\{1,4\\} \\end{cases} \\end{align}\\] Exercise 47 (Lognormal random variable) A lognormal random variable is a variable whose logarithm is normally distributed. In practice, we often encounter skewed data. Usually using a log transformation on such data makes it more symmetric and therefore more suitable for modeling with the normal distribution (more on why we wish to model data with the normal distribution in the following chapters). Let \\(X \\sim \\text{N}(\\mu,\\sigma)\\). Find the PDF of \\(Y: \\log(Y) = X\\). R: Sample from the lognormal distribution with parameters \\(\\mu = 5\\) and \\(\\sigma = 2\\). Plot a histogram of the samples. Then log-transform the samples and plot a histogram along with the theoretical normal PDF. Solution. \\[\\begin{align} p_Y(y) &amp;= p_X(\\log(y)) \\frac{d}{dy} \\log(y) \\\\ &amp;= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(\\log(y) - \\mu)^2}{2 \\sigma^2}} \\frac{1}{y} \\\\ &amp;= \\frac{1}{y \\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(\\log(y) - \\mu)^2}{2 \\sigma^2}}. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 mu &lt;- 0.5 sigma &lt;- 0.4 ln_samps &lt;- rlnorm(nsamps, mu, sigma) ln_plot &lt;- ggplot(data = data.frame(x = ln_samps), aes(x = x)) + geom_histogram(color = &quot;black&quot;) plot(ln_plot) norm_samps &lt;- log(ln_samps) n_plot &lt;- ggplot(data = data.frame(x = norm_samps), aes(x = x)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;) + stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;red&quot;) plot(n_plot) Exercise 48 (Probability integral transform) This exercise is borrowed from Wasserman. Let \\(X\\) have a continuous, strictly increasing CDF \\(F\\). Let \\(Y = F(X)\\). Find the density of \\(Y\\). This is called the probability integral transform. Let \\(U \\sim \\text{Uniform}(0,1)\\) and let \\(X = F^{-1}(U)\\). Show that \\(X \\sim F\\). R: Implement a program that takes Uniform(0,1) random variables and generates random variables from an exponential(\\(\\beta\\)) distribution. Compare your implemented function with function rexp in R. Solution. \\[\\begin{align} F_Y(y) &amp;= P(Y &lt; y) \\\\ &amp;= P(F(X) &lt; y) \\\\ &amp;= P(X &lt; F_X^{-1}(y)) \\\\ &amp;= F_X(F_X^{-1}(y)) \\\\ &amp;= y. \\end{align}\\] From the above it follows that \\(p(y) = 1\\). Note that we need to know the inverse CDF to be able to apply this procedure. \\[\\begin{align} P(X &lt; x) &amp;= P(F^{-1}(U) &lt; x) \\\\ &amp;= P(U &lt; F(x)) \\\\ &amp;= F_U(F(x)) \\\\ &amp;= F(x). \\end{align}\\] set.seed(1) nsamps &lt;- 10000 beta &lt;- 4 generate_exp &lt;- function (n, beta) { tmp &lt;- runif(n) X &lt;- qexp(tmp, beta) return (X) } df &lt;- tibble(&quot;R&quot; = rexp(nsamps, beta), &quot;myGenerator&quot; = generate_exp(nsamps, beta)) %&gt;% gather() ggplot(data = df, aes(x = value, fill = key)) + geom_histogram(position = &quot;dodge&quot;) "],
["mrvs.html", "5 Multiple random variables 5.1 General 5.2 Bivariate distribution examples 5.3 Transformations", " 5 Multiple random variables This chapter deals with multiple random variables and their distributions. The students are expected to acquire the following knowledge: Theoretical Calculation of PDF of transformed multiple random variables. Finding marginal and conditional distributions. R Scatterplots of bivariate random variables. New R functions (for example, expand.grid). .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 5.1 General Exercise 49 Let \\(X \\sim \\text{N}(0,1)\\) and \\(Y \\sim \\text{N}(0,1)\\) be independent random variables. Draw 1000 samples from \\((X,Y)\\) and plot a scatterplot. Now let \\(X \\sim \\text{N}(0,1)\\) and $Y | X = x N(ax, 1). Draw 1000 samples from \\((X,Y)\\) for \\(a = 1\\), \\(a=0\\), and \\(a=-0.5\\). Plot the scatterplots. How would you interpret parameter \\(a\\)? Plot the marginal distribution of \\(Y\\) for cases \\(a=1\\), \\(a=0\\), and \\(a=-0.5\\). Can you guess which distribution it is? set.seed(1) nsamps &lt;- 1000 x &lt;- rnorm(nsamps) y &lt;- rnorm(nsamps) ggplot(data.frame(x, y), aes(x = x, y = y)) + geom_point() y1 &lt;- rnorm(nsamps, mean = 1 * x) y2 &lt;- rnorm(nsamps, mean = 0 * x) y3 &lt;- rnorm(nsamps, mean = -0.5 * x) df &lt;- tibble(x = c(x,x,x), y = c(y1,y2,y3), a = c(rep(1, nsamps), rep(0, nsamps), rep(-0.5, nsamps))) ggplot(df, aes(x = x, y = y)) + geom_point() + facet_wrap(~a) + coord_equal(ratio=1) # Parameter a controls the scale of linear dependency between X and Y. ggplot(df, aes(x = y)) + geom_density() + facet_wrap(~a) 5.2 Bivariate distribution examples Exercise 50 (Discrete bivariate random variable) Let \\(X\\) represent the event that a die rolls an even number and let \\(Y\\) represent the event that a die rolls one, two, or a three. Find the marginal distributions of \\(X\\) and \\(Y\\). Find the PMF of \\((X,Y)\\). Find the CDF of \\((X,Y)\\). Find \\(P(X = 1 | Y = 1)\\). Solution. \\[\\begin{align} P(X = 1) = \\frac{1}{2} \\text{ and } P(X = 0) = \\frac{1}{2} \\\\ P(Y = 1) = \\frac{1}{2} \\text{ and } P(Y = 0) = \\frac{1}{2} \\\\ \\end{align}\\] \\[\\begin{align} P(X = 1, Y = 1) = \\frac{1}{6} \\\\ P(X = 1, Y = 0) = \\frac{2}{6} \\\\ P(X = 0, Y = 1) = \\frac{2}{6} \\\\ P(X = 0, Y = 0) = \\frac{1}{6} \\end{align}\\] \\[\\begin{align} P(X \\leq x, Y \\leq y) = \\begin{cases} \\frac{1}{6} &amp; x = 0, y = 0 \\\\ \\frac{3}{6} &amp; x \\neq y \\\\ 1 &amp; x = 1, y = 1 \\end{cases} \\end{align}\\] \\[\\begin{align} P(X = 1 | Y = 1) = \\frac{1}{3} \\end{align}\\] Exercise 51 (Continuous bivariate random variable) Let \\(p(x,y) = 6 (x - y)^2\\) be the PDF of a bivariate random variable \\((X,Y)\\), where both variables range from zero to one. Find CDF. Find marginal distributions. Find conditional distributions. R: Plot a grid of points and colour them by value – this can help us visualize the PDF. R: Implement a random number generator, which will generate numbers from \\((X,Y)\\) and visually check the results. R: Plot the marginal distribution of \\(Y\\) and the conditional distributions of \\(X | Y = y\\), where \\(y \\in \\{0, 0.1, 0.5\\}\\). Solution. \\[\\begin{align} F(x,y) &amp;= \\int_0^{x} \\int_0^{y} 6 (t - s)^2 ds dt\\\\ &amp;= 6 \\int_0^{x} \\int_0^{y} t^2 - 2ts + s^2 ds dt\\\\ &amp;= 6 \\int_0^{x} t^2y - ty^2 + \\frac{y^3}{3} dt \\\\ &amp;= 6 (\\frac{x^3 y}{3} - \\frac{x^2y^2}{2} + \\frac{x y^3}{3}) \\\\ &amp;= 2 x^3 y - 3 t^2y^2 + 2 x y^3 \\end{align}\\] \\[\\begin{align} p(x) &amp;= \\int_0^{1} 6 (x - y)^2 dy\\\\ &amp;= 6 (x^2 - x + \\frac{1}{3}) \\\\ &amp;= 6x^2 - 6x + 2 \\end{align}\\] \\[\\begin{align} p(y) &amp;= \\int_0^{1} 6 (x - y)^2 dx\\\\ &amp;= 6 (y^2 - y + \\frac{1}{3}) \\\\ &amp;= 6y^2 - 6y + 2 \\end{align}\\] \\[\\begin{align} p(x|y) &amp;= \\frac{p(xy)}{p(y)} \\\\ &amp;= \\frac{6 (x - y)^2}{6 (y^2 - y + \\frac{1}{3})} \\\\ &amp;= \\frac{(x - y)^2}{y^2 - y + \\frac{1}{3}} \\end{align}\\] \\[\\begin{align} p(y|x) &amp;= \\frac{p(xy)}{p(x)} \\\\ &amp;= \\frac{6 (x - y)^2}{6 (x^2 - x + \\frac{1}{3})} \\\\ &amp;= \\frac{(x - y)^2}{x^2 - x + \\frac{1}{3}} \\end{align}\\] set.seed(1) # d pxy &lt;- function (x, y) { return ((x - y)^2) } x_axis &lt;- seq(0, 1, length.out = 100) y_axis &lt;- seq(0, 1, length.out = 100) df &lt;- expand.grid(x_axis, y_axis) colnames(df) &lt;- c(&quot;x&quot;, &quot;y&quot;) df &lt;- cbind(df, pdf = pxy(df$x, df$y)) ggplot(data = df, aes(x = x, y = y, color = pdf)) + geom_point() # e samps &lt;- NULL for (i in 1:10000) { xt &lt;- runif(1, 0, 1) yt &lt;- runif(1, 0, 1) pdft &lt;- pxy(xt, yt) acc &lt;- runif(1, 0, 6) if (acc &lt;= pdft) { samps &lt;- rbind(samps, c(xt, yt)) } } colnames(samps) &lt;- c(&quot;x&quot;, &quot;y&quot;) ggplot(data = as.data.frame(samps), aes(x = x, y = y)) + geom_point() # f mar_pdf &lt;- function (x) { return (6 * x^2 - 6 * x + 2) } cond_pdf &lt;- function (x, y) { return (((x - y)^2) / (y^2 - y + 1/3)) } df &lt;- tibble(x = x_axis, mar = mar_pdf(x), y0 = cond_pdf(x, 0), y0.1 = cond_pdf(x, 0.1), y0.5 = cond_pdf(x, 0.5)) %&gt;% gather(dist, value, -x) ggplot(df, aes(x = x, y = value, color = dist)) + geom_line() Exercise 52 (Mixed bivariate random variable) Let \\(f(x,y) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)y!} x^{y+ \\alpha -1} e^{-x(1 + \\beta)}\\) be the PDF of a bivariate random variable, where \\(x \\in (0, \\infty)\\) and \\(y \\in \\mathbb{N}_0\\). Find the marginal distribution of \\(X\\). Do you recognize this distribution? Find the conditional distribution of \\(Y | X\\). Do you recognize this distribution? Calculate the probability \\(P(Y = 2 | X = 5)\\) for \\((X,Y)\\). Find the marginal distribution of \\(Y\\). Do you recognize this distribution? R: Take 1000 random samples from \\((X,Y)\\) with parameters \\(\\beta = 1\\) and \\(\\alpha = 1\\). Plot a scatterplot. Plot a bar plot of the marginal distribution of \\(Y\\), and the theoretical PMF calculated from d) on the range from 0 to 10. Hint: Use the gamma function in R.? Solution. \\[\\begin{align} p(x) &amp;= \\sum_{k = 0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)k!} x^{k + \\alpha -1} e^{-x(1 + \\beta)} &amp; \\\\ &amp;= \\sum_{k = 0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)k!} x^{k} x^{\\alpha -1} e^{-x} e^{-\\beta x} &amp; \\\\ &amp;= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha -1} e^{-\\beta x} \\sum_{k = 0}^{\\infty} \\frac{1}{k!} x^{k} e^{-x} &amp; \\\\ &amp;= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha -1} e^{-\\beta x} &amp; \\text{the last term above sums to one} \\end{align}\\] This is the Gamma PDF. \\[\\begin{align} p(y|x) &amp;= \\frac{p(x,y)}{p(x)} \\\\ &amp;= \\frac{\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)y!} x^{y+ \\alpha -1} e^{-x(1 + \\beta)}}{\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha -1} e^{-\\beta x}} \\\\ &amp;= \\frac{x^y e^{-x}}{y!}. \\end{align}\\] This is the Poisson PMF. \\[\\begin{align} P(Y = 2 | X = 2.5) = \\frac{2.5^2 e^{-2.5}}{2!} \\approx 0.26. \\end{align}\\] \\[\\begin{align} p(y) &amp;= \\int_{0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)y!} x^{y + \\alpha -1} e^{-x(1 + \\beta)} dx &amp; \\\\ &amp;= \\frac{1}{y!} \\int_{0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{(y + \\alpha) -1} e^{-(1 + \\beta)x} dx &amp; \\\\ &amp;= \\frac{1}{y!} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty} \\frac{\\Gamma(y + \\alpha)}{(1 + \\beta)^{y + \\alpha}} \\frac{(1 + \\beta)^{y + \\alpha}}{\\Gamma(y + \\alpha)} x^{(y + \\alpha) -1} e^{-(1 + \\beta)x} dx &amp; \\text{complete to Gamma PDF} \\\\ &amp;= \\frac{1}{y!} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\frac{\\Gamma(y + \\alpha)}{(1 + \\beta)^{y + \\alpha}}. \\end{align}\\] We add the terms in the third equality to get a Gamma PDF inside the integral, which then integrates to one. We do not recognize this distribution. set.seed(1) px &lt;- function (x, alpha, beta) { return((1 / factorial(x)) * (beta^alpha / gamma(alpha)) * (gamma(x + alpha) / (1 + beta)^(x + alpha))) } nsamps &lt;- 1000 rx &lt;- rgamma(nsamps, 1, 1) ryx &lt;- rpois(nsamps, rx) ggplot(data = data.frame(x = rx, y = ryx), aes(x = x, y = y)) + geom_point() ggplot(data = data.frame(x = rx, y = ryx), aes(x = y)) + geom_bar(aes(y = (..count..)/sum(..count..))) + stat_function(fun = px, args = list(alpha = 1, beta = 1), color = &quot;red&quot;) Exercise 53 Let \\(f(x,y) = cx^2y\\) for \\(x^2 \\leq y \\leq 1\\) and zero otherwise. Find such \\(c\\) that \\(f\\) is a PDF of a bivariate random variable. This exercise is borrowed from Wasserman. Solution. \\[\\begin{align} 1 &amp;= \\int_{-1}^{1} \\int_{x^2}^1 cx^2y dy dx \\\\ &amp;= \\int_{-1}^{1} cx^2 (\\frac{1}{2} - \\frac{x^4}{2}) dx \\\\ &amp;= \\frac{c}{2} \\int_{-1}^{1} x^2 - x^6 dx \\\\ &amp;= \\frac{c}{2} (\\frac{1}{3} + \\frac{1}{3} - \\frac{1}{7} - \\frac{1}{7}) \\\\ &amp;= \\frac{c}{2} \\frac{8}{21} \\\\ &amp;= \\frac{4c}{21} \\end{align}\\] It follows \\(c = \\frac{21}{4}\\). 5.3 Transformations Exercise 54 Let \\((X,Y)\\) be uniformly distributed on the unit ball \\(\\{(x,y,z) : x^2 + y^2 + z^2 \\leq 1\\}\\). Let \\(R = \\sqrt{X^2 + Y^2 + Z^2}\\). Find the CDF and PDF of \\(R\\). Solution. \\[\\begin{align} P(R &lt; r) &amp;= P(\\sqrt{X^2 + Y^2 + Z^2} &lt; r) \\\\ &amp;= P(X^2 + Y^2 + Z^2 &lt; r^2) \\\\ &amp;= \\frac{\\frac{4}{3} \\pi r^3}{\\frac{4}{3}\\pi} \\\\ &amp;= r^3. \\end{align}\\] The second line shows us that we are looking at the probability which is represented by a smaller ball with radius \\(r\\). To get the probability, we divide it by the radius of the whole ball. We get the PDF by differentiating the CDF, so \\(p(r) = 3r^2\\). "],
["integ.html", "6 Integration 6.1 Monte Carlo integration 6.2 Lebesgue integrals", " 6 Integration This chapter deals with abstract and Monte Carlo integration. The students are expected to acquire the following knowledge: Theoretical How to calculate Lebesgue integrals for non-simple functions. R Monte Carlo integration. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 6.1 Monte Carlo integration Exercise 55 Let \\(X\\) and \\(Y\\) be continuous random variables on the unit interval and \\(p(x,y) = 6(x - y)^2\\). Use Monte Carlo integration to estimate the probability \\(P(0.2 \\leq X \\leq 0.5, \\: 0.1 \\leq Y \\leq 0.2)\\). Can you find the exact value? set.seed(1) nsamps &lt;- 1000 V &lt;- (0.5 - 0.2) * (0.2 - 0.1) x1 &lt;- runif(nsamps, 0.2, 0.5) x2 &lt;- runif(nsamps, 0.1, 0.2) f_1 &lt;- function (x, y) { return (6 * (x - y)^2) } mcint &lt;- V * (1 / nsamps) * sum(f_1(x1, x2)) mcint2 &lt;- V^2 * (1 / nsamps) * sum(f_1(x1, x2)^2) sdm &lt;- (mcint2 - mcint^2) / sqrt(nsamps) mcint ## [1] 0.008793445 sdm ## [1] 1.525797e-06 F_1 &lt;- function (x, y) { return (2 * x^3 * y - 3 * x^2 * y^2 + 2 * x * y^3) } F_1(0.5, 0.2) - F_1(0.2, 0.2) - F_1(0.5, 0.1) + F_1(0.2, 0.1) ## [1] 0.0087 6.2 Lebesgue integrals Exercise 56 (borrowed from Jagannathan) Find the Lebesgue integral of the following functions on (\\(\\mathbb{R}\\), \\(\\mathcal{B}(\\mathbb{R})\\), \\(\\lambda\\)). \\[\\begin{align} f(\\omega) = \\begin{cases} \\omega, &amp; \\text{for } \\omega = 0,1,...,n \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] \\[\\begin{align} f(\\omega) = \\begin{cases} 1, &amp; \\text{for } \\omega = \\mathbb{Q}^c \\cap [0,1] \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] \\[\\begin{align} f(\\omega) = \\begin{cases} n, &amp; \\text{for } \\omega = \\mathbb{Q}^c \\cap [0,n] \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] Solution. \\[\\begin{align} \\int f(\\omega) d\\lambda = \\sum_{\\omega = 0}^n \\omega \\lambda(\\omega) = 0. \\end{align}\\] \\[\\begin{align} \\int f(\\omega) d\\lambda = 1 \\times \\lambda(\\mathbb{Q}^c \\cap [0,1]) = 1. \\end{align}\\] \\[\\begin{align} \\int f(\\omega) d\\lambda = n \\times \\lambda(\\mathbb{Q}^c \\cap [0,n]) = n^2. \\end{align}\\] Exercise 57 (borrowed from Jagannathan) Let \\(c \\in \\mathbb{R}\\) be fixed and (\\(\\mathbb{R}\\), \\(\\mathcal{B}(\\mathbb{R})\\)) a measurable space. If for any Borel set \\(A\\), \\(\\delta_c (A) = 1\\) if \\(c \\in A\\), and \\(\\delta_c (A) = 0\\) otherwise, then \\(\\delta_c\\) is called a Dirac measure. Let \\(g\\) be a non-negative, measurable function. Show that \\(\\int g d \\delta_c = g(c)\\). Solution. \\[\\begin{align} \\int g d \\delta_c &amp;= \\sup_{q \\in S(g)} \\int q d \\delta_c \\\\ &amp;= \\sup_{q \\in S(g)} \\sum_{i = 1}^n a_i \\delta_c(A_i) \\\\ &amp;= \\sup_{q \\in S(g)} \\sum_{i = 1}^n a_i \\text{I}_{A_i}(c) \\\\ &amp;= \\sup_{q \\in S(g)} q(c) \\\\ &amp;= g(c) \\end{align}\\] "],
["ev.html", "7 Expected value 7.1 Discrete random variables 7.2 Continuous random variables 7.3 Sums, functions, conditional expectations 7.4 Covariance", " 7 Expected value This chapter deals with expected values of random variables. The students are expected to acquire the following knowledge: Theoretical Calculation of the expected value. Calculation of variance and covariance. Cauchy distribution. R Estimation of expected value. Estimation of variance and covariance. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 7.1 Discrete random variables Exercise 58 (Bernoulli) Let \\(X \\sim \\text{Bernoulli}(p)\\). Find \\(E[X]\\). Find \\(Var[X]\\). R: Let \\(p = 0.4\\). Check your answers to a) and b) with a simulation. Solution. \\[\\begin{align*} E[X] = \\sum_{k=0}^1 p^k (1-p)^{1-k} k = p. \\end{align*}\\] \\[\\begin{align*} Var[X] = E[X^2] - E[X]^2 = \\sum_{k=0}^1 (p^k (1-p)^{1-k} k^2) - p^2 = p(1-p). \\end{align*}\\] set.seed(1) nsamps &lt;- 1000 x &lt;- rbinom(nsamps, 1, 0.4) mean(x) ## [1] 0.394 var(x) ## [1] 0.239003 0.4 * (1 - 0.4) ## [1] 0.24 Exercise 59 (Binomial) Let \\(X \\sim \\text{Binomial}(n,p)\\). Find \\(E[X]\\). Find \\(Var[X]\\). Solution. Let \\(X = \\sum_{i=0}^n X_i\\), where \\(X_i \\sim \\text{Bernoulli}(p)\\). Then, due to linearity of expectation \\[\\begin{align*} E[X] = E[\\sum_{i=0}^n X_i] = \\sum_{i=0}^n E[X_i] = np. \\end{align*}\\] Again let \\(X = \\sum_{i=0}^n X_i\\), where \\(X_i \\sim \\text{Bernoulli}(p)\\). Since the Bernoulli variables \\(X_i\\) are independent we have \\[\\begin{align*} Var[X] = Var[\\sum_{i=0}^n X_i] = \\sum_{i=0}^n Var[X_i] = np(1-p). \\end{align*}\\] Exercise 60 (Poisson) Let \\(X \\sim \\text{Poisson}(\\lambda)\\). Find \\(E[X]\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\sum_{k=0}^\\infty \\frac{\\lambda^k e^{-\\lambda}}{k!} k &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!} &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!} &amp; \\text{term at $k=0$ is 0} \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^{k}}{k!} &amp; \\\\ &amp;= e^{-\\lambda} \\lambda e^\\lambda &amp; \\\\ &amp;= \\lambda. \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty k \\frac{\\lambda^{k-1}}{(k - 1)!} - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty (k - 1) + 1) \\frac{\\lambda^{k-1}}{(k - 1)!} - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\sum_{k=1}^\\infty (k - 1) \\frac{\\lambda^{k-1}}{(k - 1)!} + \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!}\\Big) - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\lambda\\sum_{k=2}^\\infty \\frac{\\lambda^{k-2}}{(k - 2)!} + e^\\lambda\\Big) - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\lambda e^\\lambda + e^\\lambda\\Big) - \\lambda^2 &amp; \\\\ &amp;= \\lambda^2 + \\lambda - \\lambda^2 &amp; \\\\ &amp;= \\lambda. \\end{align*}\\] Exercise 61 (Geometric) Let \\(X \\sim \\text{Geometric}(p)\\). Find \\(E[X]\\). Hint: \\(\\frac{d}{dx} x^k = k x^{(k - 1)}\\). Solution. \\[\\begin{align*} E[X] &amp;= \\sum_{k=0}^\\infty (1 - p)^k p k &amp; \\\\ &amp;= p (1 - p) \\sum_{k=0}^\\infty (1 - p)^{k-1} k &amp; \\\\ &amp;= p (1 - p) \\sum_{k=0}^\\infty -\\frac{d}{dp}(1 - p)^k &amp; \\\\ &amp;= p (1 - p) \\Big(-\\frac{d}{dp}\\Big) \\sum_{k=0}^\\infty (1 - p)^k &amp; \\\\ &amp;= p (1 - p) \\Big(-\\frac{d}{dp}\\Big) \\frac{1}{1 - (1 - p)} &amp; \\text{geometric series} \\\\ &amp;= \\frac{1 - p}{p} \\end{align*}\\] 7.2 Continuous random variables Exercise 62 (Gamma) Let \\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\). Hint: \\(\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t} dt\\) and \\(\\Gamma(z + 1) = z \\Gamma(z)\\). Find \\(E[X]\\). Find \\(Var[X]\\). R: Let \\(\\alpha = 10\\) and \\(\\beta = 2\\). Plot the density of \\(X\\). Add a horizontal line at the expected value that touches the density curve (geom_segment). Shade the area within a standard deviation of the expected value. Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^\\alpha e^{-\\beta x} dx &amp; \\\\ &amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} \\int_0^\\infty x^\\alpha e^{-\\beta x} dx &amp; \\text{ (let $t = \\beta x$)} \\\\ &amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) }\\int_0^\\infty \\frac{t^\\alpha}{\\beta^\\alpha} e^{-t} \\frac{dt}{\\beta} &amp; \\\\ &amp;= \\frac{1}{\\beta \\Gamma(\\alpha) }\\int_0^\\infty t^\\alpha e^{-t} dt &amp; \\\\ &amp;= \\frac{\\Gamma(\\alpha + 1)}{\\beta \\Gamma(\\alpha)} &amp; \\\\ &amp;= \\frac{\\alpha \\Gamma(\\alpha)}{\\beta \\Gamma(\\alpha)} &amp; \\\\ &amp;= \\frac{\\alpha}{\\beta}. &amp; \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha+1} e^{-\\beta x} dx - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\Gamma(\\alpha + 2)}{\\beta^2 \\Gamma(\\alpha)} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{(\\alpha + 1)\\alpha\\Gamma(\\alpha)}{\\beta^2 \\Gamma(\\alpha)} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\alpha^2 + \\alpha}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\alpha}{\\beta^2}. \\end{align*}\\] set.seed(1) x &lt;- seq(0, 25, by = 0.01) y &lt;- dgamma(x, shape = 10, rate = 2) df &lt;- data.frame(x = x, y = y) ggplot(df, aes(x = x, y = y)) + geom_line() + geom_segment(aes(x = 5, y = 0, xend = 5, yend = dgamma(5, shape = 10, rate = 2)), color = &quot;red&quot;) + stat_function(fun = dgamma, args = list(shape = 10, rate = 2), xlim = c(5 - sqrt(10/4), 5 + sqrt(10/4)), geom = &quot;area&quot;, fill = &quot;gray&quot;, alpha = 0.4) Exercise 63 (Beta) Let \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\). Find \\(E[X]\\). Hint 1: \\(\\text{B}(x,y) = \\int_0^1 t^{x-1} (1 - t)^{y-1} dt\\). Hint 2: \\(\\text{B}(x + 1, y) = \\text{B}(x,y)\\frac{x}{x + y}\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^1 \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)} x dx \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)}\\int_0^1 x^{\\alpha} (1 - x)^{\\beta - 1} dx \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 1, \\beta) \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha, \\beta) \\frac{\\alpha}{\\alpha + \\beta} \\\\ &amp;= \\frac{\\alpha}{\\alpha + \\beta}. \\\\ \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\int_0^1 \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)} x^2 dx - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)}\\int_0^1 x^{\\alpha + 1} (1 - x)^{\\beta - 1} dx - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 2, \\beta) - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 1, \\beta) \\frac{\\alpha + 1}{\\alpha + \\beta + 1} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{\\alpha + 1}{\\alpha + \\beta + 1} \\frac{\\alpha}{\\alpha + \\beta} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2}\\\\ &amp;= \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}. \\end{align*}\\] Exercise 64 (Exponential) Let \\(X \\sim \\text{Exp}(\\lambda)\\). Find \\(E[X]\\). Hint: \\(\\Gamma(z + 1) = z\\Gamma(z)\\) and \\(\\Gamma(1) = 1\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\lambda e^{-\\lambda x} x dx &amp; \\\\ &amp;= \\lambda \\int_0^\\infty x e^{-\\lambda x} dx &amp; \\\\ &amp;= \\lambda \\int_0^\\infty \\frac{t}{\\lambda} e^{-t} \\frac{dt}{\\lambda} &amp; \\text{$t = \\lambda x$}\\\\ &amp;= \\lambda \\lambda^{-2} \\Gamma(2) &amp; \\text{definition of gamma function} \\\\ &amp;= \\lambda^{-1}. \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 &amp; \\\\ &amp;= \\int_0^\\infty \\lambda e^{-\\lambda x} x^2 dx - \\lambda^{-2} &amp; \\\\ &amp;= \\lambda \\int_0^\\infty \\frac{t^2}{\\lambda^2} e^{-t} \\frac{dt}{\\lambda} - \\lambda^{-2} &amp; \\text{$t = \\lambda x$} \\\\ &amp;= \\lambda \\lambda^{-3} \\Gamma(3) - \\lambda^{-2} &amp; \\text{definition of gamma function} &amp; \\\\ &amp;= \\lambda^{-2} 2 \\Gamma(2) - \\lambda^{-2} &amp; \\\\ &amp;= 2 \\lambda^{-2} - \\lambda^{-2} &amp; \\\\ &amp;= \\lambda^{-2}. &amp; \\\\ \\end{align*}\\] Exercise 65 (Normal) Let \\(X \\sim \\text{N}(\\mu, \\sigma)\\). Show that \\(E[X] = \\mu\\). Hint: Use the error function \\(\\text{erf}(x) = \\frac{1}{\\sqrt(\\pi)} \\int_{-x}^x e^{-t^2} dt\\). The statistical interpretation of this function is that if \\(Y \\sim \\text{N}(0, 0.5)\\), then the error function describes the probability of \\(Y\\) falling between \\(-x\\) and \\(x\\). Show that \\(Var[X] = \\sigma^2\\). Hint: Start with the definition of variance. Solution. \\[\\begin{align*} E[X] &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} x dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty x e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty \\Big(t \\sqrt{2\\sigma^2} + \\mu\\Big)e^{-t^2} \\sqrt{2 \\sigma^2} dt &amp; t = \\frac{x - \\mu}{\\sqrt{2\\sigma}} \\\\ &amp;= \\frac{\\sqrt{2\\sigma^2}}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty t e^{-t^2} dt + \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty \\mu e^{-t^2} dt &amp; \\\\ \\end{align*}\\] Let us calculate these integrals separately. \\[\\begin{align*} \\int t e^{-t^2} dt &amp;= -\\frac{1}{2}\\int e^{s} ds &amp; s = -t^2 \\\\ &amp;= -\\frac{e^s}{2} + C \\\\ &amp;= -\\frac{e^{-t^2}}{2} + C &amp; \\text{undoing substitution}. \\end{align*}\\] Inserting the integration limits we get \\[\\begin{align*} \\int_{-\\infty}^\\infty t e^{-t^2} dt &amp;= 0, \\end{align*}\\] due to the integrated function being symmetric. Reordering the second integral we get \\[\\begin{align*} \\mu \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty e^{-t^2} dt &amp;= \\mu \\text{erf}(\\infty) &amp; \\text{definition of error function} \\\\ &amp;= \\mu &amp; \\text{probability of $Y$ falling between $-\\infty$ and $\\infty$}. \\end{align*}\\] Combining all of the above we get \\[\\begin{align*} E[X] &amp;= \\frac{\\sqrt{2\\sigma^2}}{\\sqrt{\\pi}} \\times 0 + \\mu &amp;= \\mu.\\\\ \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[(X - E[X])^2] \\\\ &amp;= E[(X - \\mu)^2] \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty (x - \\mu)^2 e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} dx \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty t^2 e^{-\\frac{t^2}{2}} dt \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\bigg(\\Big(- t e^{-\\frac{t^2}{2}} |_{-\\infty}^\\infty \\Big) + \\int_{-\\infty}^\\infty e^{-\\frac{t^2}{2}} \\bigg) dt &amp; \\text{integration by parts} \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\sqrt{2 \\pi} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt(\\pi)}e^{-s^2} \\bigg) &amp; s = \\frac{t}{\\sqrt{2}} \\text{ and evaluating the left expression at the bounds} \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\sqrt{2 \\pi} \\Big(\\text{erf}(\\infty) &amp; \\text{definition of error function} \\\\ &amp;= \\sigma^2. \\end{align*}\\] 7.3 Sums, functions, conditional expectations Exercise 66 (Expectation of transformations) Let \\(X\\) follow a normal distribution. Find \\(E[2X + 4]\\). Find \\(E[X^2]\\). Find \\(E[\\exp(X)]\\). R: Check your results numerically for \\(\\mu = 0.4\\) and \\(\\sigma^2 = 0.25\\) and plot the densities of all four distributions. Solution. \\[\\begin{align} E[2X + 4] &amp;= 2E[X] + 4 &amp; \\text{linearity of expectation} \\\\ &amp;= 2\\mu + 4. \\\\ \\end{align}\\] \\[\\begin{align} E[X^2] &amp;= E[X]^2 - Var[X] &amp; \\text{definition of variance} \\\\ &amp;= \\mu^2 + \\sigma^2. \\end{align}\\] \\[\\begin{align} E[\\exp(X)] &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} e^x dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty e^{\\frac{2 \\sigma^2 x}{2\\sigma^2} -\\frac{(x - \\mu)^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty e^{-\\frac{x^2 - 2x(\\mu + \\sigma^2) + \\mu^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty e^{-\\frac{(x - (\\mu + \\sigma^2))^2 + \\mu^2 - (\\mu + \\sigma^2)^2}{2\\sigma^2}} dx &amp; \\text{complete the square} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{- \\mu^2 + (\\mu + \\sigma^2)^2}{2\\sigma^2}} \\int_{-\\infty}^\\infty e^{-\\frac{(x - (\\mu + \\sigma^2))^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{- \\mu^2 + (\\mu + \\sigma^2)^2}{2\\sigma^2}} \\sigma \\sqrt{2 \\pi} \\text{erf}(\\infty) &amp; \\\\ &amp;= e^{\\frac{2\\mu + \\sigma^2}{2}}. \\end{align}\\] set.seed(1) mu &lt;- 0.4 sigma &lt;- 0.5 x &lt;- rnorm(100000, mean = mu, sd = sigma) mean(2*x + 4) ## [1] 4.797756 2 * mu + 4 ## [1] 4.8 mean(x^2) ## [1] 0.4108658 mu^2 + sigma^2 ## [1] 0.41 mean(exp(x)) ## [1] 1.689794 exp((2 * mu + sigma^2) / 2) ## [1] 1.690459 Exercise 67 (Sum of independent random variables) Borrowed from Wasserman. Let \\(X_1, X_2,...,X_n\\) be IID random variables with expected value \\(E[X_i] = \\mu\\) and variance \\(Var[X_i] = \\sigma^2\\). Find the expected value and variance of \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\). \\(\\bar{X}\\) is called a statistic (a function of the values in a sample). It is itself a random variable and its distribution is called a sampling distribution. R: Take \\(n = 5, 10, 100, 1000\\) samples from the N(\\(2\\), \\(6\\)) distribution 10000 times. Plot the theoretical density and the densities of \\(\\bar{X}\\) statistic for each \\(n\\). Intuitively, are the results in correspondence with your calculations? Check them numerically. Solution. Let us start with the expectation of \\(\\bar{X}\\). \\[\\begin{align} E[\\bar{X}] &amp;= E[\\frac{1}{n} \\sum_{i=1}^n X_i] &amp; \\\\ &amp;= \\frac{1}{n} E[\\sum_{i=1}^n X_i] &amp; \\text{ (multiplication with a scalar)} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n E[X_i] &amp; \\text{ (linearity)} \\\\ &amp;= \\frac{1}{n} n \\mu &amp; \\\\ &amp;= \\mu. \\end{align}\\] Now the variance \\[\\begin{align} Var[\\bar{X}] &amp;= Var[\\frac{1}{n} \\sum_{i=1}^n X_i] &amp; \\\\ &amp;= \\frac{1}{n^2} Var[\\sum_{i=1}^n X_i] &amp; \\text{ (multiplication with a scalar)} \\\\ &amp;= \\frac{1}{n^2} \\sum_{i=1}^n Var[X_i] &amp; \\text{ (independence of samples)} \\\\ &amp;= \\frac{1}{n^2} n \\sigma^2 &amp; \\\\ &amp;= \\frac{1}{n} \\sigma^2. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 mu &lt;- 2 sigma &lt;- sqrt(6) N &lt;- c(5, 10, 100, 500) X &lt;- matrix(data = NA, nrow = nsamps, ncol = length(N)) ind &lt;- 1 for (n in N) { for (i in 1:nsamps) { X[i,ind] &lt;- mean(rnorm(n, mu, sigma)) } ind &lt;- ind + 1 } colnames(X) &lt;- N X &lt;- melt(as.data.frame(X)) ggplot(data = X, aes(x = value, colour = variable)) + geom_density() + stat_function(data = data.frame(x = seq(-2, 6, by = 0.01)), aes(x = x), fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;black&quot;) Exercise 68 (Conditional expectation) Let \\(X \\in \\mathbb{R}_0^+\\) and \\(Y \\in \\mathbb{N}_0\\) be random variables with joint distribution \\(p_{XY}(X,Y) = \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}} 0.5^{y + 1}\\). Find \\(E[X | Y]\\) by first finding \\(p_Y\\) and then \\(p_{X|Y}\\). Find \\(E[X]\\). R: check your answers to a) and b) by drawing 10000 samples from \\(p_Y\\) and \\(p_{X|Y}\\). Solution. \\[\\begin{align} p(y) &amp;= \\int_0^\\infty \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}} 0.5^{y + 1} dx \\\\ &amp;= \\frac{0.5^{y + 1}}{y + 1} \\int_0^\\infty e^{-\\frac{x}{y + 1}} dx \\\\ &amp;= \\frac{0.5^{y + 1}}{y + 1} (y + 1) \\\\ &amp;= 0.5^{y + 1} \\\\ &amp;= 0.5(1 - 0.5)^y. \\end{align}\\] We recognize this as the geometric distribution. \\[\\begin{align} p(x|y) &amp;= \\frac{p(X,Y)}{P(Y)} \\\\ &amp;= \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}}. \\end{align}\\] We recognize this as the exponential distribution. \\[\\begin{align} E[X | Y] &amp;= \\int_0^\\infty x \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}} dx \\\\ &amp;= y + 1 &amp; \\text{expected value of the exponential distribution} \\end{align}\\] Use the law of iterated expectation. \\[\\begin{align} E[X] &amp;= E[E[X | Y]] \\\\ &amp;= E[Y + 1] \\\\ &amp;= E[Y] + 1 \\\\ &amp;= \\frac{1 - 0.5}{0.5} + 1 \\\\ &amp;= 2. \\end{align}\\] set.seed(1) y &lt;- rgeom(100000, 0.5) x &lt;- rexp(100000, rate = 1 / (y + 1)) x2 &lt;- x[y == 3] mean(x2) ## [1] 4.048501 3 + 1 ## [1] 4 mean(x) ## [1] 2.007639 (1 - 0.5) / 0.5 + 1 ## [1] 2 Exercise 69 (Cauchy distribution) Let \\(p(x | x_0, \\gamma) = \\frac{1}{\\pi \\gamma \\Big(1 + \\big(\\frac{x - x_0}{\\gamma}\\big)^2\\Big)}\\). A random variable with this PDF follows a Cauchy distribution. This distribution is symmetric and has wider tails than the normal distribution. R: Draw \\(n = 1,...,1000\\) samples from a standard normal and \\(\\text{Cauchy}(0, 1)\\). For each \\(n\\) plot the mean and the median of the sample using facets. Interpret the results. To get a mathematical explanation of the results in a), evaluate the integral \\(\\int_0^\\infty \\frac{x}{1 + x^2} dx\\) and consider that \\(E[X] = \\int_{-\\infty}^\\infty \\frac{x}{1 + x^2}dx\\). set.seed(1) n &lt;- 1000 means_n &lt;- vector(mode = &quot;numeric&quot;, length = n) means_c &lt;- vector(mode = &quot;numeric&quot;, length = n) medians_n &lt;- vector(mode = &quot;numeric&quot;, length = n) medians_c &lt;- vector(mode = &quot;numeric&quot;, length = n) for (i in 1:n) { tmp_n &lt;- rnorm(i) tmp_c &lt;- rcauchy(i) means_n[i] &lt;- mean(tmp_n) means_c[i] &lt;- mean(tmp_c) medians_n[i] &lt;- median(tmp_n) medians_c[i] &lt;- median(tmp_c) } df &lt;- data.frame(&quot;distribution&quot; = c(rep(&quot;normal&quot;, 2 * n), rep(&quot;Cauchy&quot;, 2 * n)), &quot;type&quot; = c(rep(&quot;mean&quot;, n), rep(&quot;median&quot;, n), rep(&quot;mean&quot;, n), rep(&quot;median&quot;, n)), &quot;value&quot; = c(means_n, medians_n, means_c, medians_c), &quot;n&quot; = rep(1:n, times = 4)) ggplot(df, aes(x = n, y = value)) + geom_line(alpha = 0.5) + facet_wrap(~ type + distribution , scales = &quot;free&quot;) Solution. b. \\[\\begin{align} \\int_0^\\infty \\frac{x}{1 + x^2} dx &amp;= \\frac{1}{2} \\int_1^\\infty \\frac{1}{u} du &amp; u = 1 + x^2 \\\\ &amp;= \\frac{1}{2} \\ln(x) |_0^\\infty. \\end{align}\\] This integral is not finite. The same holds for the negative part. Therefore, the expectation is undefined, as \\(E[|X|] = \\infty\\). See Riemann rearrangement theorem. 7.4 Covariance Exercise 70 Below is a table of values for random variables \\(X\\) and \\(Y\\). X Y 2.1 8 -0.5 11 1 10 -2 12 4 9 Find sample covariance of \\(X\\) and \\(Y\\). Find sample variances of \\(X\\) and \\(Y\\). Find sample correlation of \\(X\\) and \\(Y\\). Find sample variance of \\(Z = 2X - 3Y\\). Solution. a. \\(\\bar{X} = 0.92\\) and \\(\\bar{Y} = 10\\). \\[\\begin{align} s(X, Y) &amp;= \\frac{1}{n - 1} \\sum_{i=1}^5 (X_i - 0.92) (Y_i - 10) \\\\ &amp;= -3.175. \\end{align}\\] \\[\\begin{align} s(X) &amp;= \\frac{\\sum_{i=1}^5(X_i - 0.92)^2}{5 - 1} \\\\ &amp;= 5.357. \\end{align}\\] \\[\\begin{align} s(Y) &amp;= \\frac{\\sum_{i=1}^5(Y_i - 10)^2}{5 - 1} \\\\ &amp;= 2.5. \\end{align}\\] \\[\\begin{align} r(X,Y) &amp;= \\frac{Cov(X,Y)}{\\sqrt{Var[X]Var[Y]}} \\\\ &amp;= \\frac{-3.175}{\\sqrt{5.357 \\times 2.5}} \\\\ &amp;= -8.68. \\end{align}\\] \\[\\begin{align} s(Z) &amp;= 2^2 s(X) + 3^2 s(Y) + 2 \\times 2 \\times 3 s(X, Y) \\\\ &amp;= 4 \\times 5.357 + 9 \\times 2.5 + 12 \\times 3.175 \\\\ &amp;= 82.028. \\end{align}\\] Exercise 71 Let \\(X \\sim \\text{Uniform}(0,1)\\) and \\(Y | X = x \\sim \\text{Uniform(0,x)}\\). Find the covariance of \\(X\\) and \\(Y\\). Find the correlation of \\(X\\) and \\(Y\\). R: check your answers to a) and b) with simulation. Plot \\(X\\) against \\(Y\\) on a scatterplot. Solution. a. The joint PDF is \\(p(x,y) = p(x)p(y|x) = \\frac{1}{x}\\). \\[\\begin{align} Cov(X,Y) &amp;= E[XY] - E[X]E[Y] \\\\ \\end{align}\\] Let us first evaluate the first term: \\[\\begin{align} E[XY] &amp;= \\int_0^1 \\int_0^x x y \\frac{1}{x} dy dx \\\\ &amp;= \\int_0^1 \\int_0^x y dy dx \\\\ &amp;= \\int_0^1 \\frac{x^2}{2} dx \\\\ &amp;= \\frac{1}{6}. \\end{align}\\] Now let us find \\(E[Y]\\), \\(E[X]\\) is trivial. \\[\\begin{align} E[Y] = E[E[Y | X]] = E[\\frac{X}{2}] = \\frac{1}{2} \\int_0^1 x dx = \\frac{1}{4}. \\end{align}\\] Combining all: \\[\\begin{align} Cov(X,Y) &amp;= \\frac{1}{6} - \\frac{1}{2} \\frac{1}{4} = \\frac{1}{24}. \\end{align}\\] \\[\\begin{align} \\rho(X,Y) &amp;= \\frac{Cov(X,Y)}{\\sqrt{Var[X]Var[Y]}} \\\\ \\end{align}\\] Let us calculate \\(Var[X]\\). \\[\\begin{align} Var[X] &amp;= E[X^2] - \\frac{1}{4} \\\\ &amp;= \\int_0^1 x^2 - \\frac{1}{4} \\\\ &amp;= \\frac{1}{3} - \\frac{1}{4} \\\\ &amp;= \\frac{1}{12}. \\end{align}\\] Let us calculate \\(E[E[Y^2|X]]\\). \\[\\begin{align} E[E[Y^2|X]] &amp;= E[\\frac{x^2}{3}] \\\\ &amp;= \\frac{1}{9}. \\end{align}\\] Then \\(Var[Y] = \\frac{1}{9} - \\frac{1}{16} = \\frac{7}{144}\\). Combining all \\[\\begin{align} \\rho(X,Y) &amp;= \\frac{\\frac{1}{24}}{\\sqrt{\\frac{1}{12}\\frac{5}{144}}} \\\\ &amp;= 0.65. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 x &lt;- runif(nsamps) y &lt;- runif(nsamps, 0, x) cov(x, y) ## [1] 0.04274061 1/24 ## [1] 0.04166667 cor(x, y) ## [1] 0.6629567 (1 / 24) / (sqrt(7 / (12 * 144))) ## [1] 0.6546537 ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) "],
["mrv.html", "8 Multivariate random variables 8.1 Multinomial random variables 8.2 Multivariate normal random variables 8.3 Transformations", " 8 Multivariate random variables This chapter deals with multivariate random variables. The students are expected to acquire the following knowledge: Theoretical Multinomial distribution. Multivariate normal distribution. Cholesky decomposition. Eigendecomposition. R Sampling from the multinomial distribution. Sampling from the multivariate normal distribution. Matrix decompositions. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 8.1 Multinomial random variables Exercise 72 Let \\(X_i\\), \\(i = 1,...,k\\) represent \\(k\\) events, and \\(p_i\\) the probabilities of these events happening in a trial. Let \\(n\\) be the number of trials, and \\(X\\) a multivariate random variable, the collection of \\(X_i\\). Then \\(p(x) = \\frac{n!}{x_1!x_2!...x_k!} p_1^{x_1} p_2^{x_2}...p_k^{x_k}\\) is the PMF of a multinomial distribution, where \\(n = \\sum_{i = 1}^k x_i\\). Show that the marginal distribution of \\(X_i\\) is a binomial distribution. Take 1000 samples from the multinomial distribution with \\(n=4\\) and probabilities \\(p = (0.2, 0.2, 0.5, 0.1)\\). Then take 1000 samples from four binomial distributions with the same parameters. Inspect the results visually. Solution. We will approach this proof from the probabilistic point of view. W.L.O.G. let \\(x_1\\) be the marginal distribution we are interested in. The term \\(p^{x_1}\\) denotes the probability that event 1 happened \\(x_1\\) times. For this event not to happen, one of the other events needs to happen. So for each of the remaining trials, the probability of another event is \\(\\sum_{i=2}^k p_i = 1 - p_1\\), and there were \\(n - x_1\\) such trials. What is left to do is to calculate the number of permutations of event 1 happening and event 1 not happening. We choose \\(x_1\\) trials, from \\(n\\) trials. Therefore \\(p(x_1) = \\binom{n}{x_1} p_1^{x_1} (1 - p_1)^{n - x_1}\\), which is the binomial PMF. Interested students are encouraged to prove this mathematically. set.seed(1) nsamps &lt;- 1000 samps_mult &lt;- rmultinom(nsamps, 4, prob = c(0.2, 0.2, 0.5, 0.1)) samps_mult &lt;- as_tibble(t(samps_mult)) %&gt;% gather() samps &lt;- tibble( V1 = rbinom(nsamps, 4, 0.2), V2 = rbinom(nsamps, 4, 0.2), V3 = rbinom(nsamps, 4, 0.5), V4 = rbinom(nsamps, 4, 0.1) ) %&gt;% gather() %&gt;% bind_rows(samps_mult) %&gt;% bind_cols(&quot;dist&quot; = c(rep(&quot;binomial&quot;, 4*nsamps), rep(&quot;multinomial&quot;, 4*nsamps))) ggplot(samps, aes(x = value, fill = dist)) + geom_bar(position = &quot;dodge&quot;) + facet_wrap(~ key) Exercise 73 (Multinomial expected value) Find the expected value, variance and covariance of the multinomial distribution. Hint: First find the expected value for \\(n = 1\\) and then use the fact that the trials are independent. Solution. Let us first calculate the expected value of \\(X_1\\), when \\(n = 1\\). \\[\\begin{align} E[X_1] &amp;= \\sum_{n_1 = 0}^1 \\sum_{n_2 = 0}^1 ... \\sum_{n_k = 0}^1 \\frac{1}{n_1!n_2!...n_k!}p_1^{n_1}p_2^{n_2}...p_k^{n_k}n_1 \\\\ &amp;= \\sum_{n_1 = 0}^1 \\frac{p_1^{n_1} n_1}{n_1!} \\sum_{n_2 = 0}^1 ... \\sum_{n_k = 0}^1 \\frac{1}{n_2!...n_k!}p_2^{n_2}...p_k^{n_k} \\end{align}\\] When \\(n_1 = 0\\) then the whole terms is zero, so we do not need to evaluate other sums. When \\(n_1 = 1\\), all other \\(n_i\\) must be zero, as we have \\(1 = \\sum_{i=1}^k n_i\\). Therefore the other sums equal \\(1\\). So \\(E[X_1] = p_1\\) and \\(E[X_i] = p_i\\) for \\(i = 1,...,k\\). Now let \\(Y_j\\), \\(j = 1,...,n\\), have a multinomial distribution with \\(n = 1\\), and let \\(X\\) have a multinomial distribution with and arbitrary \\(n\\). Then we can write \\(X = \\sum_{j=1}^n Y_j\\). And due to independence \\[\\begin{align} E[X] &amp;= E[\\sum_{j=1}^n Y_j] \\\\ &amp;= \\sum_{j=1}^n E[Y_j] \\\\ &amp;= np. \\end{align}\\] For the variance, we need \\(E[X^2]\\). Let us follow the same procedure as above and first calculate \\(E[X_i]\\) for \\(n = 1\\). The only thing that changes is that the term \\(n_i\\) becomes \\(n_i^2\\). Since we only have \\(0\\) and \\(1\\) this does not change the outcome. So \\[\\begin{align} Var[X_i] &amp;= E[X_i^2] - E[X_i]^2\\\\ &amp;= p_i(1 - p_i). \\end{align}\\] Analogous to above for arbitrary \\(n\\) \\[\\begin{align} Var[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\sum_{j=1}^n E[Y_j^2] - \\sum_{j=1}^n E[Y_j]^2 \\\\ &amp;= \\sum_{j=1}^n E[Y_j^2] - E[Y_j]^2 \\\\ &amp;= \\sum_{j=1}^n p(1-p) \\\\ &amp;= np(1-p). \\end{align}\\] To calculate the covariance, we need \\(E[X_i X_j]\\). Again, let us start with \\(n = 1\\). Without loss of generality, let us assume \\(i = 1\\) and \\(j = 2\\). \\[\\begin{align} E[X_1 X_2] = \\sum_{n_1 = 0}^1 \\sum_{n_2 = 0}^1 \\frac{p_1^{n_1} n_1}{n_1!} \\frac{p_2^{n_2} n_2}{n_2!} \\sum_{n_3 = 0}^1 ... \\sum_{n_k = 0}^1 \\frac{1}{n_3!...n_k!}p_3^{n_3}...p_k^{n_k}. \\end{align}\\] In the above expression, at each iteration we multiply with \\(n_1\\) and \\(n_2\\). Since \\(n = 1\\), one of these always has to be zero. Therefore \\(E[X_1 X_2] = 0\\) and \\[\\begin{align} Cov(X_i, X_j) &amp;= E[X_i X_j] - E[X_i]E[X_j] \\\\ &amp;= - p_i p_j. \\end{align}\\] For arbitrary \\(n\\), let \\(X = \\sum_{t = 1}^n Y_t\\) be the sum of independent multinomial random variables \\(Y_t = [X_{1t}, X_{2t},...,X_{kt}]^T\\) with \\(n=1\\). Then \\(X_1 = \\sum_{t = 1}^n X_{1t}\\) and \\(X_2 = \\sum_{l = 1}^n X_{2l}\\). \\[\\begin{align} Cov(X_1, X_2) &amp;= E[X_1 X_2] - E[X_1] E[X_2] \\\\ &amp;= E[\\sum_{t = 1}^n X_{1t} \\sum_{l = 1}^n X_{2l}] - n^2 p_1 p_2 \\\\ &amp;= \\sum_{t = 1}^n \\sum_{l = 1}^n E[X_{1t} X_{2l}] - n^2 p_1 p_2. \\end{align}\\] For \\(X_{1t}\\) and \\(X_{2l}\\) the expected value is zero when \\(t = l\\). When \\(t \\neq l\\) then they are independent, so the expected value is the product \\(p_1 p_2\\). There are \\(n^2\\) total terms, and for \\(n\\) of them \\(t = l\\) holds. So \\(E[X_1 X_2] = (n^2 - n) p_1 p_2\\). Inserting into the above \\[\\begin{align} Cov(X_1, X_2) &amp;= (n^2 - n) p_1 p_2 - n^2 p_1 p_2 \\\\ &amp;= - n p_1 p_2. \\end{align}\\] 8.2 Multivariate normal random variables Exercise 74 (Cholesky decomposition) Let \\(X\\) be a random vector of length \\(k\\) with \\(X_i \\sim \\text{N}(0, 1)\\) and \\(LL^*\\) the Cholesky decomposition of a Hermitian positive-definite matrix \\(A\\). Let \\(\\mu\\) be a vector of length \\(k\\). Find the distribution of the random vector \\(Y = \\mu + L X\\). Find the Cholesky decomposition of \\(A = \\begin{bmatrix} 2 &amp; 1.2 \\\\ 1.2 &amp; 1 \\end{bmatrix}\\). R: Use the results from a) and b) to sample from the MVN distribution \\(\\text{N}(\\mu, A)\\), where \\(\\mu = [1.5, -1]^T\\). Plot a scatterplot and compare it to direct samples from the multivariate normal distribution (rmvnorm). Solution. \\(X\\) has an independent normal distribution of dimension \\(k\\). Then \\[\\begin{align} Y = \\mu + L X &amp;\\sim \\text{N}(\\mu, LL^T) \\\\ &amp;\\sim \\text{N}(\\mu, A). \\end{align}\\] Solve \\[\\begin{align} \\begin{bmatrix} a &amp; 0 \\\\ b &amp; c \\end{bmatrix} \\begin{bmatrix} a &amp; b \\\\ 0 &amp; c \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1.2 \\\\ 1.2 &amp; 1 \\end{bmatrix} \\end{align}\\] # a set.seed(1) nsamps &lt;- 1000 X &lt;- matrix(data = rnorm(nsamps * 2), ncol = 2) mu &lt;- c(1.5, -1) L &lt;- matrix(data = c(sqrt(2), 0, 1.2 / sqrt(2), sqrt(1 - 1.2^2/2)), ncol = 2, byrow = TRUE) Y &lt;- t(mu + L %*% t(X)) plot_df &lt;- data.frame(rbind(X, Y), c(rep(&quot;X&quot;, nsamps), rep(&quot;Y&quot;, nsamps))) colnames(plot_df) &lt;- c(&quot;D1&quot;, &quot;D2&quot;, &quot;var&quot;) ggplot(data = plot_df, aes(x = D1, y = D2, colour = as.factor(var))) + geom_point() Exercise 75 (Eigendecomposition) R: Let \\(\\Sigma = U \\Lambda U^T\\) be the eigendecomposition of covariance matrix \\(\\Sigma\\). Follow the procedure below, to sample from a multivariate normal with \\(\\mu = [-2, 1]^T\\) and \\(\\Sigma = \\begin{bmatrix} 0.3, -0.5 \\\\ -0.5, 1.6 \\end{bmatrix}\\): Sample from two independent standardized normal distributions to get \\(X\\). Find the eigendecomposition of \\(X\\) (eigen). Multiply \\(X\\) by \\(\\Lambda^{\\frac{1}{2}}\\) to get \\(X2\\). Consider how the eigendecomposition for \\(X2\\) changes compared to \\(X\\). Multiply \\(X2\\) by \\(U\\) to get \\(X3\\). Consider how the eigendecomposition for \\(X3\\) changes compared to \\(X2\\). Add \\(\\mu\\) to \\(X3\\). Consider how the eigendecomposition for \\(X4\\) changes compared to \\(X3\\). Plot the data and the eigenvectors (scaled with \\(\\Lambda^{\\frac{1}{2}}\\)) at each step. Hint: Use geom_segment for the eigenvectors. # a set.seed(1) sigma &lt;- matrix(data = c(0.3, -0.5, -0.5, 1.6), nrow = 2, byrow = TRUE) ed &lt;- eigen(sigma) e_val &lt;- ed$values e_vec &lt;- ed$vectors # b set.seed(1) nsamps &lt;- 1000 X &lt;- matrix(data = rnorm(nsamps * 2), ncol = 2) vec1 &lt;- matrix(c(1,0,0,1), nrow = 2) X2 &lt;- t(sqrt(diag(e_val)) %*% t(X)) vec2 &lt;- sqrt(diag(e_val)) %*% vec1 X3 &lt;- t(e_vec %*% t(X2)) vec3 &lt;- e_vec %*% vec2 X4 &lt;- t(c(-2, 1) + t(X3)) vec4 &lt;- c(-2, 1) + vec3 vec_mat &lt;- data.frame(matrix(c(0,0,0,0,0,0,0,0,0,0,0,0,-2,1,-2,1), ncol = 2, byrow = TRUE), t(cbind(vec1, vec2, vec3, vec4)), c(1,1,2,2,3,3,4,4)) df &lt;- data.frame(rbind(X, X2, X3, X4), c(rep(1, nsamps), rep(2, nsamps), rep(3, nsamps), rep(4, nsamps))) colnames(df) &lt;- c(&quot;D1&quot;, &quot;D2&quot;, &quot;wh&quot;) colnames(vec_mat) &lt;- c(&quot;D1&quot;, &quot;D2&quot;, &quot;E1&quot;, &quot;E2&quot;, &quot;wh&quot;) ggplot(data = df, aes(x = D1, y = D2)) + geom_point() + geom_segment(data = vec_mat, aes(xend = E1, yend = E2), color = &quot;red&quot;) + facet_wrap(~ wh) + coord_fixed() Exercise 76 (Marginal and conditional distributions) Let \\(X \\sim \\text{N}(\\mu, \\Sigma)\\), where \\(\\mu = [2, 0, -1]^T\\) and \\(\\Sigma = \\begin{bmatrix} 1 &amp; -0.2 &amp; 0.5 \\\\ -0.2 &amp; 1.4 &amp; -1.2 \\\\ 0.5 &amp; -1.2 &amp; 2 \\\\ \\end{bmatrix}\\). Let \\(A\\) represent the first two random variables and \\(B\\) the third random variable. R: For the calculation in the following points, you can use R. Find the marginal distribution of \\(B\\). Find the conditional distribution of \\(B | A\\). Find the marginal distribution of \\(A\\). Find the conditional distribution of \\(A | B\\). R: Visually compare the distributions of a) and b), and c) and d) at three different conditional values. mu &lt;- c(2, 0, -1) Sigma &lt;- matrix(c(1, -0.2, 0.5, -0.2, 1.4, -1.2, 0.5, -1.2, 2), nrow = 3, byrow = TRUE) mu_A &lt;- c(2, 0) mu_B &lt;- -1 Sigma_A &lt;- Sigma[1:2, 1:2] Sigma_B &lt;- Sigma[3, 3] Sigma_AB &lt;- Sigma[1:2, 3] # b tmp_b &lt;- t(Sigma_AB) %*% solve(Sigma_A) mu_b &lt;- mu_B - tmp_b %*% mu_A Sigma_b &lt;- Sigma_B - t(Sigma_AB) %*% solve(Sigma_A) %*% Sigma_AB mu_b ## [,1] ## [1,] -1.676471 tmp_b ## [,1] [,2] ## [1,] 0.3382353 -0.8088235 Sigma_b ## [,1] ## [1,] 0.8602941 # d tmp_a &lt;- Sigma_AB * (1 / Sigma_B) mu_a &lt;- mu_A - tmp_a * mu_B Sigma_d &lt;- Sigma_A - (Sigma_AB * (1 / Sigma_B)) %*% t(Sigma_AB) mu_a ## [1] 2.25 -0.60 tmp_a ## [1] 0.25 -0.60 Sigma_d ## [,1] [,2] ## [1,] 0.875 0.10 ## [2,] 0.100 0.68 Solution. \\(B \\sim \\text{N}(-1, 2)\\). \\(B | A = a \\sim \\text{N}(-1.68 + [0.34, -0.81] a, 0.86)\\). \\(\\mu_A = [2, 0, -1]^T\\) and \\(\\Sigma_A = \\begin{bmatrix} 1 &amp; -0.2 &amp; \\\\ -0.2 &amp; 1.4 \\\\ \\end{bmatrix}\\). \\[\\begin{align} X_A | X_B = b &amp;\\sim \\text{N}(\\mu_t, \\Sigma_t), \\\\ \\mu_t &amp;= [2.25, -0.6]^T + [0.25, -0.6]^T b, \\\\ \\Sigma_t &amp;= \\begin{bmatrix} 0.875 &amp; 0.1 \\\\ 0.1 &amp; 0.68 \\\\ \\end{bmatrix} \\end{align}\\] library(mvtnorm) set.seed(1) nsamps &lt;- 1000 # a and b samps &lt;- as.data.frame(matrix(data = NA, nrow = 4 * nsamps, ncol = 2)) samps[1:nsamps,1] &lt;- rnorm(nsamps, mu_B, Sigma_B) samps[1:nsamps,2] &lt;- &quot;marginal&quot; for (i in 1:3) { a &lt;- rmvnorm(1, mu_A, Sigma_A) samps[(i*nsamps + 1):((i + 1) * nsamps), 1] &lt;- rnorm(nsamps, mu_b + tmp_b %*% t(a), Sigma_b) samps[(i*nsamps + 1):((i + 1) * nsamps), 2] &lt;- paste0(# &quot;cond&quot;, round(a, digits = 2), collapse = &quot;-&quot;) } colnames(samps) &lt;- c(&quot;x&quot;, &quot;dist&quot;) ggplot(samps, aes(x = x)) + geom_density() + facet_wrap(~ dist) # c and d samps &lt;- as.data.frame(matrix(data = NA, nrow = 4 * nsamps, ncol = 3)) samps[1:nsamps,1:2] &lt;- rmvnorm(nsamps, mu_A, Sigma_A) samps[1:nsamps,3] &lt;- &quot;marginal&quot; for (i in 1:3) { b &lt;- rnorm(1, mu_B, Sigma_B) samps[(i*nsamps + 1):((i + 1) * nsamps), 1:2] &lt;- rmvnorm(nsamps, mu_a + tmp_a * b, Sigma_d) samps[(i*nsamps + 1):((i + 1) * nsamps), 3] &lt;- b } colnames(samps) &lt;- c(&quot;x&quot;, &quot;y&quot;, &quot;dist&quot;) ggplot(samps, aes(x = x, y = y)) + geom_point() + geom_smooth(method = &quot;lm&quot;) + facet_wrap(~ dist) 8.3 Transformations Exercise 77 Let \\((U,V)\\) be a random variable with PDF \\(p(u,v) = \\frac{1}{4 \\sqrt{u}}\\), \\(U \\in [0,4]\\) and \\(V \\in [\\sqrt{U}, \\sqrt{U} + 1]\\). Let \\(X = \\sqrt{U}\\) and \\(Y = V - \\sqrt{U}\\). Find PDF of \\((X,Y)\\). What can you tell about distributions of \\(X\\) and \\(Y\\)? This exercise shows how we can simplify a probabilistic problem with a clever use of transformations. R: Take 1000 samples from \\((X,Y)\\) and transform them with inverses of the above functions to get samples from \\((U,V)\\). Plot both sets of samples. Solution. First we need to find the inverse functions. Since \\(x = \\sqrt{u}\\) it follows that \\(u = x^2\\), and that \\(x \\in [0,2]\\). Similarly \\(v = y + x\\) and \\(y \\in [0,1]\\). Let us first find the Jacobian. \\[\\renewcommand\\arraystretch{1.6} J(x,y) = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} &amp; \\frac{\\partial v}{\\partial x} \\\\%[1ex] % &lt;-- 1ex more space between rows of matrix \\frac{\\partial u}{\\partial y} &amp; \\frac{\\partial v}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x &amp; 1 \\\\%[1ex] % &lt;-- 1ex more space between rows of matrix 0 &amp; 1 \\end{bmatrix}, \\] and the determinant is \\(|J(x,y)| = 2x\\). Putting everything together, we get \\[\\begin{align} p_{X,Y}(x,y) = p_{U,V}(x^2, y + x) |J(x,y)| = \\frac{1}{4 \\sqrt{x^2}} 2x = \\frac{1}{2}. \\end{align}\\] This reminds us of the Uniform distribution. Indeed we can see that \\(p_X(x) = \\frac{1}{2}\\) and \\(p_Y(y) = 1\\). So instead of dealing with an awkward PDF of \\((U,V)\\) and the corresponding dynamic bounds, we are now looking at two independent Uniform random variables. In practice, this could make modeling much easier. set.seed(1) nsamps &lt;- 2000 x &lt;- runif(nsamps, min = 0, max = 2) y &lt;- runif(nsamps) orig &lt;- tibble(x = x, y = y, vrs = &quot;original&quot;) u &lt;- x^2 v &lt;- y + x transf &lt;- tibble(x = u, y = v, vrs = &quot;transformed&quot;) df &lt;- bind_rows(orig, transf) ggplot(df, aes(x = x, y = y, color = vrs)) + geom_point(alpha = 0.3) Exercise 78 R: Write a function that will calculate the probability density of an arbitraty multivariate normal distribution, based on independent standardized normal PDFs. Compare with dmvnorm from the mvtnorm package. library(mvtnorm) set.seed(1) mvn_dens &lt;- function (y, mu, Sigma) { L &lt;- chol(Sigma) L_inv &lt;- solve(t(L)) g_inv &lt;- L_inv %*% t(y - mu) J &lt;- L_inv J_det &lt;- det(J) return(prod(dnorm(g_inv)) * J_det) } mu_v &lt;- c(-2, 0, 1) cov_m &lt;- matrix(c(1, -0.2, 0.5, -0.2, 2, 0.3, 0.5, 0.3, 1.6), ncol = 3, byrow = TRUE) n_comp &lt;- 20 for (i in 1:n_comp) { x &lt;- rmvnorm(1, mean = mu_v, sigma = cov_m) print(paste0(&quot;My function: &quot;, mvn_dens(x, mu_v, cov_m), &quot;, dmvnorm: &quot;, dmvnorm(x, mu_v, cov_m))) } ## [1] &quot;My function: 0.0229514237156383, dmvnorm: 0.0229514237156383&quot; ## [1] &quot;My function: 0.00763138915406231, dmvnorm: 0.00763138915406232&quot; ## [1] &quot;My function: 0.0230688881105741, dmvnorm: 0.0230688881105741&quot; ## [1] &quot;My function: 0.0113616213114732, dmvnorm: 0.0113616213114732&quot; ## [1] &quot;My function: 0.00151808500121908, dmvnorm: 0.00151808500121908&quot; ## [1] &quot;My function: 0.0257658045974509, dmvnorm: 0.0257658045974509&quot; ## [1] &quot;My function: 0.0157963825730805, dmvnorm: 0.0157963825730805&quot; ## [1] &quot;My function: 0.00408856287529248, dmvnorm: 0.00408856287529248&quot; ## [1] &quot;My function: 0.0327793540101256, dmvnorm: 0.0327793540101256&quot; ## [1] &quot;My function: 0.0111606542967978, dmvnorm: 0.0111606542967978&quot; ## [1] &quot;My function: 0.0147636757585684, dmvnorm: 0.0147636757585684&quot; ## [1] &quot;My function: 0.0142948300412208, dmvnorm: 0.0142948300412208&quot; ## [1] &quot;My function: 0.0203093820657542, dmvnorm: 0.0203093820657542&quot; ## [1] &quot;My function: 0.0287533273357481, dmvnorm: 0.0287533273357481&quot; ## [1] &quot;My function: 0.0213402305128623, dmvnorm: 0.0213402305128623&quot; ## [1] &quot;My function: 0.0218356957993885, dmvnorm: 0.0218356957993885&quot; ## [1] &quot;My function: 0.0250750113961771, dmvnorm: 0.0250750113961771&quot; ## [1] &quot;My function: 0.0166498666348048, dmvnorm: 0.0166498666348048&quot; ## [1] &quot;My function: 0.0018972510687466, dmvnorm: 0.0018972510687466&quot; ## [1] &quot;My function: 0.0196697814975113, dmvnorm: 0.0196697814975113&quot; "],
["ard.html", "9 Alternative representation of distributions 9.1 Probability generating functions (PGFs) 9.2 Moment generating functions (MGFs)", " 9 Alternative representation of distributions This chapter deals with alternative representation of distributions. The students are expected to acquire the following knowledge: Theoretical Probability generating functions. Moment generating functions. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 9.1 Probability generating functions (PGFs) Exercise 79 Show that the sum of independent Poisson random variables is itself a Poisson random variable. R: Let \\(X\\) be a sum of three Poisson distributions with \\(\\lambda_i \\in \\{2, 5.2, 10\\}\\). Take 1000 samples and plot the three distributions and the sum. Then take 1000 samples from the theoretical distribution of \\(X\\) and compare them to the sum. Solution. Let \\(X_i \\sim \\text{Poisson}(\\lambda_i)\\) for \\(i = 1,...,n\\), and let \\(X = \\sum_{i=1}^n X_i\\). \\[\\begin{align} \\alpha_X(t) &amp;= \\prod_{i=1}^n \\alpha_{X_i}(t) \\\\ &amp;= \\prod_{i=1}^n \\bigg( \\sum_{j=0}^\\infty t^j \\frac{\\lambda_i^j e^{-\\lambda_i}}{j!} \\bigg) \\\\ &amp;= \\prod_{i=1}^n \\bigg( e^{-\\lambda_i} \\sum_{j=0}^\\infty \\frac{(t\\lambda_i)^j }{j!} \\bigg) \\\\ &amp;= \\prod_{i=1}^n \\bigg( e^{-\\lambda_i} e^{t \\lambda_i} \\bigg) &amp; \\text{power series} \\\\ &amp;= \\prod_{i=1}^n \\bigg( e^{\\lambda_i(t - 1)} \\bigg) \\\\ &amp;= e^{\\sum_{i=1}^n \\lambda_i(t - 1)} \\\\ &amp;= e^{t \\sum_{i=1}^n \\lambda_i - \\sum_{i=1}^n \\lambda_i} \\\\ &amp;= e^{-\\sum_{i=1}^n \\lambda_i} \\sum_{j=0}^\\infty \\frac{(t \\sum_{i=1}^n \\lambda_i)^j}{j!}\\\\ &amp;= \\sum_{j=0}^\\infty \\frac{e^{-\\sum_{i=1}^n \\lambda_i} (t \\sum_{i=1}^n \\lambda_i)^j}{j!}\\\\ \\end{align}\\] The last term is the PGF of a Poisson random variable with parameter \\(\\sum_{i=1}^n \\lambda_i\\). Because the PGF is unique, \\(X\\) is a Poisson random variable. set.seed(1) library(tidyr) nsamps &lt;- 1000 samps &lt;- matrix(data = NA, nrow = nsamps, ncol = 4) samps[ ,1] &lt;- rpois(nsamps, 2) samps[ ,2] &lt;- rpois(nsamps, 5.2) samps[ ,3] &lt;- rpois(nsamps, 10) samps[ ,4] &lt;- samps[ ,1] + samps[ ,2] + samps[ ,3] colnames(samps) &lt;- c(2, 2.5, 10, &quot;sum&quot;) gsamps &lt;- as_tibble(samps) gsamps &lt;- gather(gsamps, key = &quot;dist&quot;, value = &quot;value&quot;) ggplot(gsamps, aes(x = value)) + geom_bar() + facet_wrap(~ dist) samps &lt;- cbind(samps, &quot;theoretical&quot; = rpois(nsamps, 2 + 5.2 + 10)) gsamps &lt;- as_tibble(samps[ ,4:5]) gsamps &lt;- gather(gsamps, key = &quot;dist&quot;, value = &quot;value&quot;) ggplot(gsamps, aes(x = value, fill = dist)) + geom_bar(position = &quot;dodge&quot;) Exercise 80 Find the expected value and variance of the negative binomial distribution. Hint: Find the Taylor series of \\((1 - y)^{-r}\\) at point 0. Solution. Let \\(X \\sim \\text{NB}(r, p)\\). \\[\\begin{align} \\alpha_X(t) &amp;= E[t^X] \\\\ &amp;= \\sum_{j=0}^\\infty t^j \\binom{j + r - 1}{j} (1 - p)^r p^j \\\\ &amp;= (1 - p)^r \\sum_{j=0}^\\infty \\binom{j + r - 1}{j} (tp)^j \\\\ &amp;= (1 - p)^r \\sum_{j=0}^\\infty \\frac{(j + r - 1)(j + r - 2)...r}{j!} (tp)^j. \\\\ \\end{align}\\] Let us look at the Taylor series of \\((1 - y)^{-r}\\) at 0 \\[\\begin{align} (1 - y)^{-r} = &amp;1 + \\frac{-r(-1)}{1!}y + \\frac{-r(-r - 1)(-1)^2}{2!}y^2 + \\\\ &amp;\\frac{-r(-r - 1)(-r - 2)(-1)^3}{3!}y^3 + ... \\\\ \\end{align}\\] How does the \\(k\\)-th term look like? We have \\(k\\) derivatives of our function so \\[\\begin{align} \\frac{d^k}{d^k y} (1 - y)^{-r} &amp;= \\frac{-r(-r - 1)...(-r - k + 1)(-1)^k}{k!}y^k \\\\ &amp;= \\frac{r(r + 1)...(r + k - 1)}{k!}y^k. \\end{align}\\] We observe that this equals to the \\(j\\)-th term in the sum of NB PGF. Therefore \\[\\begin{align} \\alpha_X(t) &amp;= (1 - p)^r (1 - tp)^{-r} \\\\ &amp;= \\Big(\\frac{1 - p}{1 - tp}\\Big)^r \\end{align}\\] To find the expected value, we need to differentiate \\[\\begin{align} \\frac{d}{dt} \\Big(\\frac{1 - p}{1 - tp}\\Big)^r &amp;= r \\Big(\\frac{1 - p}{1 - tp}\\Big)^{r-1} \\frac{d}{dt} \\frac{1 - p}{1 - tp} \\\\ &amp;= r \\Big(\\frac{1 - p}{1 - tp}\\Big)^{r-1} \\frac{p(1 - p)}{(1 - tp)^2}. \\\\ \\end{align}\\] Evaluating this at 1, we get: \\[\\begin{align} E[X] = \\frac{rp}{1 - p}. \\end{align}\\] For the variance we need the second derivative. \\[\\begin{align} \\frac{d^2}{d^2t} \\Big(\\frac{1 - p}{1 - tp}\\Big)^r &amp;= \\frac{p^2 r (r + 1) (\\frac{1 - p}{1 - tp})^r}{(tp - 1)^2} \\end{align}\\] Evaluating this at 1 and inserting the first derivatives, we get: \\[\\begin{align} Var[X] &amp;= \\frac{d^2}{dt^2} \\alpha_X(1) + \\frac{d}{dt}\\alpha_X(1) - \\Big(\\frac{d}{dt}\\alpha_X(t) \\Big)^2 \\\\ &amp;= \\frac{p^2 r (r + 1)}{(1 - p)^2} + \\frac{rp}{1 - p} - \\frac{r^2p^2}{(1 - p)^2} \\\\ &amp;= \\frac{rp}{(1 - p)^2}. \\end{align}\\] library(tidyr) set.seed(1) nsamps &lt;- 100000 find_p &lt;- function (mu, r) { return (10 / (r + 10)) } r &lt;- c(1,2,10,20) p &lt;- find_p(10, r) sigma &lt;- rep(sqrt(p*r / (1 - p)^2), each = nsamps) samps &lt;- cbind(&quot;r=1&quot; = rnbinom(nsamps, size = r[1], prob = 1 - p[1]), &quot;r=2&quot; = rnbinom(nsamps, size = r[2], prob = 1 - p[2]), &quot;r=4&quot; = rnbinom(nsamps, size = r[3], prob = 1 - p[3]), &quot;r=20&quot; = rnbinom(nsamps, size = r[4], prob = 1 - p[4])) gsamps &lt;- gather(as.data.frame(samps)) iw &lt;- (gsamps$value &gt; sigma + 10) | (gsamps$value &lt; sigma - 10) ggplot(gsamps, aes(x = value, fill = iw)) + geom_bar() + # geom_density() + facet_wrap(~ key) 9.2 Moment generating functions (MGFs) Exercise 81 Find the variance of the geometric distribution. Solution. Let \\(X \\sim \\text{Geometric}(p)\\). The MGF of the geometric distribution is \\[\\begin{align} M_X(t) &amp;= E[e^{tX}] \\\\ &amp;= \\sum_{k=0}^\\infty p(1 - p)^k e^{tk} \\\\ &amp;= p \\sum_{k=0}^\\infty ((1 - p)e^t)^k. \\end{align}\\] Let us assume that \\((1 - p)e^t &lt; 1\\). Then, by using the geometric series we get \\[\\begin{align} M_X(t) &amp;= \\frac{p}{1 - e^t + pe^t}. \\end{align}\\] The first derivative of the above expression is \\[\\begin{align} \\frac{d}{dt}M_X(t) &amp;= \\frac{-p(-e^t + pe^t)}{(1 - e^t + pe^t)^2}, \\end{align}\\] and evaluating at \\(t = 0\\), we get \\(\\frac{1 - p}{p}\\), which we already recognize as the expected value of the geometric distribution. The second derivative is \\[\\begin{align} \\frac{d^2}{dt^2}M_X(t) &amp;= \\frac{(p-1)pe^t((p-1)e^t - 1)}{((p - 1)e^t + 1)^3}, \\end{align}\\] and evaluating at \\(t = 0\\), we get \\(\\frac{(p - 1)(p - 2)}{p^2}\\). Combining we get the variance \\[\\begin{align} Var(X) &amp;= \\frac{(p - 1)(p - 2)}{p^2} - \\frac{(1 - p)^2}{p^2} \\\\ &amp;= \\frac{(p-1)(p-2) - (1-p)^2}{p^2} \\\\ &amp;= \\frac{1 - p}{p^2}. \\end{align}\\] Exercise 82 Find the distribution of sum of two normal random variables \\(X\\) and \\(Y\\), by comparing \\(M_{X+Y}(t)\\) to \\(M_X(t)\\). R: To illustrate the result draw random samples from N\\((-3, 1)\\) and N\\((5, 1.2)\\) and calculate the empirical mean and variance of \\(X+Y\\). Plot all three histograms in one plot. Solution. Let \\(X \\sim \\text{N}(\\mu_X, 1)\\) and \\(Y \\sim \\text{N}(\\mu_Y, 1)\\). The MGF of the sum is \\[\\begin{align} M_{X+Y}(t) &amp;= M_X(t) M_Y(t). \\end{align}\\] Let us calculate \\(M_X(t)\\), the MGF for \\(Y\\) then follows analogously. \\[\\begin{align} M_X(t) &amp;= \\int_{-\\infty}^\\infty e^{tx} \\frac{1}{\\sqrt{2 \\pi \\sigma_X^2}} e^{-\\frac{(x - mu_X)^2}{2\\sigma_X^2}} dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi \\sigma_X^2}} e^{-\\frac{(x - mu_X)^2 - 2\\sigma_X tx}{2\\sigma_X^2}} dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi \\sigma_X^2}} e^{-\\frac{x^2 - 2\\mu_X x + \\mu_X^2 - 2\\sigma_X tx}{2\\sigma_X^2}} dx \\\\ &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi \\sigma_X^2}} e^{-\\frac{(x - (\\mu_X + \\sigma_X^2 t))^2 + \\mu_X^2 - (\\mu_X + \\sigma_X^2 t)^2}{2\\sigma_X^2}} dx &amp; \\text{complete the square}\\\\ &amp;= e^{-\\frac{\\mu_X^2 - (\\mu_X + \\sigma_X^2 t)^2}{2\\sigma_X^2}} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2 \\pi \\sigma_X^2}} e^{-\\frac{(x - (\\mu_X + \\sigma_X^2 t))^2}{2\\sigma_X^2}} dx &amp; \\\\ &amp;= e^{-\\frac{\\mu_X^2 - (\\mu_X + \\sigma_X^2 t)^2}{2\\sigma_X^2}} &amp; \\text{normal PDF} \\\\ &amp;= e^{-\\frac{\\mu_X^2 - \\mu_X^2 - \\mu_X \\sigma_X^2 t - 2 \\sigma_X^4 t^2}{2\\sigma_X^2}} \\\\ &amp;= e^{\\sigma_X^2 t^2 + \\frac{\\mu_X t}{2}}. \\\\ \\end{align}\\] The MGF of the sum is then \\[\\begin{align} M_{X+Y}(t) &amp;= e^{\\sigma_X^2 t^2 + 0.5\\mu_X t} e^{\\sigma_Y^2 t^2 + 0.5\\mu_Y t} \\\\ &amp;= e^{t^2(\\sigma_X^2 + \\sigma_Y^2) + 0.5 t(\\mu_X + \\mu_Y)}. \\end{align}\\] By comparing \\(M_{X+Y}(t)\\) and \\(M_X(t)\\) we observe that both have two terms. The first is \\(2t^2\\) multiplied by the variance, and the second is \\(2t\\) multiplied by the mean. Since MGFs are unique, we conclude that \\(Z = X + Y \\sim \\text{N}(\\mu_X + \\mu_Y, \\sigma_X^2 + \\sigma_Y^2)\\). library(tidyr) library(ggplot2) set.seed(1) nsamps &lt;- 1000 x &lt;- rnorm(nsamps, -3, 1) y &lt;- rnorm(nsamps, 5, 1.2) z &lt;- x + y mean(z) ## [1] 1.968838 var(z) ## [1] 2.645034 df &lt;- data.frame(x = x, y = y, z = z) %&gt;% gather() ggplot(df, aes(x = value, fill = key)) + geom_histogram(position = &quot;dodge&quot;) "],
["ci.html", "10 Concentration inequalities 10.1 Comparison 10.2 Practical", " 10 Concentration inequalities This chapter deals with concentration inequalities. The students are expected to acquire the following knowledge: Theoretical More assumptions produce closer bounds. R Optimization. Estimating probability inequalities. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 10.1 Comparison Exercise 83 R: Let \\(X\\) be geometric random variable with \\(p = 0.7\\). Visually compare the Markov bound, Chernoff bound, and the theoretical probabilities. To get the best fitting Chernoff bound, you will need to optimize the bound depending on \\(t\\). Use either analytical or numerical optimization. bound_chernoff &lt;- function (t, p, a) { return ((p / (1 - exp(t) + p * exp(t))) / exp(a * t)) } set.seed(1) p &lt;- 0.7 a &lt;- seq(1, 12, by = 1) ci_markov &lt;- (1 - p) / p / a t &lt;- vector(mode = &quot;numeric&quot;, length = length(a)) for (i in 1:length(t)) { t[i] &lt;- optimize(bound_chernoff, interval = c(0, log(1 / (1 - p))), p = p, a = a[i])$minimum } t ## [1] 0.5108267 0.7984981 0.9162927 0.9808238 1.0216635 1.0498233 1.0704327 ## [8] 1.0861944 1.0986159 1.1086800 1.1169653 1.1239426 ci_chernoff &lt;- (p / (1 - exp(t) + p * exp(t))) / exp(a * t) actual &lt;- 1 - pgeom(a, 0.7) plot_df &lt;- rbind( data.frame(x = a, y = ci_markov, type = &quot;Markov&quot;), data.frame(x = a, y = ci_chernoff, type = &quot;Chernoff&quot;), data.frame(x = a, y = actual, type = &quot;Actual&quot;) ) ggplot(plot_df, aes(x = x, y = y, color = type)) + geom_line() Exercise 84 R: Let \\(X\\) be a sum of 100 Beta distributions with random parameters. Take 1000 samples and plot the Chebyshev bound, Hoeffding bound, and the empirical probabilities. set.seed(1) nvars &lt;- 100 nsamps &lt;- 1000 samps &lt;- matrix(data = NA, nrow = nsamps, ncol = nvars) Sn_mean &lt;- 0 Sn_var &lt;- 0 for (i in 1:nvars) { alpha1 &lt;- rgamma(1, 10, 1) beta1 &lt;- rgamma(1, 10, 1) X &lt;- rbeta(nsamps, alpha1, beta1) Sn_mean &lt;- Sn_mean + alpha1 / (alpha1 + beta1) Sn_var &lt;- Sn_var + alpha1 * beta1 / ((alpha1 + beta1)^2 * (alpha1 + beta1 + 1)) samps[ ,i] &lt;- X } mean(apply(samps, 1, sum)) ## [1] 51.12511 Sn_mean ## [1] 51.15723 var(apply(samps, 1, sum)) ## [1] 1.170652 Sn_var ## [1] 1.166183 a &lt;- 1:30 b &lt;- a / sqrt(Sn_var) ci_chebyshev &lt;- 1 / b^2 ci_hoeffding &lt;- 2 * exp(- 2 * a^2 / nvars) empirical &lt;- NULL for (i in 1:length(a)) { empirical[i] &lt;- sum(abs((apply(samps, 1, sum)) - Sn_mean) &gt;= a[i])/ nsamps } plot_df &lt;- rbind( data.frame(x = a, y = ci_chebyshev, type = &quot;Chebyshev&quot;), data.frame(x = a, y = ci_hoeffding, type = &quot;Hoeffding&quot;), data.frame(x = a, y = empirical, type = &quot;Empirical&quot;) ) ggplot(plot_df, aes(x = x, y = y, color = type)) + geom_line() ggplot(plot_df, aes(x = x, y = y, color = type)) + geom_line() + coord_cartesian(xlim = c(15, 25), ylim = c(0, 0.05)) 10.2 Practical Exercise 85 From Jagannathan. Let \\(X_i\\), \\(i = 1,...n\\), be a random sample of size \\(n\\) of a random variable \\(X\\). Let \\(X\\) have mean \\(\\mu\\) and variance \\(\\sigma^2\\). Find the size of the sample \\(n\\) required so that the probability that the difference between sample mean and true mean is smaller than \\(\\frac{\\sigma}{10}\\) is at least 0.95. Hint: Derive a version of the Chebyshev inequality for \\(P(|X - \\mu| \\geq a)\\) using Markov inequality. Solution. Let \\(\\bar{X} = \\sum_{i=1}^n X_i\\). Then \\(E[\\bar{X}] = \\mu\\) and \\(Var[\\bar{X}] = \\frac{\\sigma^2}{n}\\). Let us first derive another representation of Chebyshev inequality. \\[\\begin{align} P(|X - \\mu| \\geq a) = P(|X - \\mu|^2 \\geq a^2) \\leq \\frac{E[|X - \\mu|^2]}{a^2} = \\frac{Var[X]}{a^2}. \\end{align}\\] Let us use that on our sampling distribution: \\[\\begin{align} P(|\\bar{X} - \\mu| \\geq \\frac{\\sigma}{10}) \\leq \\frac{100 Var[\\bar{X}]}{\\sigma^2} = \\frac{100 Var[X]}{n \\sigma^2} = \\frac{100}{n}. \\end{align}\\] We are interested in the difference being smaller, therefore \\[\\begin{align} P(|\\bar{X} - \\mu| &lt; \\frac{\\sigma}{10}) = 1 - P(|\\bar{X} - \\mu| \\geq \\frac{\\sigma}{10}) \\geq 1 - \\frac{100}{n} \\geq 0.95. \\end{align}\\] It follows that we need a sample size of \\(n \\geq \\frac{100}{0.05} = 2000\\). "],
["crv.html", "11 Convergence of random variables", " 11 Convergence of random variables This chapter deals with convergence of random variables. The students are expected to acquire the following knowledge: Theoretical Finding convergences of random variables. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } Exercise 86 Let \\(X_1\\), \\(X_2\\),…, \\(X_n\\) be a sequence of Bernoulli random variables. Let \\(Y_k = \\frac{X_1 + X_2 + ... + X_n}{n^2}\\). Show that this sequence converges point-wise to the zero random variable. R: Use a simulation to check your answer. Solution. Let \\(\\epsilon\\) be arbitrary. We need to find such \\(n_0\\), that for every \\(n\\) greater than \\(n_0\\) \\(|Y_n| &lt; \\epsilon\\) holds. \\[\\begin{align} |Y_n| &amp;= |\\frac{X_1 + X_2 + ... + X_n}{n^2}| \\\\ &amp;\\leq |\\frac{n}{n^2}| \\\\ &amp;= \\frac{1}{n}. \\end{align}\\] So we need to find such \\(n_0\\), that for every \\(n &gt; n_0\\) we will have \\(\\frac{1}{n} &lt; \\epsilon\\). So \\(n_0 &gt; \\frac{1}{\\epsilon}\\). x &lt;- 1:1000 X &lt;- matrix(data = NA, nrow = length(x), ncol = 100) y &lt;- vector(mode = &quot;numeric&quot;, length = length(x)) for (i in 1:length(x)) { X[i, ] &lt;- rbinom(100, size = 1, prob = 0.5) } X &lt;- apply(X, 2, cumsum) tmp_mat &lt;- matrix(data = (1:1000)^2, nrow = 1000, ncol = 100) X &lt;- X / tmp_mat y &lt;- apply(X, 1, mean) ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + geom_line() Exercise 87 Let \\(\\Omega = [0,1]\\) and let \\(X_n\\) be a sequence of random variables, defined as \\[\\begin{align} X_n(\\omega) = \\begin{cases} \\omega^3, &amp;\\omega = \\frac{i}{n}, &amp;0 \\leq i \\leq 1 \\\\ 1, &amp; \\text{otherwise.} \\end{cases} \\end{align}\\] Show that \\(X_n\\) converges almost surely to \\(X \\sim \\text{Uniform}(0,1)\\). Solution. We need to show \\(P(\\{\\omega: X_n(\\omega) \\xrightarrow{\\text{a.s.}} X(\\omega)\\}) = 1\\). Let \\(\\omega \\neq \\frac{i}{n}\\). Then for any \\(\\omega\\), \\(X_n\\) converges pointwise to \\(X\\): \\[\\begin{align} X_n(\\omega) = 1 \\implies |X_n(\\omega) - X(s)| = |1 - 1| &lt; \\epsilon. \\end{align}\\] The above is independent of \\(n\\). Since there are countably infinite number of elements in the complement ($), the probability of this set is 1. Exercise 88 Borrowed from Wasserman. Let \\(X_n \\sim \\text{N}(0, \\frac{1}{n})\\) and let \\(X\\) be a random variable with CDF \\[\\begin{align} F_X(x) = \\begin{cases} 0, &amp;x &lt; 0 \\\\ 1, &amp;x \\geq 0. \\end{cases} \\end{align}\\] Does \\(X_n\\) converge to \\(X\\) in distribution? How about in probability? Prove or disprove these statement. R: Plot the CDF of \\(X_n\\) for \\(n = 1, 2, 5, 10, 100, 1000\\). Solution. Let us first check convergence in distribution. \\[\\begin{align} \\lim_{n \\rightarrow \\infty} F_{X_n}(x) &amp;= \\lim_{n \\rightarrow \\infty} \\phi (\\sqrt(n) x). \\end{align}\\] We have two cases, for \\(x &lt; 0\\) and \\(x &gt; 0\\). We do not need to check for \\(x = 0\\), since \\(F_X\\) is not continuous in that point. \\[\\begin{align} \\lim_{n \\rightarrow \\infty} \\phi (\\sqrt(n) x) = \\begin{cases} 0, &amp; x &lt; 0 \\\\ 1, &amp; x &gt; 0. \\end{cases} \\end{align}\\] This is the same as \\(F_X\\). Let us now check convergence in probability. Since \\(X\\) is a point-mass distribution at zero, we have \\[\\begin{align} \\lim_{n \\rightarrow \\infty} P(|X_n| &gt; \\epsilon) &amp;= \\lim_{n \\rightarrow \\infty} (P(X_n &gt; \\epsilon) + P(X_n &lt; -\\epsilon)) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} (1 - P(X_n &lt; \\epsilon) + P(X_n &lt; -\\epsilon)) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} (1 - \\phi(\\sqrt{n} \\epsilon) + \\phi(- \\sqrt{n} \\epsilon)) \\\\ &amp;= 0. \\end{align}\\] n &lt;- c(1,2,5,10,100,1000) ggplot(data = data.frame(x = seq(-5, 5, by = 0.01)), aes(x = x)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1/1), aes(color = &quot;sd = 1/1&quot;)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1/2), aes(color = &quot;sd = 1/2&quot;)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1/5), aes(color = &quot;sd = 1/5&quot;)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1/10), aes(color = &quot;sd = 1/10&quot;)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1/100), aes(color = &quot;sd = 1/100&quot;)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1/1000), aes(color = &quot;sd = 1/1000&quot;)) + stat_function(fun = pnorm, args = list(mean = 0, sd = 1/10000), aes(color = &quot;sd = 1/10000&quot;)) Exercise 89 Let \\(X_i\\) be i.i.d. and \\(\\mu = E(X_1)\\). Let variance of \\(X_1\\) be finite. Show that the mean of \\(X_i\\) converges in quadratic mean to \\(\\mu\\). Solution. \\[\\begin{align} \\lim_{n \\rightarrow \\infty} E(|\\bar{X_n} - \\mu|^2) &amp;= \\lim_{n \\rightarrow \\infty} E(\\bar{X_n}^2 - 2 \\bar{X_n} \\mu + \\mu^2) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} (E(\\bar{X_n}^2) - 2 \\mu E(\\frac{\\sum_{i=1}^n X_i}{n}) + \\mu^2) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} E(\\bar{X_n})^2 - \\lim_{n \\rightarrow \\infty} Var(\\bar{X_n}) - 2 \\mu^2 + \\mu^2 \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\frac{n^2 \\mu^2}{n^2} - \\lim_{n \\rightarrow \\infty} \\frac{\\sigma^2}{n} - \\mu^2 \\\\ &amp;= \\mu^2 - \\mu^2 - \\lim_{n \\rightarrow \\infty} \\frac{\\sigma^2}{n} \\\\ &amp;= 0. \\end{align}\\] "],
["lt.html", "12 Limit theorems", " 12 Limit theorems This chapter deals with limit theorems. The students are expected to acquire the following knowledge: Theoretical Monte Carlo integration. Difference between weak and strong law of large numbers. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } Exercise 90 Show that Monte Carlo integration converges almost surely to the true integral \\(\\int_{\\Omega} f(x) dx\\), where \\(\\Omega\\) is a bounded space. Solution. Let \\(g\\) be a PDF defined on \\(\\Omega\\). Let \\(X_i\\), \\(i = 1,...,n\\) be i.i.d. (multivariate) uniform random variables with bounds defined on \\(\\Omega\\). Let \\(Y_i\\) = \\(g(X_i)\\). Then it follows that \\(Y_i\\) are also i.i.d. random variables and their expected value is \\(E[g(X)] = \\int_{\\Omega} g(x) f(x) dx = \\frac{1}{V_{\\Omega}} \\int_{\\Omega} g(x) dx\\). By the strong law of large numbers, we have \\[\\begin{equation} \\frac{1}{n}\\sum_{i=1}^n Y_i \\rightarrow E[g(X)]. \\end{equation}\\] Exercise 91 Let \\(X\\) be a geometric random variable with probability 0.5 and support in positive integers. Let \\(Y = 2^X (-1)^X X^{-1}\\). Find the expected value of \\(Y\\) by using conditional convergence (this variable does not have an expected value in the conventional sense – the series is not absolutely convergent). R: Draw \\(10000\\) samples from a geometric distribution with probability 0.5 and support in positive integers to get \\(X\\). Then calculate \\(Y\\) and plot the means at each iteration (sample). Additionally, plot the expected value calculated in a. Try it with different seeds. What do you notice? Solution. \\[\\begin{align*} E[Y] &amp;= \\sum_{i=1}^n \\frac{2^x (-1)^x}{x} 0.5^x \\\\ &amp;= \\sum_{i=1}^n \\frac{(-1)^x}{x} \\\\ &amp;= - \\sum_{i=1}^n \\frac{(-1)^{x+1}}{x} \\\\ &amp;= - \\ln(2) \\end{align*}\\] set.seed(3) x &lt;- rgeom(100000, prob = 0.5) + 1 y &lt;- 2^x * (-1)^x * x^{-1} y_means &lt;- cumsum(y) / seq_along(y) df &lt;- data.frame(x = 1:length(y_means), y = y_means) ggplot(data = df, aes(x = x, y = y)) + geom_line() + geom_hline(yintercept = -log(2)) "],
["appendix-appendix.html", "(APPENDIX) Appendix", " (APPENDIX) Appendix "],
["A1.html", "13 R programming language 13.1 Basic characteristics 13.2 Why R? 13.3 Setting up 13.4 R basics 13.5 Functions 13.6 Other tips 13.7 Further reading and references", " 13 R programming language 13.1 Basic characteristics R is free software for statistical computing and graphics. It is widely used by statisticians, scientists, and other professionals for software development and data analysis. It is an interpreted language and therefore the programs do not need compilation. 13.2 Why R? R is one of the main two languages used for statistics and machine learning (the other being Python). Pros Libraries. Comprehensive collection of statistical and machine learning packages. Easy to code. Open source. Anyone can access R and develop new methods. Additionally, it is relatively simple to get source code of established methods. Large community. The use of R has been rising for some time, in industry and academia. Therefore a large collection of blogs and tutorials exists, along with people offering help on pages like StackExchange and CrossValidated. Integration with other languages and LaTeX. New methods. Many researchers develop R packages based on their research, therefore new methods are available soon after development. Cons Slow. Programs run slower than in other programming languages, however this can be somewhat ammended by effective coding or integration with other languages. Memory intensive. This can become a problem with large data sets, as they need to be stored in the memory, along with all the information the models produce. Some packages are not as good as they should be, or have poor documentation. Object oriented programming in R can be very confusing and complex. 13.3 Setting up https://www.r-project.org/. 13.3.1 RStudio RStudio is the most widely used IDE for R. It is free, you can download it from https://rstudio.com/. While console R is sufficient for the requirements of this course, we recommend the students install RStudio for its better user interface. 13.3.2 Libraries for data science Listed below are some of the more useful libraries (packages) for data science. Students are also encouraged to find other useful packages. dplyr Efficient data manipulation. Part of the wider package collection called tidyverse. ggplot2 Plotting based on grammar of graphics. stats Several statistical models. rstan Bayesian inference using Hamiltonian Monte Carlo. Very flexible model building. MCMCpack Bayesian inference. rmarkdown, knitr, and bookdown Dynamic reports (for example such as this one). devtools Package development. 13.4 R basics 13.4.1 Variables and types Important information and tips: no type declaration define variables with &lt;- instead of = (although both work, there is a slight difference, additionally most of the packages use the arrow) for strings use \"\" for comments use # change types with as.type() functions no special type for single character like C++ for example n &lt;- 20 x &lt;- 2.7 m &lt;- n # m gets value 20 my_flag &lt;- TRUE student_name &lt;- &quot;Luka&quot; typeof(n) ## [1] &quot;double&quot; typeof(student_name) ## [1] &quot;character&quot; typeof(my_flag) ## [1] &quot;logical&quot; typeof(as.integer(n)) ## [1] &quot;integer&quot; typeof(as.character(n)) ## [1] &quot;character&quot; 13.4.2 Basic operations n + x ## [1] 22.7 n - x ## [1] 17.3 diff &lt;- n - x # variable diff gets the difference between n and x diff ## [1] 17.3 n * x ## [1] 54 n / x ## [1] 7.407407 x^2 ## [1] 7.29 sqrt(x) ## [1] 1.643168 n &gt; 2 * n ## [1] FALSE n == n ## [1] TRUE n == 2 * n ## [1] FALSE n != n ## [1] FALSE paste(student_name, &quot;is&quot;, n, &quot;years old&quot;) ## [1] &quot;Luka is 20 years old&quot; 13.4.3 Vectors use c() to combine elements into vectors can only contain one type of variable if different types are provided, all are transformed to the most basic type in the vector access elements by indexes or logical vectors of the same length a scalar value is regarded as a vector of length 1 1:4 # creates a vector of integers from 1 to 4 ## [1] 1 2 3 4 student_ages &lt;- c(20, 23, 21) student_names &lt;- c(&quot;Luke&quot;, &quot;Jen&quot;, &quot;Mike&quot;) passed &lt;- c(TRUE, TRUE, FALSE) length(student_ages) ## [1] 3 # access by index student_ages[2] ## [1] 23 student_ages[1:2] ## [1] 20 23 student_ages[2] &lt;- 24 # change values # access by logical vectors student_ages[passed == TRUE] # same as student_ages[passed] ## [1] 20 24 student_ages[student_names %in% c(&quot;Luke&quot;, &quot;Mike&quot;)] ## [1] 20 21 student_names[student_ages &gt; 20] ## [1] &quot;Jen&quot; &quot;Mike&quot; 13.4.3.1 Operations with vectors most operations are element-wise if we operate on vectors of different lengths, the shorter vector periodically repeats its elements until it reaches the length of the longer one a &lt;- c(1, 3, 5) b &lt;- c(2, 2, 1) d &lt;- c(6, 7) a + b ## [1] 3 5 6 a * b ## [1] 2 6 5 a + d ## Warning in a + d: longer object length is not a multiple of shorter object ## length ## [1] 7 10 11 a + 2 * b ## [1] 5 7 7 a &gt; b ## [1] FALSE TRUE TRUE b == a ## [1] FALSE FALSE FALSE a %*% b # vector multiplication, not element-wise ## [,1] ## [1,] 13 13.4.4 Factors vectors of finite predetermined classes suitable for categorical variables ordinal (ordered) or nominal (unordered) car_brand &lt;- factor(c(&quot;Audi&quot;, &quot;BMW&quot;, &quot;Mercedes&quot;, &quot;BMW&quot;), ordered = FALSE) car_brand ## [1] Audi BMW Mercedes BMW ## Levels: Audi BMW Mercedes freq &lt;- factor(x = NA, levels = c(&quot;never&quot;,&quot;rarely&quot;,&quot;sometimes&quot;,&quot;often&quot;,&quot;always&quot;), ordered = TRUE) freq[1:3] &lt;- c(&quot;rarely&quot;, &quot;sometimes&quot;, &quot;rarely&quot;) freq ## [1] rarely sometimes rarely ## Levels: never &lt; rarely &lt; sometimes &lt; often &lt; always freq[4] &lt;- &quot;quite_often&quot; # non-existing level, returns NA ## Warning in `[&lt;-.factor`(`*tmp*`, 4, value = &quot;quite_often&quot;): invalid factor ## level, NA generated freq ## [1] rarely sometimes rarely &lt;NA&gt; ## Levels: never &lt; rarely &lt; sometimes &lt; often &lt; always 13.4.5 Matrices two-dimensional generalizations of vectors my_matrix &lt;- matrix(c(1, 2, 1, 5, 4, 2), nrow = 2, byrow = TRUE) my_matrix ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 5 4 2 my_square_matrix &lt;- matrix(c(1, 3, 2, 3), nrow = 2) my_square_matrix ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 3 my_matrix[1,2] # first row, second column ## [1] 2 my_matrix[2, ] # second row ## [1] 5 4 2 my_matrix[ ,3] # third column ## [1] 1 2 13.4.5.1 Matrix functions and operations most operation element-wise mind the dimensions when using matrix multiplication %*% nrow(my_matrix) # number of matrix rows ## [1] 2 ncol(my_matrix) # number of matrix columns ## [1] 3 dim(my_matrix) # matrix dimension ## [1] 2 3 t(my_matrix) # transpose ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 4 ## [3,] 1 2 diag(my_matrix) # the diagonal of the matrix as vector ## [1] 1 4 diag(1, nrow = 3) # creates a diagonal matrix ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 det(my_square_matrix) # matrix determinant ## [1] -3 my_matrix + 2 * my_matrix ## [,1] [,2] [,3] ## [1,] 3 6 3 ## [2,] 15 12 6 my_matrix * my_matrix # element-wise multiplication ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 25 16 4 my_matrix %*% t(my_matrix) # matrix multiplication ## [,1] [,2] ## [1,] 6 15 ## [2,] 15 45 my_vec &lt;- as.vector(my_matrix) # transform to vector my_vec ## [1] 1 5 2 4 1 2 13.4.6 Arrays multi-dimensional generalizations of matrices my_array &lt;- array(c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2)) my_array[1, 1, 1] ## [1] 1 my_array[2, 2, 1] ## [1] 4 my_array[1, , ] ## [,1] [,2] ## [1,] 1 5 ## [2,] 3 7 dim(my_array) ## [1] 2 2 2 13.4.7 Data frames basic data structure for analysis differ from matrices as columns can be of different types student_data &lt;- data.frame(&quot;Name&quot; = student_names, &quot;Age&quot; = student_ages, &quot;Pass&quot; = passed) student_data ## Name Age Pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE ## 3 Mike 21 FALSE colnames(student_data) &lt;- c(&quot;name&quot;, &quot;age&quot;, &quot;pass&quot;) # change column names student_data[1, ] ## name age pass ## 1 Luke 20 TRUE student_data[ ,colnames(student_data) %in% c(&quot;name&quot;, &quot;pass&quot;)] ## name pass ## 1 Luke TRUE ## 2 Jen TRUE ## 3 Mike FALSE student_data$pass # access column by name ## [1] TRUE TRUE FALSE student_data[student_data$pass == TRUE, ] ## name age pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE 13.4.8 Lists useful for storing different data structures access elements with double square brackets elements can be named first_list &lt;- list(student_ages, my_matrix, student_data) second_list &lt;- list(student_ages, my_matrix, student_data, first_list) first_list[[1]] ## [1] 20 24 21 second_list[[4]] ## [[1]] ## [1] 20 24 21 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 5 4 2 ## ## [[3]] ## name age pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE ## 3 Mike 21 FALSE second_list[[4]][[1]] # first element of the fourth element of second_list ## [1] 20 24 21 length(second_list) ## [1] 4 second_list[[length(second_list) + 1]] &lt;- &quot;add_me&quot; # append an element names(first_list) &lt;- c(&quot;Age&quot;, &quot;Matrix&quot;, &quot;Data&quot;) first_list$Age ## [1] 20 24 21 13.4.9 Loops mostly for loop for loop can iterate over an arbitrary vector # iterate over consecutive natural numbers my_sum &lt;- 0 for (i in 1:10) { my_sum &lt;- my_sum + i } my_sum ## [1] 55 # iterate over an arbirary vector my_sum &lt;- 0 some_numbers &lt;- c(2, 3.5, 6, 100) for (i in some_numbers) { my_sum &lt;- my_sum + i } my_sum ## [1] 111.5 13.5 Functions for help use ?function_name 13.5.1 Writing functions We can write our own functions with function(). In the brackets, we define the parameters the function gets, and in curly brackets we define what the function does. We use return() to return values. sum_first_n_elements &lt;- function (n) { my_sum &lt;- 0 for (i in 1:n) { my_sum &lt;- my_sum + i } return (my_sum) } sum_first_n_elements(10) ## [1] 55 13.6 Other tips Use set.seed(arbitrary_number) at the beginning of a script to set the seed and ensure replication. To dynamically set the working directory in R Studio to the parent folder of a R script use setwd(dirname(rstudioapi::getSourceEditorContext()$path)). To avoid slow R loops use the apply family of functions. See ?apply and ?lapply. To make your data manipulation (and therefore your life) a whole lot easier, use the dplyr package. Use getAnywhere(function_name) to get the source code of any function. Use browser for debugging. See ?browser. 13.7 Further reading and references Getting started with R Studio: https://www.youtube.com/watch?v=lVKMsaWju8w Official R manuals: https://cran.r-project.org/manuals.html Cheatsheets: https://www.rstudio.com/resources/cheatsheets/ Workshop on R, dplyr, ggplot2, and R Markdown: https://github.com/bstatcomp/Rworkshop "],
["probability-distributions.html", "14 Probability distributions", " 14 Probability distributions Name parameters pdf/pmf cdf mean variance Bernoulli \\(p\\) \\(p^k (1 - p)^{1 - k}\\) 11 \\(p\\) 58 \\(p(1-p)\\) 58 binomial \\(n\\), \\(p\\) \\(\\binom{n}{k} p^k (1 - p)^{n - k}\\) 35 \\(np\\) 59 \\(np(1-p)\\) 59 Poisson \\(\\lambda\\) \\(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\) 37 \\(\\lambda\\) 60 \\(\\lambda\\) 60 geometric \\(p\\) \\(p(1-p)^k\\) 36 \\(1 - (1-p)^{k + 1}\\) 36 \\(\\frac{1 - p}{p}\\) 61 normal \\(\\mu\\), \\(\\sigma\\) \\(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(x - \\mu)^2}{2 \\sigma^2}}\\) 43 \\(\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(t - \\mu)^2}{2 \\sigma^2}} dt\\) 43 \\(\\mu\\) 65 \\(\\sigma\\) 65 beta \\(\\alpha\\), \\(\\beta\\) \\(\\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}\\) 41 \\(\\frac{\\alpha}{\\alpha + \\beta}\\) 63 \\(\\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}\\) 63 gamma \\(\\alpha\\), \\(\\beta\\) \\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\beta x}\\) 42 \\(\\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}\\) 42 \\(\\frac{\\alpha}{\\beta}\\) 62 \\(\\frac{\\alpha}{\\beta^2}\\) 62 exponential \\(\\lambda\\) \\(\\lambda e^{-\\lambda x}\\) 39 \\(1 - e^{-\\lambda x}\\) 39 \\(\\lambda^{-1}\\) 64 \\(\\lambda^{-2}\\) 64 logistic \\(\\mu\\), \\(s\\) \\(\\frac{e^{-\\frac{x - \\mu}{s}}}{(1 + e{-\\frac{x - \\mu}{s}})^2}\\) 44 \\(\\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}}\\) 44 negative binomial \\(r\\), \\(p\\) \\(\\binom{k + r - 1}{k}(1-p)^r p^k\\) 38 multinomial \\(n\\), \\(p \\in [0,1]^k\\), \\(\\sum p = 1\\) \\(\\frac{n!}{x_1!x_2!...x_k!} p_1^{x_1} p_2^{x_2}...p_k^{x_k}\\) 72 "],
["references.html", "References", " References Jagannathan, Krishna. 2015. “Probability Foundation for Electrical Engineers.” R Core Team. 2016. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/. Ross, Sheldon M. 1998. A First Course in Probability. Prentice Hall Upper Saddle River, NJ. "]
]
