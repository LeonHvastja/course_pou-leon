[
["ev.html", "Chapter 7 Expected value 7.1 Discrete random variables 7.2 Continuous random variables 7.3 Sums, functions, conditional expectations", " Chapter 7 Expected value This chapter deals with expected values of random variables. The students are expected to acquire the following knowledge: Theoretical Calculation of the expected value. Calculation of variance and covariance. Cauchy distribution. R Estimation of expected value. Estimation of variance and covariance. 7.1 Discrete random variables Exercise 7.1 (Bernoulli) Let \\(X \\sim \\text{Bernoulli}(p)\\). Find \\(E[X]\\). Find \\(Var[X]\\). R: Let \\(p = 0.4\\). Check your answers to a) and b) with a simulation. Solution. \\[\\begin{align*} E[X] = \\sum_{k=0}^1 p^k (1-p)^{1-k} k = p. \\end{align*}\\] \\[\\begin{align*} Var[X] = E[X^2] - E[X]^2 = \\sum_{k=0}^1 (p^k (1-p)^{1-k} k^2) - p^2 = p(1-p). \\end{align*}\\] set.seed(1) nsamps &lt;- 1000 x &lt;- rbinom(nsamps, 1, 0.4) mean(x) ## [1] 0.394 var(x) ## [1] 0.239003 0.4 * (1 - 0.4) ## [1] 0.24 Exercise 7.2 (Binomial) Let \\(X \\sim \\text{Binomial}(n,p)\\). Find \\(E[X]\\). Find \\(Var[X]\\). Solution. Let \\(X = \\sum_{i=0}^n X_i\\), where \\(X_i \\sim \\text{Bernoulli}(p)\\). Then, due to linearity of expectation \\[\\begin{align*} E[X] = E[\\sum_{i=0}^n X_i] = \\sum_{i=0}^n E[X_i] = np. \\end{align*}\\] Again let \\(X = \\sum_{i=0}^n X_i\\), where \\(X_i \\sim \\text{Bernoulli}(p)\\). Since the Bernoulli variables \\(X_i\\) are independent we have \\[\\begin{align*} Var[X] = Var[\\sum_{i=0}^n X_i] = \\sum_{i=0}^n Var[X_i] = np(1-p). \\end{align*}\\] Exercise 7.3 (Poisson) Let \\(X \\sim \\text{Poisson}(\\lambda)\\). Find \\(E[X]\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\sum_{k=0}^\\infty \\frac{\\lambda^k e^{-\\lambda}}{k!} k &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!} &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!} &amp; \\text{term at $k=0$ is 0} \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^{k}}{k!} &amp; \\\\ &amp;= e^{-\\lambda} \\lambda e^\\lambda &amp; \\\\ &amp;= \\lambda. \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty k \\frac{\\lambda^{k-1}}{(k - 1)!} - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty (k - 1) + 1) \\frac{\\lambda^{k-1}}{(k - 1)!} - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\sum_{k=1}^\\infty (k - 1) \\frac{\\lambda^{k-1}}{(k - 1)!} + \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!}\\Big) - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\lambda\\sum_{k=2}^\\infty \\frac{\\lambda^{k-2}}{(k - 2)!} + e^\\lambda\\Big) - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\lambda e^\\lambda + e^\\lambda\\Big) - \\lambda^2 &amp; \\\\ &amp;= \\lambda^2 + \\lambda - \\lambda^2 &amp; \\\\ &amp;= \\lambda. \\end{align*}\\] Exercise 7.4 (Geometric) Let \\(X \\sim \\text{Geometric}(p)\\). Find \\(E[X]\\). Hint: \\(\\frac{d}{dx} x^k = k x^(k - 1)\\). Solution. \\[\\begin{align*} E[X] &amp;= \\sum_{k=0}^\\infty (1 - p)^k p k &amp; \\\\ &amp;= p (1 - p) \\sum_{k=0}^\\infty (1 - p)^{k-1} k &amp; \\\\ &amp;= p (1 - p) \\sum_{k=0}^\\infty -\\frac{d}{dp}(1 - p)^k &amp; \\\\ &amp;= p (1 - p) \\Big(-\\frac{d}{dp}\\Big) \\sum_{k=0}^\\infty (1 - p)^k &amp; \\\\ &amp;= p (1 - p) \\Big(-\\frac{d}{dp}\\Big) \\frac{1}{1 - (1 - p)} &amp; \\text{geometric series} \\\\ &amp;= \\frac{1 - p}{p} \\end{align*}\\] 7.2 Continuous random variables Exercise 7.5 (Gamma) Let \\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\). Hint: \\(\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t} dt\\) and \\(\\Gamma(z + 1) = z \\Gamma(z)\\). Find \\(E[X]\\). Find \\(Var[X]\\). R: Let \\(\\alpha = 10\\) and \\(\\beta = 2\\). Plot the density of \\(X\\). Add a horizontal line at the expected value that touches the density curve (geom_segment). Shade the area within a standard deviation of the expected value. Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^\\alpha e^{-\\beta x} dx &amp; \\\\ &amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) \\int_0^\\infty }x^\\alpha e^{-\\beta x} dx &amp; \\text{ (let $t = \\beta x$)} \\\\ &amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) \\int_0^\\infty }\\frac{t^\\alpha}{\\beta^\\alpha} e^{-t} \\frac{dt}{\\beta} &amp; \\\\ &amp;= \\frac{1}{\\beta \\Gamma(\\alpha) \\int_0^\\infty }t^\\alpha e^{-t} dt &amp; \\\\ &amp;= \\frac{\\Gamma(\\alpha + 1)}{\\beta \\Gamma(\\alpha)} &amp; \\\\ &amp;= \\frac{\\alpha \\Gamma(\\alpha)}{\\beta \\Gamma(\\alpha)} &amp; \\\\ &amp;= \\frac{\\alpha}{\\beta}. &amp; \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha+1} e^{-\\beta x} dx - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\Gamma(\\alpha + 2)}{\\beta^2 \\Gamma(\\alpha)} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{(\\alpha + 1)\\alpha\\Gamma(\\alpha)}{\\beta^2 \\Gamma(\\alpha)} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\alpha^2 + \\alpha}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\alpha}{\\beta^2}. \\end{align*}\\] set.seed(1) x &lt;- seq(0, 25, by = 0.01) y &lt;- dgamma(x, shape = 10, rate = 2) df &lt;- data.frame(x = x, y = y) ggplot(df, aes(x = x, y = y)) + geom_line() + geom_segment(aes(x = 5, y = 0, xend = 5, yend = dgamma(5, shape = 10, rate = 2)), color = &quot;red&quot;) + stat_function(fun = dgamma, args = list(shape = 10, rate = 2), xlim = c(5 - sqrt(10/4), 5 + sqrt(10/4)), geom = &quot;area&quot;, fill = &quot;gray&quot;, alpha = 0.4) Exercise 7.6 (Beta) Let \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\). Find \\(E[X]\\). Hint 1: \\(\\text{B}(x,y) = \\int_0^1 t^{x-1} (1 - t)^{y-1} dt\\). Hint 2: \\(\\text{B}(x + 1, y) = \\text{B}(x,y)\\frac{x}{x + y}\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^1 \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)} x dx \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)}\\int_0^1 x^{\\alpha} (1 - x)^{\\beta - 1} dx \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 1, \\beta) \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha, \\beta) \\frac{\\alpha}{\\alpha + \\beta} \\\\ &amp;= \\frac{\\alpha}{\\alpha + \\beta}. \\\\ \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\int_0^1 \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)} x^2 dx - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)}\\int_0^1 x^{\\alpha + 1} (1 - x)^{\\beta - 1} dx - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 2, \\beta) - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 1, \\beta) \\frac{\\alpha + 1}{\\alpha + \\beta + 1} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{\\alpha + 1}{\\alpha + \\beta + 1} \\frac{\\alpha}{\\alpha + \\beta} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2}\\\\ &amp;= \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}. \\end{align*}\\] Exercise 7.7 (Exponential) Let \\(X \\sim \\text{Exp}(\\lambda)\\). Find \\(E[X]\\). Hint: \\(\\Gamma(z + 1) = z\\Gamma(z)\\) and \\(\\Gamma(1) = 1\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\lambda e^{-\\lambda x} x dx &amp; \\\\ &amp;= \\lambda \\int_0^\\infty x e^{-\\lambda x} dx &amp; \\\\ &amp;= \\lambda \\int_0^\\infty \\frac{t}{\\lambda} e^{-t} \\frac{dt}{\\lambda} &amp; \\text{$t = \\lambda x$}\\\\ &amp;= \\lambda \\lambda^{-2} \\Gamma(2) &amp; \\text{definition of gamma function} \\\\ &amp;= \\lambda^{-1}. \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 &amp; \\\\ &amp;= \\int_0^\\infty \\lambda e^{-\\lambda x} x^2 dx - \\lambda^{-2} &amp; \\\\ &amp;= \\lambda \\int_0^\\infty \\frac{t^2}{\\lambda^2} e^{-t} \\frac{dt}{\\lambda} - \\lambda^{-2} &amp; \\text{$t = \\lambda x$} \\\\ &amp;= \\lambda \\lambda^{-3} \\Gamma(3) - \\lambda^{-2} &amp; \\text{definition of gamma function} &amp; \\\\ &amp;= \\lambda^{-2} 2 \\Gamma(2) - \\lambda^{-2} &amp; \\\\ &amp;= 2 \\lambda^{-2} - \\lambda^{-2} &amp; \\\\ &amp;= \\lambda^{-2}. &amp; \\\\ \\end{align*}\\] Exercise 7.8 (Normal) Let \\(X \\sim \\text{N}(\\mu, \\sigma)\\). Show that \\(E[X] = \\mu\\). Hint: Use the error function \\(\\text{erf}(x) = \\frac{1}{\\sqrt(\\pi)} \\int_{-x}^x e^{-t^2} dt\\). The statistical interpretation of this function is that if \\(Y \\sim \\text{N}(0, 0.5)\\), then the error function describes the probability of \\(Y\\) falling between \\(-x\\) and \\(x\\). Show that \\(Var[X] = \\sigma^2\\). Hint: Start with the definition of variance. Solution. \\[\\begin{align*} E[X] &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} x dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty x e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty \\Big(t \\sqrt{2\\sigma^2} + \\mu\\Big)e^{-t^2} \\sqrt{2 \\sigma^2} dt &amp; t = \\frac{x - \\mu}{\\sqrt{2\\sigma}} \\\\ &amp;= \\frac{\\sqrt{2\\sigma^2}}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty t e^{-t^2} dt + \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty \\mu e^{-t^2} dt &amp; \\\\ \\end{align*}\\] Let us calculate these integrals separately. \\[\\begin{align*} \\int t e^{-t^2} dt &amp;= -\\frac{1}{2}\\int e^{s} ds &amp; s = -t^2 \\\\ &amp;= -\\frac{e^s}{2} + C \\\\ &amp;= -\\frac{e^{-t^2}}{2} + C &amp; \\text{undoing substitution}. \\end{align*}\\] Inserting the integration limits we get \\[\\begin{align*} \\int_{-\\infty}^\\infty t e^{-t^2} dt &amp;= 0, \\end{align*}\\] due to the integrated function being symmetric. Reordering the second integral we get \\[\\begin{align*} \\mu \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty e^{-t^2} dt &amp;= \\mu \\text{erf}(\\infty) &amp; \\text{definition of error function} \\\\ &amp;= \\mu &amp; \\text{probability of $Y$ falling between $-\\infty$ and $\\infty$}. \\end{align*}\\] Combining all of the above we get \\[\\begin{align*} E[X] &amp;= \\frac{\\sqrt{2\\sigma^2}}{\\sqrt{\\pi}} \\times 0 + \\mu &amp;= \\mu.\\\\ \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[(X - E[X])^2] \\\\ &amp;= E[(X - \\mu)^2] \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty (x - \\mu)^2 e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} dx \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty t^2 e^{-\\frac{t^2}{2}} dt \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\bigg(\\Big(- t e^{-\\frac{t^2}{2}} |_{-\\infty}^\\infty \\Big) + \\int_{-\\infty}^\\infty e^{-\\frac{t^2}{2}} \\bigg) dt &amp; \\text{integration by parts} \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\sqrt{2 \\pi} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt(\\pi)}e^{-s^2} \\bigg) &amp; s = \\frac{t}{\\sqrt{2}} \\text{ and evaluating the left expression at the bounds} \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\sqrt{2 \\pi} \\Big(\\text{erf}(\\infty) &amp; \\text{definition of error function} \\\\ &amp;= \\sigma^2. \\end{align*}\\] 7.3 Sums, functions, conditional expectations Exercise 7.9 (Sum of independent random variables) Let \\(X_1, X_2,...,X_n\\) be IID random variables with expected value \\(E[X_i] = \\mu\\) and variance \\(Var[X_i] = \\sigma^2\\). Find the expected value and variance of \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\). \\(\\bar{X}\\) is called a statistic (a function of the values in a sample). It is itself a random variable. R: Take \\(n = 5, 10, 100, 1000\\) samples from the N(\\(2\\), \\(6\\)) distribution 10000 times. Plot the theoretical density and the densities of \\(\\bar{X}\\) statistic for each \\(n\\). Intuitively, are the results in correspondence with your calculations? Check them numerically. Solution. Let us start with the expectation of \\(\\bar{X}\\). \\[\\begin{align} E[\\bar{X}] &amp;= E[\\frac{1}{n} \\sum_{i=1}^n X_i] &amp; \\\\ &amp;= \\frac{1}{n} E[\\sum_{i=1}^n X_i] &amp; \\text{ (multiplication with a scalar)} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n E[X_i] &amp; \\text{ (linearity)} \\\\ &amp;= \\frac{1}{n} n \\mu &amp; \\\\ &amp;= \\mu. \\end{align}\\] Now the variance \\[\\begin{align} Var[\\bar{X}] &amp;= Var[\\frac{1}{n} \\sum_{i=1}^n X_i] &amp; \\\\ &amp;= \\frac{1}{n^2} Var[\\sum_{i=1}^n X_i] &amp; \\text{ (multiplication with a scalar)} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n Var[X_i] &amp; \\text{ (independence of samples)} \\\\ &amp;= \\frac{1}{n^2} n \\sigma^2 &amp; \\\\ &amp;= \\frac{1}{n} \\sigma^2. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 mu &lt;- 2 sigma &lt;- sqrt(6) N &lt;- c(5, 10, 100, 500) X &lt;- matrix(data = NA, nrow = nsamps, ncol = length(N)) ind &lt;- 1 for (n in N) { for (i in 1:nsamps) { X[i,ind] &lt;- mean(rnorm(n, mu, sigma)) } ind &lt;- ind + 1 } colnames(X) &lt;- N X &lt;- melt(as.data.frame(X)) ggplot(data = X, aes(x = value, colour = variable)) + geom_density() + stat_function(data = data.frame(x = seq(-2, 6, by = 0.01)), aes(x = x), fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;black&quot;) "]
]
