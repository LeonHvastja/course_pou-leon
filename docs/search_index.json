[
["index.html", "Principles of Uncertainty – exercises Preface", " Principles of Uncertainty – exercises Gregor Pirš and Erik Štrumbelj 2019-08-16 Preface These are the exercises for the Principles of uncertainty course of the Data Science Master’s at University of Ljubljana, Faculty of computer and information science. "],
["introduction.html", "Chapter 1 Probability spaces 1.1 Measure and probability spaces 1.2 Properties of probability measures 1.3 Discrete probability spaces", " Chapter 1 Probability spaces This chapter deals with measures and probability spaces. At the end of the chapter, we look more closely at discrete probability spaces. The students are expected to acquire the following knowledge: Theoretical Use properties of probability to calculate probabilities. Combinatorics. Understanding of continuity of probability. R vectors for loop estimating probability with simulation sample function matrices 1.1 Measure and probability spaces Exercise 1.1 (Completing a set to a Sigma algebra) Let \\(\\Omega = \\{1,2,...,10\\}\\). Let \\(A = [\\{ \\}, \\{1\\}, \\{2\\}, \\Omega]\\). Show that \\(A\\) is not a sigma algebra of \\(\\Omega\\). Find the minimum number of elements to complete A to a sigma algebra of \\(\\Omega\\). Solution. \\(1^c = \\{2,3,...,10\\} \\notin A \\implies\\) \\(A\\) is not sigma algebra. First we need the complements of all elements, so we need to add sets \\(\\{2,3,...,10\\}\\) and \\(\\{1,3,4,...,10\\}\\). Next we need unions of all sets – we add the set \\(\\{1,2\\}\\). Again we need the complement of this set, so we add \\(\\{3,4,...,10\\}\\). So the minimum number of elements we need to add is 4. Exercise 1.2 (Diversity of sigma algebras) Let \\(\\Omega\\) be a set. Show that \\(2^{\\Omega}\\) is a sigma algebra. Find the smallest sigma algebra of \\(\\Omega\\). Find the largest Sigma algebra of \\(\\Omega\\). Solution. TODO \\(A = \\{\\emptyset, \\Omega\\}\\) \\(2^{\\Omega}\\) Exercise 1.3 Find all sigma algebras for \\(\\Omega = \\{0, 1, 2\\}\\). Solution. \\(A = \\{\\emptyset, \\Omega\\}\\) \\(A = 2^{\\Omega}\\) \\(A = \\{\\emptyset, \\{0\\}, \\{1,2\\}, \\Omega\\}\\) \\(A = \\{\\emptyset, \\{1\\}, \\{0,2\\}, \\Omega\\}\\) \\(A = \\{\\emptyset, \\{2\\}, \\{0,1\\}, \\Omega\\}\\) Exercise 1.4 (Difference between algebra and sigma algebra) Let \\(\\Omega = \\mathbb{N}\\) and \\(\\mathcal{A} = \\{A \\subseteq \\mathbb{N}: A \\text{ is finite or } A^c \\text{ is finite.} \\}\\). Show that \\(\\mathcal{A}\\) is an algebra but not a sigma algebra. Solution. - \\(\\emptyset\\) is finite so \\(\\emptyset \\in \\mathcal{A}\\). - Let \\(A \\in \\mathcal{A}\\) and \\(B \\in \\mathcal{A}\\). If both are finite, then their union is also finite and therefore in \\(\\mathcal{A}\\). Let at least one of them not be finite. Then their union is not finite. But \\((A \\cup B)^c = A^c \\cap B^c\\). And since at least one is infinite, then its complement is finite and the intersection is too. So finite unions are in \\(\\mathcal{A}\\). - Let us look at numbers \\(2n\\). For any \\(n\\), \\(2n \\in \\mathcal{A}\\) as it is finite. But \\(\\bigcup_{k = 1}^{\\infty} \\in \\mathcal{A}\\). Exercise 1.5 (Intro to measure) Take the measurable space \\(\\Omega = \\{1,2\\}\\), \\(F = 2^{\\Omega}\\). Which of the following is a measure? Which is a probability measure? \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 5\\), \\(\\mu(\\{2\\}) = 6\\), \\(\\mu(\\{1,2\\}) = 11\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 0\\), \\(\\mu(\\{1,2\\}) = 1\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 0\\), \\(\\mu(\\{1,2\\}) = 0\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 1\\), \\(\\mu(\\{1,2\\}) = 1\\) \\(\\mu(\\emptyset)=0\\), \\(\\mu(\\{1\\})=0\\), \\(\\mu(\\{2\\})=\\infty\\), \\(\\mu(\\{1,2\\})=\\infty\\) Solution. Measure. Not probability measure since \\(\\mu(\\Omega) &gt; 1\\). Neither due to countable additivity. Measure. Not probability measure since \\(\\mu(\\Omega) = 0\\). Probability measure. Measure. Not probability measure since \\(\\mu(\\Omega) &gt; 1\\). Exercise 1.6 Define a probability space that could be used to model a fair 6-sided die. Solution. \\(\\Omega = \\{1,2,3,4,5,6\\}\\) \\(F = 2^{\\Omega}\\) \\(\\forall \\omega \\in \\Omega\\), \\(P(\\omega) = \\frac{1}{6}\\) 1.2 Properties of probability measures Exercise 1.7 A standard deck (52 cards) is distributed to two persons: 26 cards to each person. All partitions are equally likely. Find the probability that: The first person gets 4 Queens. The first person gets at least 2 Queens. The first person gets at least 2 Queens and at least 2 Kings. R: Use simulation (sample) to check the above answers. Solution. \\(\\frac{\\binom{48}{22}}{\\binom{52}{26}}\\) For the simulation, let us represent cards with numbers from 1 to 52, and let 1 through 4 represent Queens, and 5 through 8 represent Kings. set.seed(1) cards &lt;- 1:52 n &lt;- 10000 q4 &lt;- vector(mode = &quot;logical&quot;, length = n) q2 &lt;- vector(mode = &quot;logical&quot;, length = n) q2k2 &lt;- vector(mode = &quot;logical&quot;, length = n) for (i in 1:n) { p1 &lt;- sample(1:52, 26) q4[i] &lt;- sum(1:4 %in% p1) == 4 q2[i] &lt;- sum(1:4 %in% p1) &gt;= 2 q2k2[i] &lt;- (sum(1:4 %in% p1) &gt;= 2) &amp; (sum(5:8 %in% p1) &gt;= 2) } sum(q4) / n ## [1] 0.0556 sum(q2) / n ## [1] 0.6995 sum(q2k2) / n ## [1] 0.4711 Exercise 1.8 Let \\(A\\) and \\(B\\) be events with probabilities \\(P(A) = \\frac{2}{3}\\) and \\(P(B) = \\frac{1}{2}\\). Show that \\(\\frac{1}{6} \\leq P(A\\cap B) \\leq \\frac{1}{2}\\), and give examples to show that both extremes are possible. Find corresponding bounds for \\(P(A\\cup B)\\). R: Draw samples from the examples and show the probability bounds of \\(P(A \\cap B)\\). Solution. Smth From the properties of probability we have \\[\\begin{equation} P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\leq 1. \\end{equation}\\] From this follows \\[\\begin{align} P(A \\cap B) &amp;\\geq P(A) + P(B) - 1 \\\\ &amp;= \\frac{2}{3} + \\frac{1}{2} - 1 \\\\ &amp;= \\frac{1}{6}, \\end{align}\\] which is the lower bound for the intersection. Conversely, we have \\[\\begin{equation} P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\geq P(A). \\end{equation}\\] From this follows \\[\\begin{align} P(A \\cap B) &amp;\\leq P(B) \\\\ &amp;= \\frac{1}{2}, \\end{align}\\] which is the upper bound for the intersection. For an example take a fair die. To achieve the lower bound let \\(A = \\{1,2,3\\}\\) and \\(B = \\{3,4,5,6\\}\\), then their intersection is \\(A \\cap B = \\{3\\}\\). To achieve the upper bound take \\(A = \\{1,2,3\\}\\) and \\(B = \\{1,2,3,4\\}\\). For the bounds of the union we will use the results from the first part. Again from the properties of probability we have \\[\\begin{align} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;\\geq P(A) + P(B) - \\frac{1}{2} \\\\ &amp;= \\frac{2}{3}. \\end{align}\\] Conversely \\[\\begin{align} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;\\leq P(A) + P(B) - \\frac{1}{6} \\\\ &amp;= 1. \\end{align}\\] Therefore \\(\\frac{2}{3} \\leq P(A \\cup B) \\leq 1\\). We use sample in R: set.seed(1) n &lt;- 10000 samps &lt;- sample(1:6, n, replace = TRUE) # lower bound lb &lt;- vector(mode = &quot;logical&quot;, length = n) A &lt;- c(1,2,3) B &lt;- c(3,4,5,6) for (i in 1:n) { lb[i] &lt;- samps[i] %in% A &amp; samps[i] %in% B } sum(lb) / n ## [1] 0.1724 # upper bound ub &lt;- vector(mode = &quot;logical&quot;, length = n) A &lt;- c(1,2,3) B &lt;- c(1,2,3,4) for (i in 1:n) { ub[i] &lt;- samps[i] %in% A &amp; samps[i] %in% B } sum(ub) / n ## [1] 0.5047 Exercise 1.9 A fair coin is tossed repeatedly. Show that, with probability one, a head turns up soonder or later. Show similarly that any given finite sequence of heads and tails occurs eventually with probability one. Solution. \\[\\begin{align} P(\\text{no heads}) &amp;= \\lim_{n \\rightarrow \\infty} P(\\text{no heads in first }n \\text{ tosses}) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\frac{1}{2^n} \\\\ &amp;= 0. \\end{align}\\] For the second part, let us fix the given sequence of heads and tails of length \\(k\\) as \\(s\\). A probability that this happens in \\(k\\) tosses is \\(\\frac{1}{2^k}\\). \\[\\begin{align} P(s \\text{ occurs}) &amp;= \\lim_{n \\rightarrow \\infty} P(s \\text{ occurs in first } nk \\text{ tosses}) \\end{align}\\] The right part of the upper equation is greater than if \\(s\\) occurs eiter in the first \\(k\\) tosses, second \\(k\\) tosses,…, \\(n\\)-th \\(k\\) tosses. Therefore \\[\\begin{align} P(s \\text{ occurs}) &amp;\\geq \\lim_{n \\rightarrow \\infty} P(s \\text{ occurs in first } n \\text{ disjoint sequences of length } k) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} (1 - P(s \\text{ does not occur in first } n \\text{ disjoint sequences})) \\\\ &amp;= 1 - \\lim_{n \\rightarrow \\infty} P(s \\text{ does not occur in first } n \\text{ disjoint sequences}) \\\\ &amp;= 1 - \\lim_{n \\rightarrow \\infty} (1 - \\frac{1}{2^k})^n \\\\ &amp;= 1. \\end{align}\\] Exercise 1.10 An Erdos-Renyi random graph \\(G(n,p)\\) is a model with n nodes, where each pair of nodes is connected with probability \\(p\\). Calculate the probability that there exists a node that is not connected to any other node in \\(G(4,0.6)\\). Show that the upper bound for the probability that there exist 2 nodes that are not connected to any other node for an arbitrary \\(G(n,p)\\) is \\(\\binom{n}{2} (1-p)^{2n - 3}\\). R: Estimate the probability from the first point using simulation. Solution. Let \\(A_i\\) be the event that the \\(i\\)-th node is not connected to any other node. Then our goal is to calculate \\(P(\\cup_{i=1}^n A_i)\\). Using the inclusion-exclusion principle, we get \\[\\begin{align} P(\\cup_{i=1}^n A_i) &amp;= \\sum_i A_i - \\sum_{i&lt;j} P(A_i \\cap A_j) + \\sum_{i&lt;j&lt;k} P(A_i \\cap A_j \\cap A_k) - P(A_1 \\cap A_2 \\cap A_3 \\cap A_4) \\\\ &amp;=4 (1 - p)^3 - \\binom{4}{2} (1 - p)^5 + \\binom{4}{3} (1 - p)^6 - (1 - p)^6 \\\\ &amp;\\approx 0.21. \\end{align}\\] Let \\(A_{ij}\\) be the event that nodes \\(i\\) and \\(j\\) are not connected to any other node. We are interested in \\(P(\\cup_{i&lt;j}A_{ij})\\). By using Boole`s inequality, we get \\[\\begin{align} P(\\cup_{i&lt;j}A_{ij}) \\leq \\sum_{i&lt;j} P(A_{ij}). \\end{align}\\] What is the probability of \\(A_{ij}\\)? There need to be no connections to the \\(i\\)-th node to the remaining nodes (excluding \\(j\\)), the same for the \\(j\\)-th node, and there can be no connection between them. Therefore \\[\\begin{align} P(\\cup_{i&lt;j}A_{ij}) &amp;\\leq \\sum_{i&lt;j} (1 - p)^{2(n-2) + 1} \\\\ &amp;= \\binom{n}{2} (1 - p)^{2n - 3}. \\end{align}\\] set.seed(1) n_samp &lt;- 100000 n &lt;- 4 p &lt;- 0.6 conn_samp &lt;- vector(mode = &quot;logical&quot;, length = n_samp) for (i in 1:n_samp) { tmp_mat &lt;- matrix(data = 0, nrow = n, ncol = n) samp_conn &lt;- sample(c(0,1), choose(4,2), replace = TRUE, prob = c(1 - p, p)) tmp_mat[lower.tri(tmp_mat)] &lt;- samp_conn tmp_mat[upper.tri(tmp_mat)] &lt;- t(tmp_mat)[upper.tri(t(tmp_mat))] not_conn &lt;- apply(tmp_mat, 1, sum) if (any(not_conn == 0)) { conn_samp[i] &lt;- TRUE } else { conn_samp[i] &lt;- FALSE } } sum(conn_samp) / n_samp ## [1] 0.20565 1.3 Discrete probability spaces Exercise 1.11 Show that the standard measurable space on $= {0,1,…,n} equipped with binomial measure is a discrete probability space. Define another probability measure on this measurable space. Show that for \\(n=1\\) the binomial measure is the same as the bernoulli measure. R: Draw 1000 samples from the binomial distribution \\(p=0.5\\), \\(n=20\\) (rbinom) and compare relative frequencies with theoretical probability measure. Solution. We need to show that the terms of \\(\\sum_{k=0}^n \\binom{n}{k} p^k (1 - p)^{n - k}\\) sum to 1. For that we use the binomial theorem \\(\\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} = (x + y)^n\\). So \\[\\begin{equation} \\sum_{k=0}^n \\binom{n}{k} p^k (1 - p)^{n - k} = (p + 1 - p)^n = 1. \\end{equation}\\] \\(P(\\{k\\}) = \\frac{1}{n + 1}\\). When \\(n=1\\) then \\(k \\in \\{0,1\\}\\). Inserting \\(n=1\\) into the binomial measure, we get \\(\\binom{1}{k}p^k (1-p)^{1 - k}\\). Now \\(\\binom{1}{1} = \\binom{1}{0} = 1\\), so the measure is \\(p^k (1-p)^{1 - k}\\), which is the Bernoulli measure. set.seed(1) library(ggplot2) library(dplyr) bin_samp &lt;- rbinom(n = 1000, size = 20, prob = 0.5) bin_samp &lt;- data.frame(x = bin_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dbinom(0:20, size = 20, prob = 0.5), type = &quot;theoretical_measure&quot;)) bin_plot &lt;- ggplot(data = bin_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(bin_plot) Exercise 1.12 Show that the standard measurable space on \\(\\Omega = \\{0,1,...,\\infty\\}\\) equipped with geometric measure is a discrete probability space. equipped with Poisson measure is a discrete probability space. Define another probability measure on this measurable space. R: Draw 1000 samples from the Poisson distribution \\(\\lambda = 10\\) (rpois) and compare relative frequencies with theoretical probability measure. Solution. \\(\\sum_{k = 0}^{\\infty} p(1 - p)^k = p \\sum_{k = 0}^{\\infty} (1 - p)^k = p \\frac{1}{1 - 1 + p} = 1\\). We used the formula for geometric series. \\(\\sum_{k = 0}^{\\infty} \\frac{\\lambda^k e^{-\\lambda}}{k!} = e^{-\\lambda} \\sum_{k = 0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{-\\lambda} e^{\\lambda} = 1.\\) We used the Taylor expansion of the exponential function. TODO Since we only have to define a probability measure, we could only assign probabilities that sum to one to a finite number of events in \\(\\Omega\\), and probability zero to the other infinite number of events. However to make this solution more educational, we will try to find a measure that assigns a non-zero probability to all events in \\(\\Omega\\). A good start for this would be to find a converging infinite series, as the probabilities will have to sum to one. One simple converging series is the geometric series \\(\\sum_{k=0}^n p^k\\) for \\(|p| &lt; 1\\). Let us choose an arbitrary \\(p = 0.5\\). Then \\(\\sum_{k=0}^n p^k = \\frac{1}{1 - 0.5} = 2\\). To complete the measure, we have to normalize it, so it sums to one, therefore \\(P(\\{k\\}) = \\frac{0.5^k}{2}\\) is a probability measure on \\(\\Omega\\). We could make it even more difficult by making this measure dependent on some parameter \\(\\alpha\\), but this is out of the scope of this introductory chapter. set.seed(1) pois_samp &lt;- rpois(n = 1000, lambda = 10) pois_samp &lt;- data.frame(x = pois_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:25, n = dpois(0:25, lambda = 10), type = &quot;theoretical_measure&quot;)) pois_plot &lt;- ggplot(data = pois_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(pois_plot) Exercise 1.13 Define a probability measure on \\((\\Omega = \\mathbb{Z}, 2^{\\mathbb{Z}})\\). Define a probability measure such that \\(P(\\omega) &gt; 0, \\forall \\omega \\in \\Omega\\). R: Implement a random generator that will generate samples with the relative frequency that corresponds to your probability measure. compare relative frequencies with theoretical probability measure . Solution. \\(P(0) = 1, P(\\omega) = 0, \\forall \\omega \\neq 0\\). \\(P(\\{k\\}) = \\sum_{k = -\\infty}^{\\infty} \\frac{p(1 - p)^{|k|}}{2^{1 - 1_0(k)}}\\), where \\(1_0(k)\\) is the indicator function, which equals to one if \\(k\\) is 0, and equals to zero in every other case. n &lt;- 1000 geom_samps &lt;- rgeom(n, prob = 0.5) sign_samps &lt;- sample(c(FALSE, TRUE), size = n, replace = TRUE) geom_samps[sign_samps] &lt;- -geom_samps[sign_samps] my_pmf &lt;- function (k, p) { indic &lt;- rep(1, length(k)) indic[k == 0] &lt;- 0 return ((p * (1 - p)^(abs(k))) / 2^indic) } geom_samps &lt;- data.frame(x = geom_samps) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = -10:10, n = my_pmf(-10:10, 0.5), type = &quot;theoretical_measure&quot;)) geom_plot &lt;- ggplot(data = geom_samps, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geom_plot) Exercise 1.14 Define a probability measure on \\(\\Omega = \\{1,2,3,4,5,6\\}\\) with parameter \\(m \\in \\{1,2,3,4,5,6\\}\\), so that the probability of outcome at distance \\(1\\) from \\(m\\) is half of the probability at distance \\(0\\), at distance \\(2\\) is half of the probability at distance \\(1\\), etc. R: Implement a random generator that will generate samples with the relative frequency that corresponds to your probability measure. compare relative frequencies with theoretical probability measure . Solution. \\(P(\\{k\\}) = \\frac{\\frac{1}{2}^{|m - k|}}{\\sum_{i=1}^6 \\frac{1}{2}^{|m - i|}}\\) n &lt;- 10000 m &lt;- 4 my_pmf &lt;- function (k, m) { denom &lt;- sum(0.5^abs(m - 1:6)) return (0.5^abs(m - k) / denom) } samps &lt;- c() for (i in 1:n) { a &lt;- sample(1:6, 1) a_val &lt;- my_pmf(a, m) prob &lt;- runif(1) if (prob &lt; a_val) { samps &lt;- c(samps, a) } } samps &lt;- data.frame(x = samps) %&gt;% count(x) %&gt;% mutate(n = n / length(samps), type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 1:6, n = my_pmf(1:6, m), type = &quot;theoretical_measure&quot;)) my_plot &lt;- ggplot(data = samps, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(my_plot) "],
["uprobspaces.html", "Chapter 2 Uncountable probability spaces 2.1 Borel sets 2.2 Lebesgue measure", " Chapter 2 Uncountable probability spaces This chapter deals with uncountable probability spaces. The students are expected to acquire the following knowledge: Theoretical Understand Borel sets and identify them. Estimate Lebesgue measure for different sets. Know when sets are Borel-measurable. Understanding of countable and uncountable sets. R uniform sampling recursion Cantor sets factors 2.1 Borel sets Exercise 1.1 Prove that the intersection of two sigma algebras on \\(\\Omega\\) is a sigma algebra. Prove that the collection of all open subsets \\((a,b)\\) on \\((0,1]\\) is not a sigma algebra of \\((0,1]\\). Solution. Empty set: \\[\\begin{equation} \\emptyset \\in \\mathcal{A} \\wedge \\emptyset \\in \\mathcal{B} \\Rightarrow \\emptyset \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Complement: \\[\\begin{equation} \\text{Let } A \\in \\mathcal{A} \\cap \\mathcal{B} \\Rightarrow A \\in \\mathcal{A} \\wedge A \\in \\mathcal{B} \\Rightarrow A^c \\in \\mathcal{A} \\wedge A^c \\in \\mathcal{B} \\Rightarrow A^c \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Countable additivity: Let \\(\\{A_i\\}\\) be a countable sequence of subsets in \\(\\mathcal{A} \\cap \\mathcal{B}\\). \\[\\begin{equation} \\forall i: A_i \\in \\mathcal{A} \\cap \\mathcal{B} \\Rightarrow A_i \\in \\mathcal{A} \\wedge A_i \\in \\mathcal{B} \\Rightarrow \\cup A_i \\in \\mathcal{A} \\wedge \\cup A_i \\in \\mathcal{B} \\Rightarrow \\cup A_i \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Let \\(A\\) denote the collection of all open subsets \\((a,b)\\) on \\((0,1]\\). Then \\((0,1) \\in A\\). But \\((0,1)^c = 1 \\notin A\\). Exercise 1.2 Show that \\(\\mathcal{C} = \\sigma(\\mathcal{C})\\) if and only if \\(\\mathcal{C}\\) is a sigma algebra. Solution. “\\(\\Rightarrow\\)” This follows from the definition of a generated sigma algebra. “\\(\\Leftarrow\\)” Let \\(\\mathcal{F} = \\cap_i F_i\\) be the intersection of all sigma algebras that contain \\(\\mathcal{C}\\). Then \\(\\sigma(\\mathcal{C}) = \\mathcal{F}\\). Additionally, \\(\\forall i: \\mathcal{C} \\in F_i\\). So each \\(F_i\\) can be written as \\(F_i = \\mathcal{C} \\cup D\\), where \\(D\\) are the rest of the elements in the sigma algebra. In other words, each sigma algebra in the collection contains at least \\(\\mathcal{C}\\), but can contain other elements. Now for some \\(j\\), \\(F_j = \\mathcal{C}\\) as \\(\\{F_i\\}\\) contains all sigma algebras that contain \\(\\mathcal{C}\\) and \\(\\mathcal{C}\\) is such a sigma algebra. Since this is the smallest subset in the intersection it follows that \\(\\sigma(\\mathcal{C}) = \\mathcal{F} = \\mathcal{C}\\). Exercise 1.3 Let \\(\\mathcal{C}\\) and \\(\\mathcal{D}\\) be two collections of subsets on \\(\\Omega\\) such that \\(\\mathcal{C} \\subset \\mathcal{D}\\). Prove that \\(\\sigma(\\mathcal{C}) \\subset \\sigma(\\mathcal{D})\\). Solution. \\(\\sigma(\\mathcal{D})\\) is a sigma algebra that contains \\(\\mathcal{D}\\). It follows that \\(\\sigma(\\mathcal{D})\\) is a sigma algebra that contains \\(\\mathcal{C}\\). Let us write \\(\\sigma(\\mathcal{C}) = \\cap_i F_i\\), where \\(\\{F_i\\}\\) is the collection of all sigma algebras that contain \\(\\mathcal{C}\\). Since \\(\\sigma(\\mathcal{D})\\) is such a sigma algebra, there exists an index \\(j\\), so that \\(F_j = \\sigma(\\mathcal{D})\\). Then we can write \\[\\begin{align} \\sigma(\\mathcal{C}) &amp;= (\\cap_{i \\neq j} F_i) \\cap \\sigma(\\mathcal{D}) \\\\ &amp;\\subset \\sigma(\\mathcal{D}). \\end{align}\\] Exercise 1.4 R: Simulate from the 2nd, 3rd, and 4th Cantor sets and plot results in a meaningful way. set.seed(1) nsamps &lt;- 100 cantor_n &lt;- function (n) { if (n &lt;= 0) { return (c(0, 1)) } cantor_prev &lt;- cantor_n(n - 1) return (c(cantor_prev / 3, 2/3 + cantor_prev / 3)) } sample_cantor &lt;- function (n, nsamps) { samps &lt;- vector(mode = &quot;numeric&quot;, length = nsamps) cant &lt;- cantor_n(n) mat &lt;- matrix(cant, ncol = 2, byrow = TRUE) for(i in 1:nsamps) { ind &lt;- sample(1:nrow(mat), 1) bounds &lt;- mat[ind, ] samps[i] &lt;- runif(1, min = bounds[1], max = bounds[2]) } return(samps) } depth_2 &lt;- sample_cantor(2, nsamps) depth_3 &lt;- sample_cantor(3, nsamps) depth_4 &lt;- sample_cantor(4, nsamps) plot_data &lt;- data.frame(samps = c(depth_2, depth_3, depth_4), depth = c(rep(2, nsamps), rep(3, nsamps), rep(4, nsamps))) cantor_plot &lt;- ggplot(data = plot_data, aes(x = samps)) + geom_linerange(aes(ymin = 0, ymax = depth, col = factor(depth))) + ylab(&quot;depth&quot;) plot(cantor_plot) Exercise 1.5 Prove that the following subsets of \\((0,1]\\) are Borel-measurable. Any countable set. The set of numbers in (0,1] whose decimal expansion does not contain 7. Solution. This follows directly from the fact that every countable set is a union of singletons, whose measure is 0. Let us first look at numbers which have a 7 as the first decimal numbers. Their measure is 0.1. Then we take all the numbers with a 7 as the second decimal number (excluding those who already have it as the first). These have the measure 0.01, and there are 9 of them, so their total measure is 0.09. We can continue to do so infinitely many times. At each \\(n\\), we have the measure of the intervals which is \\(10^n\\) and the number of those intervals is \\(9^{n-1}\\). Now \\[\\begin{align} \\lambda(A) &amp;= 1 - \\sum_{n = 0}^{\\infty} \\frac{9^n}{10^{n+1}} \\\\ &amp;= 1 - \\frac{1}{10} \\sum_{n = 0}^{\\infty} (\\frac{9}{10})^n \\\\ &amp;= 1 - \\frac{1}{10} \\frac{10}{1} \\\\ &amp;= 0. \\end{align}\\] Since we have shown that the measure of the set is \\(0\\), we have also shown that the set is measurable. Exercise 1.6 Let \\(\\Omega = [0,1]\\), and let \\(\\mathcal{F}_3\\) consist of all countable subsets of \\(\\Omega\\), and all subsets of \\(\\Omega\\) having a countable complement. Show that \\(\\mathcal{F}_3\\) is a sigma algebra. Let us define \\(P(A)=0\\) if \\(A\\) is countable, and \\(P(A) = 1\\) if \\(A\\) has a countable complement. Is \\((\\Omega, \\mathcal{F}_3, P)\\) a legitimate probability space? Solution. The empty set is countable, therefore it is in \\(\\mathcal{F}_3\\). For any \\(A \\in \\mathcal{F}_3\\). If \\(A\\) is countable, then \\(A^c\\) has a countable complement and is in \\(\\mathcal{F}_3\\). If \\(A\\) is uncountable, then it has a countable complement \\(A^c\\) which is therefore also in \\(\\mathcal{F}_3\\). We are left with showing countable additivity. Let \\(\\{A_i\\}\\) be an arbitrary collection of sets in \\(\\mathcal{F}_3\\). We will look at two possibilities. First let all \\(A_i\\) be countable. A countable union of countable sets is countable, and therefore in \\(\\mathcal{F}_3\\). Second, let at least one \\(A_i\\) be uncountable. It follows that it has a countable complement. We can write \\[\\begin{equation} (\\cup_{i=1}^{\\infty} A_i)^c = \\cap_{i=1}^{\\infty} A_i^c. \\end{equation}\\] Since at least one \\(A_i^c\\) on the right side is countable, the whole intersection is countable, and therefore the union has a countable complement. It follows that the union is in \\(\\mathcal{F}_3\\). The tuple \\((\\Omega, \\mathcal{F}_3)\\) is a measurable space. Therefore, we only need to check whether \\(P\\) is a probability measure. The measure of the empty set is zero as it is countable. We have to check for countable additivity. Let us look at three situations. Let \\(A_i\\) be disjoint sets. First, let all \\(A_i\\) be countable. \\[\\begin{equation} P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty}P( A_i)) = 0. \\end{equation}\\] Since the union is countable, the above equation holds. Second, let exactly one \\(A_i\\) be uncountable. W.L.O.G. let that be \\(A_1\\). Then \\[\\begin{equation} P(\\cup_{i=1}^{\\infty} A_i) = 1 + \\sum_{i=2}^{\\infty}P( A_i)) = 1. \\end{equation}\\] Since the union is uncountable, the above equation holds. Third, let at least two \\(A_i\\) be uncountable. We have to check whether it is possible for two uncountable sets in \\(\\mathcal{F}_3\\) to be disjoint. If that is possible, then their measures would sum to more than one and \\(P\\) would not be a probability measure. W.L.O.G. let \\(A_1\\) and \\(A_2\\) be uncountable. Then we have \\[\\begin{equation} A_1 \\cap A_2 = (A_1^c \\cup A_2^c)^c. \\end{equation}\\] Now \\(A_1^c\\) and \\(A_2^c\\) are countable and their union is therefore countable. Let \\(B = A_1^c \\cup A_2^c\\). So the intersection of \\(A_1\\) and \\(A_2\\) equals the complement of \\(B\\), which is countable. For the intersection to be the empty set, \\(B\\) would have to equal to \\(\\Omega\\). But \\(\\Omega\\) is uncountable and therefore \\(B\\) can not equal to \\(\\Omega\\). It follows that two uncountable sets in \\(\\mathcal{F}_3\\) can not have an empty intersection. Therefore the tuple is a legitimate probability space. 2.2 Lebesgue measure Exercise 1.7 Show that the Lebesgue measure of rational numbers on \\([0,1]\\) is 0. R: Implement a random number generator, which generates uniform samples of irrational numbers in \\([0,1]\\) by uniformly sampling from \\([0,1]\\) and rejecting a sample if it is rational. Solution. There are a countable number of rational numbers. Therefore, we can write \\[\\begin{align} \\lambda(\\mathbb{Q}) &amp;= \\lambda(\\cup_{i = 1}^{\\infty} q_i) &amp;\\\\ &amp;= \\sum_{i = 1}^{\\infty} \\lambda(q_i) &amp;\\text{ (countable additivity)} \\\\ &amp;= \\sum_{i = 1}^{\\infty} 0 &amp;\\text{ (Lebesgue measure of a singleton)} \\\\ &amp;= 0. \\end{align}\\] Exercise 2.1 Prove that the Lebesgue measure of \\(\\mathbb{R}\\) is infinity. Paradox. Show that the cardinality of \\(\\mathbb{R}\\) and \\((0,1)\\) is the same, while their Lebesgue measures are infinity and one respectably. Solution. Let \\(a_i\\) be the \\(i\\)-th integer for \\(i \\in \\mathbb{Z}\\). We can write \\(\\mathbb{R} = \\cup_{-\\infty}^{\\infty} (a_i, a_{i + 1}]\\). \\[\\begin{align} \\lambda(\\mathbb{R}) &amp;= \\lambda(\\cup_{i = -\\infty}^{\\infty} (a_i, a_{i + 1}]) \\\\ &amp;= \\lambda(\\lim_{n \\rightarrow \\infty} \\cup_{i = -n}^{n} (a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\lambda(\\cup_{i = -n}^{n} (a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\sum_{i = -n}^{n} \\lambda((a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\sum_{i = -n}^{n} 1 \\\\ &amp;= \\lim_{n \\rightarrow \\infty} 2n \\\\ &amp;= \\infty. \\end{align}\\] We need to find a bijection between \\(\\mathbb{R}\\) and \\((0,1)\\). A well-known function that maps from a bounded interval to \\(\\mathbb{R}\\) is the tangent. To make the bijection easier to achieve, we will take the inverse, which maps from \\(\\mathbb{R}\\) to \\((-\\frac{\\pi}{2}, \\frac{\\pi}{2})\\). However, we need to change the function so it maps to \\((0,1)\\). First we add \\(\\frac{\\pi}{2}\\), so that we move the function above zero. Then we only have to divide by the max value, which in this case is \\(\\pi\\). So our bijection is \\[\\begin{equation} f(x) = \\frac{\\tan^{-1}(x) + \\frac{\\pi}{2}}{\\pi}. \\end{equation}\\] Exercise 2.2 Take the measure space \\((\\Omega_1 = (0,1], B_{(0,1], \\lambda})\\) (we know that this is a probability space on \\((0,1]\\)). Define an injective map (function) from \\(\\Omega_1\\) to \\(\\Omega_2 = \\{1,2,3,4,5,6\\}\\) such that the measure space \\((\\Omega_2, 2^{\\Omega_2}, \\lambda(f^{-1}()))\\) will be a discrete probability space with uniform probabilities (\\(P(\\omega) = \\frac{1}{6}, \\forall \\omega \\in \\Omega_2)\\). Is the map that you defined in (a) the only such map? How would you in the same fashion define an injective map that would result in a probability space that can be interpreted as a coin toss with probability \\(p\\) of heads? R: Use the map in (a) as a basis for a random generator for this fair die. Solution. In other words, we have to assign disjunct intervals of the same size to each element of \\(\\Omega_2\\). Therefore \\[\\begin{equation} f(x) = \\lceil 6x \\rceil. \\end{equation}\\] No, we could for example rearrange the order in which the intervals are mapped to integers. Additionally, we could have several disjoint intervals that mapped to the same integer, as long as the Lebesgue measure of their union would be \\(\\frac{1}{6}\\) and the function would remain injective. We have \\(\\Omega_3 = \\{0,1\\}\\), where zero represents heads and one represents tails. Then \\[\\begin{equation} f(x) = 0^{I_{A}(x)}, \\end{equation}\\] where \\(A = \\{y \\in (0,1] : y &lt; p\\}\\). set.seed(1) unif_s &lt;- runif(1000) die_s &lt;- ceiling(6 * unif_s) summary(as.factor(die_s)) ## 1 2 3 4 5 6 ## 166 154 200 146 166 168 "],
["condprob.html", "Chapter 3 Conditional probability 3.1 Calculating conditional probabilities 3.2 Conditional independence 3.3 Monty Hall problem", " Chapter 3 Conditional probability This chapter deals with conditional probability. The students are expected to acquire the following knowledge: Theoretical identify whether variables are independent calculation of conditional probabilities understanding of conditional dependence and independence how to apply Bayes’ theorem to solve difficult probabilistic questions R simulating conditional probabilities cumsum apply 3.1 Calculating conditional probabilities Exercise 1.1 A military officer is in charge of identifying enemy aircraft and shooting them down. He is able to positively identify an enemy airplane 95% of the time and positively identify a friendly airplane 90% of the time. Furthermore, 99% of the airplanes are friendly. When the officer identifies an airplane as an enemy airplane, what is the probability that it is not and they will shoot at a friendly airplane? Solution. Let \\(E = 0\\) denote that the observed plane is friendly and \\(E=1\\) that it is an enemy. Let \\(I = 0\\) denote that the officer identified it as friendly and \\(I = 1\\) as enemy. Then \\[\\begin{align} P(E = 0 | I = 1) &amp;= \\frac{P(I = 1 | E = 0)P(E = 0)}{P(I = 1)} \\\\ &amp;= \\frac{P(I = 1 | E = 0)P(E = 0)}{P(I = 1 | E = 0)P(E = 0) + P(I = 1 | E = 1)P(E = 1)} \\\\ &amp;= \\frac{0.1 \\times 0.99}{0.1 \\times 0.99 + 0.95 \\times 0.01} \\\\ &amp;= 0.91. \\end{align}\\] Exercise 1.2 R: Consider tossing a fair die. Let \\(A = \\{2,4,6\\}\\) and \\(B = \\{1,2,3,4\\}\\). Then \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{2}{3}\\) and \\(P(AB) = \\frac{1}{3}\\). Since \\(P(AB) = P(A)P(B)\\), the events \\(A\\) and \\(B\\) are independent. Simulate draws from the sample space and verify that the proportions are the same. Then find two events \\(C\\) and \\(D\\) that are not independent and repeat the simulation. set.seed(1) nsamps &lt;- 10000 tosses &lt;- sample(1:6, nsamps, replace = TRUE) PA &lt;- sum(tosses %in% c(2,4,6)) / nsamps PB &lt;- sum(tosses %in% c(1,2,3,4)) / nsamps PA * PB ## [1] 0.3283998 sum(tosses %in% c(2,4)) / nsamps ## [1] 0.3217 # Let C = {1,2} and D = {2,3} PC &lt;- sum(tosses %in% c(1,2)) / nsamps PD &lt;- sum(tosses %in% c(2,3)) / nsamps PC * PD ## [1] 0.1114867 sum(tosses %in% c(2)) / nsamps ## [1] 0.1631 Exercise 1.3 A machine reports the true value of a thrown 12-sided die 5 out of 6 times. If the machine reports a 1 has been tossed, what is the probability that it is actually a 1? Now let the machine only report whether a 1 has been tossed or not. Does the probability change? R: Use simulation to check your answers to a) and b). Solution. Let \\(T = 1\\) denote that the toss is 1 and \\(M = 1\\) that the machine reports a 1. \\[\\begin{align} P(T = 1 | M = 1) &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{P(M = 1)} \\\\ &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{\\sum_{k=1}^{12} P(M = 1 | T = k)P(T = k)} \\\\ &amp;= \\frac{\\frac{5}{6}\\frac{1}{12}}{\\frac{5}{6}\\frac{1}{12} + 11 \\frac{1}{6} \\frac{1}{11} \\frac{1}{12}} \\\\ &amp;= \\frac{5}{6}. \\end{align}\\] Yes. \\[\\begin{align} P(T = 1 | M = 1) &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{P(M = 1)} \\\\ &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{\\sum_{k=1}^{12} P(M = 1 | T = k)P(T = k)} \\\\ &amp;= \\frac{\\frac{5}{6}\\frac{1}{12}}{\\frac{5}{6}\\frac{1}{12} + 11 \\frac{1}{6} \\frac{1}{12}} \\\\ &amp;= \\frac{5}{16}. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 report_a &lt;- vector(mode = &quot;numeric&quot;, length = nsamps) report_b &lt;- vector(mode = &quot;logical&quot;, length = nsamps) truths &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:10000) { toss &lt;- sample(1:12, size = 1) truth &lt;- sample(c(TRUE, FALSE), size = 1, prob = c(5/6, 1/6)) truths[i] &lt;- truth if (truth) { report_a[i] &lt;- toss report_b[i] &lt;- toss == 1 } else { remaining &lt;- (1:12)[1:12 != toss] report_a[i] &lt;- sample(remaining, size = 1) report_b[i] &lt;- toss != 1 } } truth_a1 &lt;- truths[report_a == 1] sum(truth_a1) / length(truth_a1) ## [1] 0.8384075 truth_b1 &lt;- truths[report_b] sum(truth_b1) / length(truth_b1) ## [1] 0.3110339 Exercise 3.1 A coin is tossed independently \\(n\\) times. The probability of heads at each toss is \\(p\\). At each time \\(k\\), \\((k = 2,3,...,n)\\) we get a reward at time \\(k+1\\) if \\(k\\)-th toss was a head and the previous toss was a tail. Let \\(A_k\\) be the evebt that a reward is obtained at time \\(k\\). Are events \\(A_k\\) and \\(A_{k+1}\\) independent? Are events \\(A_k\\) and \\(A_{k+2}\\) independent? R: simulate 10 tosses 10000 times, where \\(p = 0.7\\). Check your answers to a) and b) by counting the frequencies of the events \\(A_5\\), \\(A_6\\), and \\(A_7\\). Solution. For \\(A_k\\) to happen, we need the tosses \\(k-2\\) and \\(k-1\\) be tails and heads respectively. For \\(A_{k+1}\\) to happen, we need tosses \\(k-1\\) and \\(k\\) be tails and heads respectively. As the toss \\(k-1\\) need to be heads for one and tails for the other, these two events can not happen simultaneously. Therefore the probability of their intersection is 0. But the probability of each of them separately is \\(p(1-p) &gt; 0\\). Therefore, they are not independent. For \\(A_k\\) to happen, we need the tosses \\(k-2\\) and \\(k-1\\) be tails and heads respectively. For \\(A_{k+2}\\) to happen, we need tosses \\(k\\) and \\(k+1\\) be tails and heads respectively. So the probability of intersection is \\(p^2(1-p)^2\\). And the probability of each separately is again \\(p(1-p)\\). Therefore, they are independent. set.seed(1) nsamps &lt;- 10000 p &lt;- 0.7 rewardA_5 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_6 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_7 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_56 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_57 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:nsamps) { samps &lt;- sample(c(0,1), size = 10, replace = TRUE, prob = c(0.7, 0.3)) rewardA_5[i] &lt;- (samps[4] == 0 &amp; samps[3] == 1) rewardA_6[i] &lt;- (samps[5] == 0 &amp; samps[4] == 1) rewardA_7[i] &lt;- (samps[6] == 0 &amp; samps[5] == 1) rewardA_56[i] &lt;- (rewardA_5[i] &amp; rewardA_6[i]) rewardA_57[i] &lt;- (rewardA_5[i] &amp; rewardA_7[i]) } sum(rewardA_5) / nsamps ## [1] 0.2141 sum(rewardA_6) / nsamps ## [1] 0.2122 sum(rewardA_7) / nsamps ## [1] 0.2107 sum(rewardA_56) / nsamps ## [1] 0 sum(rewardA_57) / nsamps ## [1] 0.0454 Exercise 1.6 A drawer contains two coins. One is an unbiased coin, the other is a biased coin, which will turn up heads with probability \\(p\\) and tails with probability \\(1-p\\). One coin is selected uniformly at random. The selected coin is tossed \\(n\\) times. The coin turns up heads \\(k\\) times and tails \\(n-k\\) times. What is the probability that the coin is biased? The selected coin is tossed repeatedly until it turns up heads \\(k\\) times. Given that it is tossed \\(n\\) times in total, what is the probability that the coin is biased? Solution. Let \\(B = 1\\) denote that the coin is biased and let \\(H = k\\) denote that we’ve seen \\(k\\) heads. \\[\\begin{align} P(B = 1 | H = k) &amp;= \\frac{P(H = k | B = 1)P(B = 1)}{P(H = k)} \\\\ &amp;= \\frac{P(H = k | B = 1)P(B = 1)}{P(H = k | B = 1)P(B = 1) + P(H = k | B = 0)P(B = 0)} \\\\ &amp;= \\frac{p^k(1-p)^{n-k} 0.5}{p^k(1-p)^{n-k} 0.5 + 0.5^{n+1}} \\\\ &amp;= \\frac{p^k(1-p)^{n-k}}{p^k(1-p)^{n-k} + 0.5^n}. \\end{align}\\] The same results as in a). The only difference between these two scenarios is that in b) the last throw must be heads. However, this holds for the biased and the unbiased coin and therefore does not affect the probability of the coin being biased. Exercise 1.7 Judy goes around the company for Women’s day and shares flowers. In every office she leaves a flower, if there is at least one woman inside. The probability that there’s a woman in the office is \\(\\frac{3}{5}\\). What is the probability that Judy leaves her first flower in the fourth office? Given that she has given away exactly three flowers in the first four offices, what is the probability that she gives her fourth flower in the eighth office? What is the probability that she leaves the second flower in the fifth office? What is the probability that she leaves the second flower in the fifth office, given that she did not leave the second flower in the second office? Judy needs a new supply of flowers immediately after the office, where she gives away her last flower. What is the probability that she visits at least five offices, if she starts with two flowers? R: simulate Judy’s walk 10000 times to check your answers a) - e). Solution. Let \\(X_i = k\\) denote the event that … \\(i\\)-th sample on the \\(k\\)-th run. Since the events are independent, we can multiply their probabilities to get \\[\\begin{equation} P(X_1 = 4) = 0.4^3 \\times 0.6 = 0.0384. \\end{equation}\\] Since the events are independent, the results is the same as in a). For this to be possible, she had to leave the first flower in one of the first four offices. Therefore there are four possibilities, and for each of those the probability is \\(0.4^3 \\times 0.6\\). Additionally, the probability that she leaves a flower in the fifth office is \\(0.6\\). So \\[\\begin{equation} P(X_2 = 5) = \\binom{4}{1} \\times 0.4^3 \\times 0.6^2 = 0.09216. \\end{equation}\\] We use Bayes’ theorem. \\[\\begin{align} P(X_2 = 5 | X_2 \\neq 2) &amp;= \\frac{P(X_2 \\neq 2 | X_2 = 5)P(X_2 = 5)}{P(X_2 \\neq 2)} \\\\ &amp;= \\frac{0.09216}{0.64} \\\\ &amp;= 0.144. \\end{align}\\] The denominator in the second equation can be calculated as follows. One of three things has to happen for the second not to be dealt in the second round. First, both are zero, so \\(0.4^2\\). Second, first is zero, and second is one, so \\(0.4 \\times 0.6\\). Third, the first is one and the second one zero, so \\(0.6 \\times 0.4\\). Summing these values we get \\(0.64\\). We will look at the complement, so the events that she gave away exactly two flowers after two, three and four offices. \\[\\begin{equation} P(X_2 \\geq 5) = 1 - 0.6^2 - 2 \\times 0.4 \\times 0.6^2 - 3 \\times 0.4^2 \\times 0.6^2 = 0.1792. \\end{equation}\\] The multiplying parts represent the possibilities of the first flower. set.seed(1) nsamps &lt;- 100000 Judyswalks &lt;- matrix(data = NA, nrow = nsamps, ncol = 8) for (i in 1:nsamps) { thiswalk &lt;- sample(c(0,1), size = 8, replace = TRUE, prob = c(0.4, 0.6)) Judyswalks[i, ] &lt;- thiswalk } csJudy &lt;- t(apply(Judyswalks, 1, cumsum)) # a sum(csJudy[ ,4] == 1 &amp; csJudy[ ,3] == 0) / nsamps ## [1] 0.03848 # b csJsubset &lt;- csJudy[csJudy[ ,4] == 3 &amp; csJudy[ ,3] == 2, ] sum(csJsubset[ ,8] == 4 &amp; csJsubset[ ,7] == 3) / nrow(csJsubset) ## [1] 0.03665893 # c sum(csJudy[ ,5] == 2 &amp; csJudy[ ,4] == 1) / nsamps ## [1] 0.09117 # d sum(csJudy[ ,5] == 2 &amp; csJudy[ ,4] == 1) / sum(csJudy[ ,2] != 2) ## [1] 0.1422398 # e sum(csJudy[ ,4] &lt; 2) / nsamps ## [1] 0.17818 3.2 Conditional independence Exercise 1.8 Describe: A real-world example of two events \\(A\\) and \\(B\\) that are dependent but become conditionally independent if conditioned on a third event \\(C\\). A real-world example of two events \\(A\\) and \\(B\\) that are independent, but become dependent if conditioned on some third event \\(C\\). Solution. Let \\(A\\) be the height of a person and let \\(B\\) be the person’s knowledge in the Dutch language. These events are dependent since the Dutch are known to be taller than average. However if \\(C\\) is the nationality of the person, then \\(A\\) and \\(B\\) are independent given \\(C\\). Let \\(A\\) be the event that Mary passes the exam and let \\(B\\) be the event that John passes the exam. These events are independent. However, if the event \\(C\\) is that Mary and John studied together, then \\(A\\) and \\(B\\) are conditionally dependent given \\(C\\). Exercise 3.2 We have two coins of identical appearance. We know that one is a fair coin and the other flips heads 80% of the time. We choose one of the two coins uniformly at random. We discard the coin that was not chosen. We now flip the chosen coin independently 10 times, producing a sequence \\(Y_1 = y_1\\), \\(Y_2 = y_2\\), …, \\(Y_10 = y_10\\). Intuitively, without doing and computation, are these random variables independent? Compute the probability \\(P(Y_1 = 1)\\). Compute the probabilities \\(P(Y_2 = 1 | Y_1 = 1)\\) and P(Y_{10} = 1 | Y_1 = 1,…,Y_9 = 1)$. Given your answers to b) and c), would you now change your answer to a)? If so, discuss why your intuition had failed. Solution. $P(Y_1 = 1) = 0.5 * 0.8 + 0.5 * 0.5 = 0.65. Since we know that \\(Y_1 = 1\\) this should change our view of the probability of the coin being biased or not. Let \\(B = 1\\) denote the event that the coin is biased and let \\(B = 0\\) denote that the coin is unbiased. By using marginal probability, we can write \\[\\begin{align} P(Y_2 = 1 | Y_1 = 1) &amp;= P(Y_2 = 1, B = 1 | Y_1 = 1) + P(Y_2 = 1, B = 0 | Y_1 = 1) \\\\ &amp;= \\sum_{k=1}^2 P(Y_2 = 1 | B = k, Y_1 = 1)P(B = k | Y_1 = 1) \\\\ &amp;= 0.8 \\frac{P(Y_1 = 1 | B = 1)P(B = 1)}{P(Y_1 = 1)} + 0.5 \\frac{P(Y_1 = 1 | B = 0)P(B = 0)}{P(Y_1 = 1)} \\\\ &amp;= 0.8 \\frac{0.8 \\times 0.5}{0.65} + 0.5 \\frac{0.5 \\times 0.5}{0.65} \\\\ &amp;\\approx 0.68. \\end{align}\\] For the other calculation we follow the same procedure. Let \\(X = 1\\) denote that first nine tosses are all heads (equivalent to \\(Y_1 = 1\\),…, \\(Y_9 = 1\\)). \\[\\begin{align} P(Y_{10} = 1 | X = 1) &amp;= P(Y_2 = 1, B = 1 | X = 1) + P(Y_2 = 1, B = 0 | X = 1) \\\\ &amp;= \\sum_{k=1}^2 P(Y_2 = 1 | B = k, X = 1)P(B = k | X = 1) \\\\ &amp;= 0.8 \\frac{P(X = 1 | B = 1)P(B = 1)}{P(X = 1)} + 0.5 \\frac{P(X = 1 | B = 0)P(B = 0)}{P(X = 1)} \\\\ &amp;= 0.8 \\frac{0.8^9 \\times 0.5}{0.5 \\times 0.8^9 + 0.5 \\times 0.5^9} + 0.5 \\frac{0.5^9 \\times 0.5}{0.5 \\times 0.8^9 + 0.5 \\times 0.5^9} \\\\ &amp;\\approx 0.8. \\end{align}\\] 3.3 Monty Hall problem The Monty Hall problem is a famous probability puzzle with non-intuitive outcome. Many established mathematicians and statisticians had problems solving it and many even disregarded the correct solution until they’ve seen the proof by simulation. Here we will show how it can be solved relatively simply with the use of Bayes’ theorem if we select the variables in a smart way. Exercise 3.3 (Monty Hall problem) A prize is placed at random behind one of three doors. You pick a door. Now Monty Hall chooses one of the other two doors, opens it and shows you that it is empty. He then gives you the opportunity to keep your door or switch to the other unopened door. Should you stay or switch? Use Bayes’ theorem to c alculate the probability of winning if you switch and if you do not. R: Check your answers in R. Solution. W.L.O.G. assume we always pick the first door. The host can only open door 2 or door 3, as he can not open the door we picked. Let \\(k \\in \\{2,3\\}\\). Let us first look at what happens if we do not change. Then we have \\[\\begin{align} P(\\text{car in 1} | \\text{open $k$}) &amp;= \\frac{P(\\text{open $k$} | \\text{car in 1})P(\\text{car in 1})}{P(\\text{open $k$})} \\\\ &amp;= \\frac{P(\\text{open $k$} | \\text{car in 1})P(\\text{car in 1})}{\\sum_{n=1}^3 P(\\text{open $k$} | \\text{car in $n$})P(\\text{car in $n$)}}. \\end{align}\\] The probability that he opened \\(k\\) if the car is in 1 is \\(\\frac{1}{2}\\), as he can choose between door 2 and 3 as both have a goat behind it. Let us look at the normalization constant. Whek \\(n = 1\\) we get the value in the nominator. When \\(n=k\\), we get 0, as he will not open the door if there’s a prize behind. The remaining option is that we select 1, the car is behind \\(k\\) and he opens the only door left. Since he can’t open 1 due to it being our pick and \\(k\\) due to having the prize, the probability of opening the remaining door is 1, and the prior probability of the car being behind this door is \\(\\frac{1}{3}\\). So we have \\[\\begin{align} P(\\text{car in 1} | \\text{open $k$}) &amp;= \\frac{\\frac{1}{2}\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} \\\\ &amp;= \\frac{1}{3}. \\end{align}\\] Now let us look at what happens if we do change. Let \\(k&#39; \\in \\{2,3\\}\\) be the door that is not opened. If we change, we select this door, so we have \\[\\begin{align} P(\\text{car in $k&#39;$} | \\text{open $k$}) &amp;= \\frac{P(\\text{open $k$} | \\text{car in $k&#39;$})P(\\text{car in $k&#39;$})}{P(\\text{open $k$})} \\\\ &amp;= \\frac{P(\\text{open $k$} | \\text{car in $k&#39;$})P(\\text{car in $k&#39;$})}{\\sum_{n=1}^3 P(\\text{open $k$} | \\text{car in $n$})P(\\text{car in $n$)}}. \\end{align}\\] The denominator stays the same, the only thing that is different from before is \\(P(\\text{open $k$} | \\text{car in $k&#39;$})\\). We have a situation where we initially selected door 1 and the car is in door \\(k&#39;\\). The probability that the host will open door \\(k\\) is then 1, as he can not pick any other door. So we have \\[\\begin{align} P(\\text{car in $k&#39;$} | \\text{open $k$}) &amp;= \\frac{\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} \\\\ &amp;= \\frac{2}{3}. \\end{align}\\] Therefore it makes sense to change the door. set.seed(1) nsamps &lt;- 1000 ifchange &lt;- vector(mode = &quot;logical&quot;, length = nsamps) ifstay &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:nsamps) { where_car &lt;- sample(c(1:3), 1) where_player &lt;- sample(c(1:3), 1) open_samp &lt;- (1:3)[where_car != (1:3) &amp; where_player != (1:3)] if (length(open_samp) == 1) { where_open &lt;- open_samp } else { where_open &lt;- sample(open_samp, 1) } ifstay[i] &lt;- where_car == where_player where_ifchange &lt;- (1:3)[where_open != (1:3) &amp; where_player != (1:3)] ifchange[i] &lt;- where_ifchange == where_car } sum(ifstay) / nsamps ## [1] 0.333 sum(ifchange) / nsamps ## [1] 0.667 "],
["rvs.html", "Chapter 4 Random variables 4.1 General properties and calculations 4.2 Discrete random variables 4.3 Continuous random variables 4.4 Transformations", " Chapter 4 Random variables This chapter deals with random variables and their distributions. The students are expected to acquire the following knowledge: Theoretical Identification of random variables. Convolutions of random variables. Derivation of PDF, PMF, CDF, and quantile function. Definitions and properties of common discrete random variables. Definitions and properties of common continuous random variables. Transforming univariate random variables. R Familiarize with PDF, PMF, CDF, and quantile functions for several distributions. Visual inspection of probability distributions. Analytical and empirical calculation of probabilities based on distributions. New R functions for plotting (for example, facet_wrap). Creating random number generators based on the Uniform distribution. 4.1 General properties and calculations Exercise 1.1 Which of the functions below are valid CDFs? Find their respective densities. R: Plot the three functions. \\[\\begin{equation} F(x) = \\begin{cases} 1 - e^{-x^2} &amp; x \\geq 0 \\\\ 0 &amp; x &lt; 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} e^{-\\frac{1}{x}} &amp; x &gt; 0 \\\\ 0 &amp; x \\leq 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x \\leq 0 \\\\ \\frac{1}{3} &amp; 0 &lt; x \\leq \\frac{1}{2} \\\\ 1 &amp; x &gt; \\frac{1}{2}. \\end{cases} \\end{equation}\\] Solution. Yes. First, let us check the limits. \\(\\lim_{x \\rightarrow -\\infty} (0) = 0\\). \\(\\lim_{x \\rightarrow \\infty} (1 - e^{-x^2}) = 1 - \\lim_{x \\rightarrow \\infty} e^{-x^2} = 1 - 0 = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(1 - e^{-x^2} \\geq 1 - e^{-y^2}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} 1 - e^{-\\epsilon^2} = 1 - \\lim_{\\epsilon \\downarrow 0} e^{-\\epsilon^2} = 1 - 1 = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} 1 - e^{-x^2} = 2xe^{-x^2}.\\) Students are encouraged to check that this is a proper PDF. Yes. First, let us check the limits. $_{x -} (0) = 0 and \\(\\lim_{x \\rightarrow \\infty} (e^{-\\frac{1}{x}}) = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(e^{-\\frac{1}{x}} \\geq e^{-\\frac{1}{y}}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} e^{-\\frac{1}{\\epsilon}} = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} e^{-\\frac{1}{x}} = \\frac{1}{x^2}e^{-\\frac{1}{x}}.\\) Students are encouraged to check that this is a proper PDF. No. The function is not right continuous as \\(F(\\frac{1}{2}) = \\frac{1}{3}\\), but \\(\\lim_{\\epsilon \\downarrow 0} F(\\frac{1}{2} + \\epsilon) = 1\\). f1 &lt;- function (x) { tmp &lt;- 1 - exp(-x^2) tmp[x &lt; 0] &lt;- 0 return(tmp) } f2 &lt;- function (x) { tmp &lt;- exp(-(1 / x)) tmp[x &lt;= 0] &lt;- 0 return(tmp) } f3 &lt;- function (x) { tmp &lt;- x tmp[x == x] &lt;- 1 tmp[x &lt;= 0.5] &lt;- 1/3 tmp[x &lt;= 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 20, by = 0.001), f1 = f1(x), f2 = f2(x), f3 = f3(x)) %&gt;% melt(id.vars = &quot;x&quot;) # geo_plot &lt;- ggplot(data = data.frame(x = seq(-1, 10, by = 0.01)), aes(x = x)) + # stat_function(aes(color = &quot;f1&quot;), fun = f1) + # stat_function(aes(color = &quot;f2&quot;), fun = f2) + # stat_function(aes(color = &quot;f3&quot;), fun = f3) + # geom_hline(yintercept = 1) # plot(geo_plot) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = value, color = variable)) + geom_hline(yintercept = 1) + geom_line() plot(cdf_plot) Exercise 4.1 Let \\(X\\) be a random variable with CDF \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\frac{x^2}{2} &amp; 0 \\leq x &lt; 1 \\\\ \\frac{1}{2} + \\frac{p}{2} &amp; 1 \\leq x &lt; 2 \\\\ \\frac{1}{2} + \\frac{p}{2} + \\frac{1 - p}{2} &amp; x \\geq 2 \\end{cases} \\end{equation}\\] R: Plot this CDF for \\(p = 0.3\\). Is it a discrete, continuous, or mixed random varible? Find the probability density/mass of \\(X\\). f1 &lt;- function (x, p) { tmp &lt;- x tmp[x &gt;= 2] &lt;- 0.5 + (p * 0.5) + ((1-p) * 0.5) tmp[x &lt; 2] &lt;- 0.5 + (p * 0.5) tmp[x &lt; 1] &lt;- (x[x &lt; 1])^2 / 2 tmp[x &lt; 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 5, by = 0.001), y = f1(x, 0.3)) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = y)) + geom_hline(yintercept = 1) + geom_line(color = &quot;blue&quot;) plot(cdf_plot) Solution. \\(X\\) is a mixed random variable. Since \\(X\\) is a mixed random variable, we have to find the PDF of the continuous part and the PMF of the discrete part. We get the continuous part by differentiating the corresponding CDF, \\(\\frac{d}{dx}\\frac{x^2}{2} = x\\). So the PDF, when \\(0 \\leq x &lt; 1\\), is \\(p(x) = x\\). Let us look at the discrete part now. It has two steps, so this is a discrete distribution with two outcomes – numbers two and three. The first happens with probability \\(\\frac{p}{2}\\), and the second with probability \\(\\frac{1 - p}{2}\\). This reminds us of the Bernoulli distribution, the only difference is that the probabilities of outcomes are halved, as they need to be suitably normalized. So the PMF for the discrete part is \\(P(X = x) = (\\frac{p}{2})^{2 - \\lfloor x \\rfloor} (\\frac{1 - p}{2})^{\\lfloor x \\rfloor - 1}\\). Exercise 4.2 (Convolutions) Convolutions are probability distributions that correspond to sums of independent random variables. Let \\(X\\) and \\(Y\\) be independent discrete variables. Find the PMF of \\(Z = X + Y\\). Hint: Use the law of total probability. Let \\(X\\) and \\(Y\\) be independent continuous variables. Find the PDF of \\(Z = X + Y\\) Hint: Start with the CDF. Solution. \\[\\begin{align} P(Z = z) &amp;= P(X + Y = z) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + Y = z | Y = k) P(Y = k) &amp; \\text{ (law of total probability)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z | Y = k) P(Y = k) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z) P(Y = k) &amp; \\text{ (independence of $X$ and $Y$)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X = z - k) P(Y = k). &amp; \\end{align}\\] Let \\(f\\) and \\(g\\) be the PDFs of \\(X\\) and \\(Y\\) respectively. \\[\\begin{align} F(z) &amp;= P(Z &lt; z) \\\\ &amp;= P(X + Y &lt; z) \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + Y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X &lt; z - y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy \\end{align}\\] Now \\[\\begin{align} p(z) &amp;= \\frac{d}{dz} F(z) &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\frac{d}{dz}\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} f(z - y) g(y) dy &amp; \\text{ (fundamental theorem of calculus)}. \\end{align}\\] 4.2 Discrete random variables Exercise 4.3 (Binomial random variable) Let \\(X_k\\), \\(k = 1,...,n\\), be random variables with the Bernoulli measure as the PMF with \\(p = 0.4\\). Let \\(X = \\sum_{k=1}^n\\). We call \\(X_k\\) a Bernoulli random variable with parameter \\(p \\in (0,1)\\). Find the CDF of \\(X_k\\). Find PDF of \\(X\\). This is a Binomial random variable with support in \\(\\{0,1,2,...,n\\}\\) and parameters \\(p \\in (0,1)\\) and \\(n \\in \\mathbb{N}_0\\). We denote \\[\\begin{equation} X | n,p \\sim \\text{Binomial}(n,p). \\end{equation}\\] Find CDF of \\(X\\). R: Simulate from the binomial distribution with \\(n = 10\\) and \\(p = 0.5\\), and from \\(n\\) Bernoulli distributions with \\(p = 0.5\\). Visually compare the sum of Bernoullis and the binomial. Hint: there is no standard function like rpois for a Bernoulli random variable. Check exercise 1.11 to find out how to sample from a Bernoulli distribution. Solution. There are two outcomes – zero and one. Zero happens with probability \\(1 - p\\). Therefore \\[\\begin{equation} F(k) = \\begin{cases} 0 &amp; k &lt; 0 \\\\ 1 - p &amp; 0 \\leq k &lt; 1 \\\\ 1 &amp; k \\geq 1. \\end{cases} \\end{equation}\\] For the probability of \\(X\\) to be equal to some \\(k \\leq n\\), exactly \\(k\\) Bernoulli variables need to be one, and the others zero. So \\(p^k(1-p)^{n-k}\\). There are \\(\\binom{n}{k}\\) such possible arrangements. Therefore \\[\\begin{align} P(X = k) = P(\\sum X_k = 2) = \\binom{n}{k} p^k (1 - p)^{n-k}. \\end{align}\\] \\[\\begin{equation} F(k) = \\sum_{i = 1}^{\\lfloor k \\rfloor} \\binom{n}{i} p^i (1 - p)^{n - i} \\end{equation}\\] set.seed(1) nsamps &lt;- 10000 binom_samp &lt;- rbinom(nsamps, size = 10, prob = 0.5) bernoulli_mat &lt;- matrix(data = NA, nrow = nsamps, ncol = 10) for (i in 1:nsamps) { bernoulli_mat[i, ] &lt;- rbinom(10, size = 1, prob = 0.5) } bern_samp &lt;- apply(bernoulli_mat, 1, sum) b_data &lt;- tibble(x = c(binom_samp, bern_samp), type = c(rep(&quot;binomial&quot;, 10000), rep(&quot;Bernoulli_sum&quot;, 10000))) b_plot &lt;- ggplot(data = b_data, aes(x = x, fill = type)) + geom_bar(position = &quot;dodge&quot;) plot(b_plot) Exercise 4.4 (Geometric random variable) A variable with PMF \\[\\begin{equation} P(k) = p(1-p)^k \\end{equation}\\] is a geometric random variable with support in non-negative integers. It has one positive parameter \\(p\\). We denote \\[\\begin{equation} X | p \\sim \\text{Geometric}(p) \\end{equation}\\] Derive the CDF of a geometric random variable. R: Draw 1000 samples from the geometric distribution with \\(p\\) = 0.3$ and compare their frequencies to theoretical values. Solution. \\[\\begin{align} P(X \\leq k) &amp;= \\sum_{i = 0}^k p(1-p)^i \\\\ &amp;= p \\sum_{i = 0}^k (1-p)^i \\\\ &amp;= p \\frac{1 - (1-p)^{k+1}}{1 - (1 - p)} \\\\ &amp;= 1 - (1-p)^{k + 1} \\end{align}\\] set.seed(1) geo_samp &lt;- rgeom(n = 1000, prob = 0.3) geo_samp &lt;- data.frame(x = geo_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dgeom(0:20, prob = 0.3), type = &quot;theoretical_measure&quot;)) geo_plot &lt;- ggplot(data = geo_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geo_plot) Exercise 4.5 (Poisson random variable) A variable with PMF \\[\\begin{equation} P(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\end{equation}\\] is a Poisson random variable with support in non-negative integers. It has one positive parameter \\(\\lambda\\), which also represents its mean value and variance (a measure of the deviation of the values from the mean – more on mean and variance in the next chapter). We denote \\[\\begin{equation} X | \\lambda \\sim \\text{Poisson}(\\lambda). \\end{equation}\\] This distribution is usually the default choice for modeling counts. We have already encountered a Poisson random variable in exercise 1.12, where we also sampled from this distribution. The CDF of a Poisson random variable is \\(P(X &lt;= x) = e^{-\\lambda} \\sum_{i=0}^x \\frac{\\lambda^{i}}{i!}\\). R: Draw 1000 samples from the Poisson distribution with \\(p\\) = 0.3$ and compare their empirical cumulative distribution function with the theoretical CDF. set.seed(1) pois_samp &lt;- rpois(n = 1000, lambda = 5) pois_samp &lt;- data.frame(x = pois_samp) pois_plot &lt;- ggplot(data = pois_samp, aes(x = x, colour = &quot;ECDF&quot;)) + stat_ecdf(geom = &quot;step&quot;) + geom_step(data = tibble(x = 0:17, y = ppois(x, 5)), aes(x = x, y = y, colour = &quot;CDF&quot;)) + scale_colour_manual(&quot;Lgend title&quot;, values = c(&quot;black&quot;, &quot;red&quot;)) plot(pois_plot) Exercise 4.6 (Negative binomial random variable) A variable with PMF \\[\\begin{equation} p(k) = \\binom{k + r - 1}{k}(1-p)^r p^k \\end{equation}\\] is a negative binomial random variable with support in non-negative integers. It has two parameters \\(r &gt; 0\\) and \\(p \\in (0,1)\\). We denote \\[\\begin{equation} X | r,p \\sim \\text{NB}(r,p). \\end{equation}\\] Let us reparameterize the negative binomial distribution with \\(q = 1 - p\\). Find the PDF of \\(X \\sim \\text{NB}(1, q)\\). Do you recognize this distribution? Show that the sum of two negative binomial random variables with the same \\(p\\) is also a negative binomial random variable. Hint: Use the fact that the number of ways to place \\(n\\) indistinct balls into \\(k\\) boxes is \\(\\binom{n + k - 1}{n}\\). R: Draw samples from \\(X \\sim \\text{NB}(5, 0.4)\\) and \\(Y \\sim \\text{NB}(3, 0.4)\\). Draw samples from \\(Z = X + Y\\), where you use the parameters calculated in b). Plot both distributions, their sum, and \\(Z\\) using facet_wrap. Be careful, as R uses a different parameterization size=\\(r\\) and prob=\\(1 - p\\). Solution. \\[\\begin{align} P(X = k) &amp;= \\binom{k + 1 - 1}{k}q^1 (1-q)^k \\\\ &amp;= q(1-q)^k. \\end{align}\\] This is the geometric distribution. Let \\(X \\sim \\text{NB}(r_1, p)\\) and \\(Y \\sim \\text{NB}(r_2, p)\\). Let \\(Z = X + Y\\). \\[\\begin{align} P(Z = z) &amp;= \\sum_{k = 0}^{\\infty} P(X = z - k)P(Y = k) &amp; \\text{ if k &lt; 0, then the probabilities are 0} \\\\ &amp;= \\sum_{k = 0}^{z} P(X = z - k)P(Y = k) &amp; \\text{ if k &gt; z, then the probabilities are 0} \\\\ &amp;= \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k}(1 - p)^{r_1} p^{z - k} \\binom{k + r_2 - 1}{k}(1 - p)^{r_2} p^{k} &amp; \\\\ &amp;= \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}(1 - p)^{r_1 + r_2} p^{z} &amp; \\\\ &amp;= (1 - p)^{r_1 + r_2} p^{z} \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}&amp; \\end{align}\\] The part before the sum reminds us of the negative binomial distribution with parameters \\(r_1 + r_2\\) and \\(p\\). To complete this term to the negative binomial PDF we need \\(\\binom{z + r_1 + r_2 -1}{z}\\). So the only thing we need to prove is that the sum equals this term. Both terms in the sum can be interpreted as numbers of ways to place a number of balls into boxes. For the left term it is \\(z-k\\) balls into \\(r_1\\) boxes, and for the right \\(k\\) balls into \\(r_2\\) boxes. For each \\(k\\) we are distributing \\(z\\) balls in total. By summing over all \\(k\\), we actually get all the possible placements of \\(z\\) balls into \\(r_1 + r_2\\) boxes. Therefore \\[\\begin{align} P(Z = z) &amp;= (1 - p)^{r_1 + r_2} p^{z} \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}&amp; \\\\ &amp;= \\binom{z + r_1 + r_2 -1}{z} (1 - p)^{r_1 + r_2} p^{z}. \\end{align}\\] From this it also follows that the sum of geometric distributions with the same parameter is a negative binomial distribution. \\(Z \\sim \\text{NB}(8, 0.4)\\). set.seed(1) nsamps &lt;- 10000 x &lt;- rnbinom(nsamps, size = 5, prob = 0.6) y &lt;- rnbinom(nsamps, size = 3, prob = 0.6) xpy &lt;- x + y z &lt;- rnbinom(nsamps, size = 8, prob = 0.6) samps &lt;- tibble(x, y, xpy, z) samps &lt;- melt(samps) ggplot(data = samps, aes(x = value)) + geom_bar() + facet_wrap(~ variable) 4.3 Continuous random variables Exercise 4.7 (Exponential random variable) A variable \\(X\\) with PDF \\(\\lambda e^{-\\lambda x}\\) is an exponential random variable with support in non-negative real numbers. It has one positive parameter \\(\\lambda\\). We denote \\[\\begin{equation} X | \\lambda \\sim \\text{Exp}(\\lambda). \\end{equation}\\] Derive the CDF of an exponential random variable. Derive the quantile function of an exponential random variable. Calculate the probability \\(P(1 \\leq X \\leq 3)\\), where \\(X \\sim \\text{Exp(1.5)}\\). R: Check your answer to c) with a simulation (rexp). Plot the probability in a meaningful way. R: Implement PDF, CDF, and the quantile function and compare their values with corresponding R functions visually. Hint: use the size parameter in geom_line to make one of the curves wider. Solution. \\[\\begin{align} F(x) &amp;= \\int_{0}^{x} \\lambda e^{-\\lambda t} dt \\\\ &amp;= \\lambda \\int_{0}^{x} e^{-\\lambda t} dt \\\\ &amp;= \\lambda (\\frac{1}{-\\lambda}e^{-\\lambda t} |_{0}^{x}) \\\\ &amp;= \\lambda(\\frac{1}{\\lambda} - \\frac{1}{\\lambda} e^{-\\lambda x}) \\\\ &amp;= 1 - e^{-\\lambda x}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(x)) &amp;= x \\\\ 1 - e^{-\\lambda F^{-1}(x)} &amp;= x \\\\ e^{-\\lambda F^{-1}(x)} &amp;= 1 - x \\\\ -\\lambda F^{-1}(x) &amp;= \\ln(1 - x) \\\\ F^{-1}(x) &amp;= - \\frac{ln(1 - x)}{\\lambda}. \\end{align}\\] \\[\\begin{align} P(1 \\leq X \\leq 3) &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= 1 - e^{-1.5 \\times 3} - 1 + e^{-1.5 \\times 1} \\\\ &amp;\\approx 0.212. \\end{align}\\] set.seed(1) nsamps &lt;- 1000 samps &lt;- rexp(nsamps, rate = 1.5) sum(samps &gt;= 1 &amp; samps &lt;= 3) / nsamps ## [1] 0.212 exp_plot &lt;- ggplot(data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5)) + stat_function(fun = dexp, args = list(rate = 1.5), xlim = c(1,3), geom = &quot;area&quot;, fill = &quot;red&quot;) plot(exp_plot) exp_pdf &lt;- function(x, lambda) { return (lambda * exp(-lambda * x)) } exp_cdf &lt;- function(x, lambda) { return (1 - exp(-lambda * x)) } exp_quant &lt;- function(q, lambda) { return (-(log(1 - q) / lambda)) } ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_pdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = pexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_cdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = qexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_quant, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) Exercise 4.8 (Uniform random variable) Continuous uniform random variable with parameters \\(a\\) and \\(b\\) has the PDF \\[\\begin{equation} p(x) = \\begin{cases} \\frac{1}{b - a} &amp; x \\in [a,b] \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation}\\] Derive the CDF of the uniform random variable. Derive the quantile function of the uniform random variable. Let \\(X \\sim \\text{Uniform}(a,b)\\). Derive the CDF of the variable \\(Y = \\frac{X - a}{b - a}\\). This is the standard uniform random variable. Let \\(X \\sim \\text{Uniform}(-1, 3)\\). Find such \\(z\\) that \\(P(X &lt; z + \\mu_x) = \\frac{1}{5}\\). R: Check your result from d) using simulation. Solution. \\[\\begin{align} F(x) &amp;= \\int_{a}^x \\frac{1}{b - a} dt \\\\ &amp;= \\frac{1}{b - a} \\int_{a}^x dt \\\\ &amp;= \\frac{x - a}{b - a}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(p)) &amp;= p \\\\ \\frac{F^{-1}(p) - a}{b - a} &amp;= p \\\\ F^{-1}(p) &amp;= p(b - a) + a. \\end{align}\\] \\[\\begin{align} F_Y(y) &amp;= P(Y &lt; y) \\\\ &amp;= P(\\frac{X - a}{b - a} &lt; y) \\\\ &amp;= P(X &lt; y(b - a) + a) \\\\ &amp;= F_X(y(b - a) + a) \\\\ &amp;= \\frac{(y(b - a) + a) - a}{b - a} \\\\ &amp;= y. \\end{align}\\] \\[\\begin{align} P(X &lt; z + 1) &amp;= \\frac{1}{5} \\\\ F(z + 1) &amp;= \\frac{1}{5} \\\\ z + 1 &amp;= F^{-1}(\\frac{1}{5}) \\\\ z &amp;= \\frac{1}{5}4 - 1 - 1 \\\\ z &amp;= -1.2. \\end{align}\\] set.seed(1) a &lt;- -1 b &lt;- 3 nsamps &lt;- 10000 unif_samp &lt;- runif(nsamps, a, b) mu_x &lt;- mean(unif_samp) new_samp &lt;- unif_samp - mu_x quantile(new_samp, probs = 1/5) ## 20% ## -1.203192 punif(-0.2, -1, 3) ## [1] 0.2 Exercise 4.9 (Beta random variable) A variable \\(X\\) with PDF \\[\\begin{equation} p(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}, \\end{equation}\\] where \\(\\text{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) and \\(\\Gamma(x) = \\int_0^{\\infty} x^{z - 1} e^{-x} dx\\) is a Beta random variable with support on \\([0,1]\\). It has two positive parameters \\(\\alpha\\) and \\(\\beta\\). Notation: \\[\\begin{equation} X | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta) \\end{equation}\\] It is often used in modeling rates. Calculate the PDF for \\(\\alpha = 1\\) and \\(\\beta = 1\\). What do you notice? R: Plot densities of the beta distribution for parameter pairs (2, 2), (4, 1), (1, 4), (2, 5), and (0.1, 0.1). R: Sample from \\(X \\sim \\text{Beta}(2, 5)\\) and compare the histogram with Beta PDF. Solution. \\[\\begin{equation} p(x) = \\frac{x^{1 - 1} (1 - x)^{1 - 1}}{\\text{B}(1, 1)} = 1. \\end{equation}\\] This is the standard uniform distribution. set.seed(1) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), aes(color = &quot;alpha = 0.5&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 4, shape2 = 1), aes(color = &quot;alpha = 4&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 4), aes(color = &quot;alpha = 1&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5), aes(color = &quot;alpha = 25&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 0.1, shape2 = 0.1), aes(color = &quot;alpha = 0.1&quot;)) set.seed(1) nsamps &lt;- 1000 samps &lt;- rbeta(nsamps, 2, 5) ggplot(data = data.frame(x = samps), aes(x = x)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;) + stat_function(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x), fun = dbeta, args = list(shape1 = 2, shape2 = 5), color = &quot;red&quot;, size = 1.2) Exercise 4.10 (Gamma random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\beta x} \\end{equation}\\] is a Gamma random variable with support on the positive numbers and parameters shape \\(\\alpha &gt; 0\\) and rate \\(\\beta &gt; 0\\). We write \\[\\begin{equation} X | \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta) \\end{equation}\\] and it’s CDF is \\[\\begin{equation} \\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}, \\end{equation}\\] where \\(\\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt\\). It is usually used in modeling positive phenomena (for example insurance claims and rainfalls). Let \\(X \\sim \\text{Gamma}(1, \\beta)\\). Find the PDF of \\(X\\). Do you recognize this PDF? Let \\(k = \\alpha\\) and \\(\\theta = \\frac{1}{\\beta}\\). Find the PDF of \\(X | k, \\theta \\sim \\text{Gamma}(k, \\theta)\\). Most (?ALL) random variables can be reparameterized, and sometimes a reparameterized distribution is more suitable for certain calculations. The first parameterization is for example usually used in Bayesian statistics, while this parameterization is more common in econometrics and some other applied fields. Note that you also need to pay attention to the parameters in statistical software, so diligently read the help files when using functions like rgamma to see how the function is parameterized. R: Plot gamma CDF for random variables with shape and rate parameters (1,1), (10,1), (1,10). Solution. \\[\\begin{align} p(x) &amp;= \\frac{\\beta^1}{\\Gamma(1)} x^{1 - 1}e^{-\\beta x} \\\\ &amp;= \\beta e^{-\\beta x} \\end{align}\\] This is the PDF of the exponential distribution with parameter \\(\\beta\\). \\[\\begin{align} p(x) &amp;= \\frac{1}{\\Gamma(k)\\beta^k} x^{k - 1}e^{-\\frac{x}{\\theta}}. \\end{align}\\] set.seed(1) ggplot(data = data.frame(x = seq(0, 25, by = 0.01)), aes(x = x)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 1), aes(color = &quot;Gamma(1,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 10, rate = 1), aes(color = &quot;Gamma(10,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 10), aes(color = &quot;Gamma(1,10)&quot;)) Exercise 4.11 (Normal random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\end{equation}\\] is a normal random variable with support on the real axis and parameters \\(\\mu\\) in reals and \\(\\sigma^2 &gt; 0\\). The first is the mean parameter and the second is the variance parameter. Many statistical methods assume a normal distribution. We denote \\[\\begin{equation} X | \\mu, \\sigma \\sim \\text{N}(\\mu, \\sigma^2), \\end{equation}\\] and it’s CDF is \\[\\begin{equation} F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(t - \\mu)^2}{2 \\sigma^2}} dt, \\end{equation}\\] which is intractable and is usually approximated. Due to it’s flexibility it is also one of the most researched distributions. For that reason statisticians often use transformations of variables or approximate distributions with the normal distribution. Show that a variable \\(\\frac{X - \\mu}{\\sigma} \\sim \\text{N}(0,1)\\). This transformation is called standardization, and \\(\\text{N}(0,1)\\) is a standardized normal distribution. R: Plot the normal distribution with \\(\\mu = 0\\) and different values for the \\(\\sigma\\) parameter. R: The normal distribution provides a good approximation for the Poisson distribution with a large \\(\\lambda\\). Let \\(X \\sim \\text{Poisson}(50)\\). Approximate \\(X\\) with the normal distribution and compare it’s density with the Poisson histogram. What are the values of \\(\\mu\\) and \\(\\sigma^2\\) that should provide the best approximation? Note that R function rnorm takes standard deviation (\\(\\sigma\\)) as a parameter and not variance. Solution. \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= P(X &lt; \\sigma x + \\mu) \\\\ &amp;= F(\\sigma x + \\mu) \\\\ &amp;= \\int_{-\\infty}^{\\sigma x + \\mu} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(t - \\mu)^2}{2\\sigma^2}} dt \\end{align}\\] Now let \\(s = f(t) = \\frac{t - \\mu}{\\sigma}\\), then \\(ds = \\frac{dt}{\\sigma}\\) and \\(f(\\sigma x + \\mu) = x\\), so \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2 \\pi}} e^{\\frac{s^2}{2}} ds. \\end{align}\\] There is no need to evaluate this integral, as we recognize it as the CDF of a normal distribution with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). set.seed(1) # b ggplot(data = data.frame(x = seq(-15, 15, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = &quot;sd = 1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 0.4), aes(color = &quot;sd = 0.1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;sd = 2&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 5), aes(color = &quot;sd = 5&quot;)) # c mean_par &lt;- 50 nsamps &lt;- 100000 pois_samps &lt;- rpois(nsamps, lambda = mean_par) norm_samps &lt;- rnorm(nsamps, mean = mean_par, sd = sqrt(mean_par)) my_plot &lt;- ggplot() + geom_bar(data = tibble(x = pois_samps), aes(x = x, y = (..count..)/sum(..count..))) + geom_density(data = tibble(x = norm_samps), aes(x = x), color = &quot;red&quot;) plot(my_plot) Exercise 4.12 (Logistic random variable) A logistic random variable has CDF \\[\\begin{equation} F(x) = \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}}, \\end{equation}\\] where \\(\\mu\\) is real and \\(s &gt; 0\\). The support is on the real axis. We denote \\[\\begin{equation} X | \\mu, s \\sim \\text{Logistic}(\\mu, s). \\end{equation}\\] The distribution of the logistic random variable resembles a normal random variable, however it has heavier tails. Find the PDF of a logistic random variable. R: Implement logistic PDF and CDF and visually compare both for \\(X \\sim \\text{N}(0, 1)\\) and \\(Y \\sim \\text{logit}(0, \\sqrt{\\frac{3}{\\pi^2}})\\). These distributions have the same mean and variance. Additionally, plot the same plot on the interval [5,10], to better see the difference in the tails. R: For the distributions in b) find the probability \\(P(|X| &gt; 4)\\) and interpret the result. Solution. \\[\\begin{align} p(x) &amp;= \\frac{d}{dx} \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}} \\\\ &amp;= \\frac{- \\frac{d}{dx} (1 + e^{-\\frac{x - \\mu}{s}})}{(1 + e{-\\frac{x - \\mu}{s}})^2} \\\\ &amp;= \\frac{e^{-\\frac{x - \\mu}{s}}}{(1 + e{-\\frac{x - \\mu}{s}})^2}. \\end{align}\\] # b set.seed(1) logit_pdf &lt;- function (x, mu, s) { return ((exp(-(x - mu)/(s))) / (s * (1 + exp(-(x - mu)/(s)))^2)) } nl_plot &lt;- ggplot(data = data.frame(x = seq(-12, 12, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(12/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) nl_plot &lt;- ggplot(data = data.frame(x = seq(5, 10, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(12/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) # c logit_cdf &lt;- function (x, mu, s) { return (1 / (1 + exp(-(x - mu) / s))) } p_logistic &lt;- 1 - logit_cdf(4, 0, sqrt(12/pi^2)) + logit_cdf(-4, 0, sqrt(12/pi^2)) p_norm &lt;- 1 - pnorm(4, 0, 2) + pnorm(-4, 0, 2) p_logistic ## [1] 0.05178347 p_norm ## [1] 0.04550026 # Logistic distribution has wider tails, therefore the probability of larger # absolute values is higher. 4.4 Transformations Exercise 4.13 Let \\(X\\) be a random variable that is uniformly distributed on \\(\\{-2, -1, 0, 1, 2\\}\\). Find the PMF of \\(Y = X^2\\). Solution. \\[\\begin{align} P_Y(y) = \\sum_{x \\in \\sqrt(y)} P_X(x) = \\begin{cases} 0 &amp; y \\notin \\{0,1,4\\} \\\\ \\frac{1}{5} &amp; y = 0 \\\\ \\frac{2}{5} &amp; y \\in \\{1,4\\} \\end{cases} \\end{align}\\] Exercise 4.14 (Lognormal random variable) A lognormal random variable is a variable whose logarithm is normally distributed. In practice, we often encounter skewed data. Usually using a log transformation on such data makes it more symmetric and therefore more suitable for modeling with the normal distribution (more on why we wish to model data with the normal distribution in the following chapters). Let \\(X \\sim \\text{N}(\\mu,\\sigma)\\). Find the PDF of \\(Y: \\log(Y) = X\\). R: Sample from the lognormal distribution with parameters \\(\\mu = 5\\) and \\(\\sigma = 2\\). Plot a histogram of the samples. Then log-transform the samples and plot a histogram along with the theoretical normal PDF. Solution. \\[\\begin{align} p_Y(y) &amp;= p_X(\\log(y)) \\frac{d}{dy} \\log(y) \\\\ &amp;= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(\\log(y) - \\mu)^2}{2 \\sigma^2}} \\frac{1}{y} \\\\ &amp;= \\frac{1}{y \\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(\\log(y) - \\mu)^2}{2 \\sigma^2}}. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 mu &lt;- 0.5 sigma &lt;- 0.4 ln_samps &lt;- rlnorm(nsamps, mu, sigma) ln_plot &lt;- ggplot(data = data.frame(x = ln_samps), aes(x = x)) + geom_histogram(color = &quot;black&quot;) plot(ln_plot) norm_samps &lt;- log(ln_samps) n_plot &lt;- ggplot(data = data.frame(x = norm_samps), aes(x = x)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;) + stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;red&quot;) plot(n_plot) Exercise 4.15 (Probability integral transform) This exercise is borrowed from Wasserman. Let \\(X\\) have a continuous, strictly increasing CDF \\(F\\). Let \\(Y = F(X)\\). Find the density of \\(Y\\). This is called the probability integral transform. Let \\(U \\sim \\text{Uniform}(0,1)\\) and let \\(X = F^{-1}(U)\\). Show that \\(X \\sim F\\). R: Implement a program that takes Uniform(0,1) random variables and generates random variables from an exponential(\\(\\beta\\)) distribution. Compare your implemented function with function rexp in R. Solution. \\[\\begin{align} F_Y(y) &amp;= P(Y &lt; y) \\\\ &amp;= P(F(X) &lt; y) \\\\ &amp;= P(X &lt; F_X^{-1}(y)) \\\\ &amp;= F_X(F_X^{-1}(y)) \\\\ &amp;= y. \\end{align}\\] From the above it follows that \\(p(y) = 1\\). Note that we need to know the inverse CDF to be able to apply this procedure. \\[\\begin{align} P(X &lt; x) &amp;= P(F^{-1}(U) &lt; x) \\\\ &amp;= P(U &lt; F(x)) \\\\ &amp;= F_U(F(x)) \\\\ &amp;= F(x). \\end{align}\\] set.seed(1) nsamps &lt;- 10000 beta &lt;- 4 generate_exp &lt;- function (n, beta) { tmp &lt;- runif(n) X &lt;- qexp(tmp, beta) return (X) } df &lt;- tibble(&quot;R&quot; = rexp(nsamps, beta), &quot;myGenerator&quot; = generate_exp(nsamps, beta)) %&gt;% gather() ggplot(data = df, aes(x = value, fill = key)) + geom_histogram(position = &quot;dodge&quot;) "],
["mrvs.html", "Chapter 5 Multiple random variables 5.1 General 5.2 Bivariate distribution examples 5.3 Transformations", " Chapter 5 Multiple random variables This chapter deals with multiple random variables and their distributions. The students are expected to acquire the following knowledge: Theoretical Connection between the binomial and multinomial distribution. Calculation of PDF of transformed multiple random variables. Finding marginal and conditional distributions. R Scatterplots of bivariate random variables. Sampling from the multinomial distribution. Sampling from transformed bivariate random variables. New R functions (for example, expand.grid). New ggplot parameters (for example, alpha for transparency). Transparency in ggplot. 5.1 General Exercise 1.1 Let \\(X \\sim \\text{N}(0,1)\\) and \\(Y \\sim \\text{N}(0,1)\\) be independent random variables. Draw 1000 samples from \\((X,Y)\\) and plot a scatterplot. Now let \\(X \\sim \\text{N}(0,1)\\) and $Y | X = x N(ax, 1). Draw 1000 samples from \\((X,Y)\\) for \\(a = 1\\), \\(a=0\\), and \\(a=-0.5\\). Plot the scatterplots. How would you interpret parameter \\(a\\)? Plot the marginal distribution of \\(Y\\) for cases \\(a=1\\), \\(a=0\\), and \\(a=-0.5\\). Can you guess which distribution it is? set.seed(1) nsamps &lt;- 1000 x &lt;- rnorm(nsamps) y &lt;- rnorm(nsamps) ggplot(data.frame(x, y), aes(x = x, y = y)) + geom_point() y1 &lt;- rnorm(nsamps, mean = 1 * x) y2 &lt;- rnorm(nsamps, mean = 0 * x) y3 &lt;- rnorm(nsamps, mean = -0.5 * x) df &lt;- tibble(x = c(x,x,x), y = c(y1,y2,y3), a = c(rep(1, nsamps), rep(0, nsamps), rep(-0.5, nsamps))) ggplot(df, aes(x = x, y = y)) + geom_point() + facet_wrap(~a) + coord_equal(ratio=1) # Parameter a controls the scale of linear dependency between X and Y. ggplot(df, aes(x = y)) + geom_density() + facet_wrap(~a) 5.2 Bivariate distribution examples Exercise 1.2 (Discrete bivariate random variable) Let \\(X\\) represent the event that a die rolls an even number and let \\(Y\\) represent the event that a die rolls one, two, or a three. Find the marginal distributions of \\(X\\) and \\(Y\\). Find the PMF of \\((X,Y)\\). Find the CDF of \\((X,Y)\\). Find \\(P(X = 1 | Y = 1)\\). Solution. \\[\\begin{align} P(X = 1) = \\frac{1}{2} \\text{ and } P(X = 0) = \\frac{1}{2} \\\\ P(Y = 1) = \\frac{1}{2} \\text{ and } P(Y = 0) = \\frac{1}{2} \\\\ \\end{align}\\] \\[\\begin{align} P(X = 1, Y = 1) = \\frac{1}{6} \\\\ P(X = 1, Y = 0) = \\frac{2}{6} \\\\ P(X = 0, Y = 1) = \\frac{2}{6} \\\\ P(X = 0, Y = 0) = \\frac{1}{6} \\end{align}\\] \\[\\begin{align} P(X \\leq x, Y \\leq y) = \\begin{cases} \\frac{1}{6} &amp; x = 0, y = 0 \\\\ \\frac{3}{6} &amp; x \\neq y \\\\ 1 &amp; x = 1, y = 1 \\end{cases} \\end{align}\\] \\[\\begin{align} P(X = 1 | Y = 1) = \\frac{1}{3} \\end{align}\\] Exercise 1.3 (Continuous bivariate random variable) Let \\(p(x,y) = 6 (x - y)^2\\) be the PDF of a bivariate random variable \\((X,Y)\\), where both variables range from zero to one. Find CDF. Find marginal distributions. Find conditional distributions. R: Plot a grid of points and colour them by value – this can help us visualize the PDF. R: Plot the marginal distribution of \\(Y\\) and the conditional distributions of \\(X | Y = y\\), where \\(y \\in \\{0, 0.1, 0.5\\}\\). Solution. \\[\\begin{align} F(x,y) &amp;= \\int_0^{x} \\int_0^{y} 6 (t - s)^2 ds dt\\\\ &amp;= 6 \\int_0^{x} \\int_0^{y} t^2 - 2ts + s^2 ds dt\\\\ &amp;= 6 \\int_0^{x} t^2y - ty^2 + \\frac{y^3}{3} dt \\\\ &amp;= 6 (\\frac{x^3 y}{3} - \\frac{x^2y^2}{2} + \\frac{x y^3}{3}) \\\\ &amp;= 2 x^3 y - 3 t^2y^2 + 2 x y^3 \\end{align}\\] \\[\\begin{align} p(x) &amp;= \\int_0^{1} 6 (x - y)^2 dy\\\\ &amp;= 6 (x^2 - x + \\frac{1}{3}) \\\\ &amp;= 6x^2 - 6x + 2 \\end{align}\\] \\[\\begin{align} p(x) &amp;= \\int_0^{1} 6 (x - y)^2 dx\\\\ &amp;= 6 (y^2 - y + \\frac{1}{3}) \\\\ &amp;= 6y^2 - 6y + 2 \\end{align}\\] \\[\\begin{align} p(x|y) &amp;= \\frac{p(xy)}{p(y)} \\\\ &amp;= \\frac{6 (x - y)^2}{6 (y^2 - y + \\frac{1}{3})} \\\\ &amp;= \\frac{(x - y)^2}{y^2 - y + \\frac{1}{3}} \\end{align}\\] \\[\\begin{align} p(y|x) &amp;= \\frac{p(xy)}{p(x)} \\\\ &amp;= \\frac{6 (x - y)^2}{6 (x^2 - x + \\frac{1}{3})} \\\\ &amp;= \\frac{(x - y)^2}{x^2 - x + \\frac{1}{3}} \\end{align}\\] set.seed(1) # a get_pdf &lt;- function (x, y) { return ((x - y)^2) } x_axis &lt;- seq(0, 1, length.out = 100) y_axis &lt;- seq(0, 1, length.out = 100) df &lt;- expand.grid(x_axis, y_axis) colnames(df) &lt;- c(&quot;x&quot;, &quot;y&quot;) df &lt;- cbind(df, pdf = get_pdf(df$x, df$y)) ggplot(data = df, aes(x = x, y = y, color = pdf)) + geom_point() # b mar_pdf &lt;- function (x) { return (6 * x^2 - 6 * x + 2) } cond_pdf &lt;- function (x, y) { return (((x - y)^2) / (y^2 - y + 1/3)) } df &lt;- tibble(x = x_axis, mar = mar_pdf(x), y0 = cond_pdf(x, 0), y0.1 = cond_pdf(x, 0.1), y0.5 = cond_pdf(x, 0.5)) %&gt;% gather(dist, value, -x) ggplot(df, aes(x = x, y = value, color = dist)) + geom_line() Exercise 3.1 (Mixed bivariate random variable) Let \\(f(x,y) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)y!} x^{y+ \\alpha -1} e^{-x(1 + \\beta)}\\) be the PDF of a bivariate random variable, where \\(x \\in (0, \\infty)\\) and \\(y \\in \\mathbb{N}_0\\). Find the marginal distribution of \\(X\\). Do you recognize this distribution? Find the conditional distribution of \\(Y | X\\). Do you recognize this distribution? Calculate the probability \\(P(Y = 2 | X &lt; 5)\\) for \\((X,Y)\\). Find the marginal distribution of \\(Y\\). Do you recognize this distribution? R: Take 1000 random samples from \\((X,Y)\\) with parameters \\(\\beta = 1\\) and \\(\\alpha = 1\\). Plot a scatterplot. Plot a bar plot of the marginal distribution of \\(Y\\), and the theoretical PMF calculated from d) on the range from 0 to 10. Hint: Use the gamma function in R.? Solution. \\[\\begin{align} p(x) &amp;= \\sum_{k = 0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)k!} x^{k + \\alpha -1} e^{-x(1 + \\beta)} &amp; \\\\ &amp;= \\sum_{k = 0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)k!} x^{k} x^{\\alpha -1} e^{-x} e^{-\\beta x} &amp; \\\\ &amp;= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha -1} e^{-\\beta x} \\sum_{k = 0}^{\\infty} \\frac{1}{k!} x^{k} e^{-x} &amp; \\\\ &amp;= \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha -1} e^{-\\beta x} &amp; \\text{the last term above sums to one -- Poisson PMF} \\end{align}\\] This is the Gamma PDF. \\[\\begin{align} p(y|x) &amp;= \\frac{p(x,y)}{p(x)} \\\\ &amp;= \\frac{f(x,y) = \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)y!} x^{y+ \\alpha -1} e^{-x(1 + \\beta)}}{\\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{\\alpha -1} e^{-\\beta x}} \\\\ &amp;= \\frac{x^y e^{-x}}{y!}. \\end{align}\\] This is the Poisson PMF. \\[\\begin{align} P(Y = 2 | X = 2.5) = \\frac{2.5^2 e^{-2.5}}{2!} \\approx 0.26. \\end{align}\\] \\[\\begin{align} p(y) &amp;= \\int_{0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)y!} x^{y + \\alpha -1} e^{-x(1 + \\beta)} dx &amp; \\\\ &amp;= \\frac{1}{y!} \\int_{0}^{\\infty} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} x^{(y + \\alpha) -1} e^{-(1 + \\beta)x} dx &amp; \\\\ &amp;= \\frac{1}{y!} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\int_{0}^{\\infty} \\frac{\\Gamma(y + \\alpha)}{(1 + \\beta)^{y + \\alpha}} \\frac{(1 + \\beta)^{y + \\alpha}}{\\Gamma(y + \\alpha)} x^{(y + \\alpha) -1} e^{-(1 + \\beta)x} dx &amp; \\text{we add the term so that we have Gamma PDF inside the integral} \\\\ &amp;= \\frac{1}{y!} \\frac{\\beta^{\\alpha}}{\\Gamma(\\alpha)} \\frac{\\Gamma(y + \\alpha)}{(1 + \\beta)^{y + \\alpha}}. \\end{align}\\] We add the terms in the third equality to get a Gamma PDF inside the integral, which then integrates to one. We do not recognize this distribution. set.seed(1) px &lt;- function (x, alpha, beta) { return((1 / factorial(x)) * (beta^alpha / gamma(alpha)) * (gamma(x + alpha) / (1 + beta)^(x + alpha))) } nsamps &lt;- 1000 rx &lt;- rgamma(nsamps, 1, 1) ryx &lt;- rpois(nsamps, rx) ggplot(data = data.frame(x = rx, y = ryx), aes(x = x, y = y)) + geom_point() ggplot(data = data.frame(x = rx, y = ryx), aes(x = y)) + geom_bar(aes(y = (..count..)/sum(..count..))) + stat_function(fun = px, args = list(alpha = 1, beta = 1), color = &quot;red&quot;) Exercise 1.6 Let \\(f(x,y) = cx^2y\\) for \\(x^2 \\leq y \\leq 1\\) and zero otherwise. Find such \\(c\\) that \\(f\\) is a PDF of a bivariate random variable. This exercise is borrowed from Wasserman. Solution. \\[\\begin{align} 1 &amp;= \\int_{-1}^{1} \\int_{x^2}^1 cx^2y dy dx \\\\ &amp;= \\int_{-1}^{1} cx^2 (\\frac{1}{2} - \\frac{x^4}{2}) dx \\\\ &amp;= \\frac{c}{2} \\int_{-1}^{1} x^2 - x^6 dx \\\\ &amp;= \\frac{c}{2} (\\frac{1}{3} + \\frac{1}{3} - \\frac{1}{7} - \\frac{1}{7}) \\\\ &amp;= \\frac{c}{2} \\frac{8}{21} \\\\ &amp;= \\frac{4c}{21} \\end{align}\\] It follows \\(c = \\frac{21}{4}\\). 5.3 Transformations Exercise 1.7 Let \\((X,Y)\\) be uniformly distributed on the unit ball \\(\\{(x,y,z) : x^2 + y^2 + z^2 \\leq 1\\}\\). Let \\(R = \\sqrt{X^2 + Y^2 + Z^2}\\). Find the CDF and PDF of \\(R\\). Solution. \\[\\begin{align} P(R &lt; r) &amp;= P(\\sqrt{X^2 + Y^2 + Z^2} &lt; r) \\\\ &amp;= P(X^2 + Y^2 + Z^2 &lt; r^2) \\\\ &amp;= \\frac{\\frac{4}{3} \\pi r^2}{\\frac{4}{3}\\pi} \\\\ &amp;= r^3. \\end{align}\\] The second line shows us that we are looking at the probability which is represented by a smaller ball with radius \\(r\\). To get the probability, we divide it by the radius of the whole ball. We get the PDF by differentiating the CDF, so \\(p(r) = 3r^2\\). "],
["integ.html", "Chapter 6 Integration 6.1 Monte Carlo integration 6.2 Lebesgue integrals", " Chapter 6 Integration This chapter deals with abstract and Monte Carlo integration. The students are expected to acquire the following knowledge: Theoretical How to calculate Lebesgue integrals for non-simple functions. R Monte Carlo integration. 6.1 Monte Carlo integration Exercise 1.1 Let \\(X \\sim \\text{Gamma}(3,2)\\) and \\(Y = \\log(X)\\). Use Monte Carlo integration to estimate the probability \\(P(-1 \\leq Y \\leq 1)\\). Can you find the exact value?Is this Monte Carlo integration??? set.seed(1) nsamps &lt;- 1000 x &lt;- rgamma(nsamps, 3, 2) y &lt;- log(x) sum(y &gt; -1 &amp; y &lt; 1) / nsamps ## [1] 0.86 6.2 Lebesgue integrals Exercise 1.2 (borrowed from Binder) Find the Lebesgue integral of the following functions on (\\(\\mathbb{R}\\), \\(\\mathcal{B}(\\mathbb{R})\\), \\(\\lambda\\)). \\[\\begin{align} f(\\omega) = \\begin{cases} \\omega, &amp; \\text{for } \\omega = 0,1,...,n \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] \\[\\begin{align} f(\\omega) = \\begin{cases} 1, &amp; \\text{for } \\omega = \\mathbb{Q}^c \\cap [0,1] \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] \\[\\begin{align} f(\\omega) = \\begin{cases} n, &amp; \\text{for } \\omega = \\mathbb{Q}^c \\cap [0,n] \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] Solution. \\[\\begin{align} \\int f(\\omega) d\\lambda = \\sum_{\\omega = 0}^n \\omega \\lambda(\\omega) = 0. \\end{align}\\] \\[\\begin{align} \\int f(\\omega) d\\lambda = 1 \\times \\lambda(\\mathbb{Q}^c \\cap [0,1]) = 1. \\end{align}\\] \\[\\begin{align} \\int f(\\omega) d\\lambda = n \\times \\lambda(\\mathbb{Q}^c \\cap [0,n]) = n^2. \\end{align}\\] Exercise 1.3 (borrowed from Binder) Let \\(c \\in \\mathbb{R}\\) be fixed and (\\(\\mathbb{R}\\), \\(\\mathcal{B}(\\mathbb{R})\\)) a measurable space. If for any Borel set \\(A\\), \\(\\delta_c (A) = 1\\) if \\(c \\in A\\), and \\(\\delta_c (A) = 0\\) otherwise, then \\(\\delta_c\\) is called a Dirac measure. Let \\(g\\) be a non-negative, measurable function. Show that \\(\\int g d \\delta_c = g(c)\\). Solution. \\[\\begin{align} \\int g d \\delta_c &amp;= \\sup_{q \\in S(g)} \\int q d \\delta_c \\\\ &amp;= \\sup_{q \\in S(g)} \\sum_{i = 1}^n a_i \\delta_c(A_i) \\\\ &amp;= \\sup_{q \\in S(g)} \\sum_{i = 1}^n a_i \\text{I}_{A_i}(c) \\\\ &amp;= \\sup_{q \\in S(g)} q(c) \\\\ &amp;= g(c) \\end{align}\\] TODO: Check with Erik. "],
["ev.html", "Chapter 7 Expected value 7.1 Discrete random variables 7.2 Continuous random variables 7.3 Sums, functions, conditional expectations 7.4 Covariance", " Chapter 7 Expected value This chapter deals with expected values of random variables. The students are expected to acquire the following knowledge: Theoretical Calculation of the expected value. Calculation of variance and covariance. Cauchy distribution. Add expected value of a variable with non-Riemann integrable? ERIK? R Estimation of expected value. Estimation of variance and covariance. 7.1 Discrete random variables Exercise 7.1 (Bernoulli) Let \\(X \\sim \\text{Bernoulli}(p)\\). Find \\(E[X]\\). Find \\(Var[X]\\). R: Let \\(p = 0.4\\). Check your answers to a) and b) with a simulation. Solution. \\[\\begin{align*} E[X] = \\sum_{k=0}^1 p^k (1-p)^{1-k} k = p. \\end{align*}\\] \\[\\begin{align*} Var[X] = E[X^2] - E[X]^2 = \\sum_{k=0}^1 (p^k (1-p)^{1-k} k^2) - p^2 = p(1-p). \\end{align*}\\] set.seed(1) nsamps &lt;- 1000 x &lt;- rbinom(nsamps, 1, 0.4) mean(x) ## [1] 0.394 var(x) ## [1] 0.239003 0.4 * (1 - 0.4) ## [1] 0.24 Exercise 7.2 (Binomial) Let \\(X \\sim \\text{Binomial}(n,p)\\). Find \\(E[X]\\). Find \\(Var[X]\\). Solution. Let \\(X = \\sum_{i=0}^n X_i\\), where \\(X_i \\sim \\text{Bernoulli}(p)\\). Then, due to linearity of expectation \\[\\begin{align*} E[X] = E[\\sum_{i=0}^n X_i] = \\sum_{i=0}^n E[X_i] = np. \\end{align*}\\] Again let \\(X = \\sum_{i=0}^n X_i\\), where \\(X_i \\sim \\text{Bernoulli}(p)\\). Since the Bernoulli variables \\(X_i\\) are independent we have \\[\\begin{align*} Var[X] = Var[\\sum_{i=0}^n X_i] = \\sum_{i=0}^n Var[X_i] = np(1-p). \\end{align*}\\] Exercise 7.3 (Poisson) Let \\(X \\sim \\text{Poisson}(\\lambda)\\). Find \\(E[X]\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\sum_{k=0}^\\infty \\frac{\\lambda^k e^{-\\lambda}}{k!} k &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!} &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!} &amp; \\text{term at $k=0$ is 0} \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=0}^\\infty \\frac{\\lambda^{k}}{k!} &amp; \\\\ &amp;= e^{-\\lambda} \\lambda e^\\lambda &amp; \\\\ &amp;= \\lambda. \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty k \\frac{\\lambda^{k-1}}{(k - 1)!} - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\sum_{k=1}^\\infty (k - 1) + 1) \\frac{\\lambda^{k-1}}{(k - 1)!} - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\sum_{k=1}^\\infty (k - 1) \\frac{\\lambda^{k-1}}{(k - 1)!} + \\sum_{k=1}^\\infty \\frac{\\lambda^{k-1}}{(k - 1)!}\\Big) - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\lambda\\sum_{k=2}^\\infty \\frac{\\lambda^{k-2}}{(k - 2)!} + e^\\lambda\\Big) - \\lambda^2 &amp; \\\\ &amp;= e^{-\\lambda} \\lambda \\big(\\lambda e^\\lambda + e^\\lambda\\Big) - \\lambda^2 &amp; \\\\ &amp;= \\lambda^2 + \\lambda - \\lambda^2 &amp; \\\\ &amp;= \\lambda. \\end{align*}\\] Exercise 7.4 (Geometric) Let \\(X \\sim \\text{Geometric}(p)\\). Find \\(E[X]\\). Hint: \\(\\frac{d}{dx} x^k = k x^(k - 1)\\). Solution. \\[\\begin{align*} E[X] &amp;= \\sum_{k=0}^\\infty (1 - p)^k p k &amp; \\\\ &amp;= p (1 - p) \\sum_{k=0}^\\infty (1 - p)^{k-1} k &amp; \\\\ &amp;= p (1 - p) \\sum_{k=0}^\\infty -\\frac{d}{dp}(1 - p)^k &amp; \\\\ &amp;= p (1 - p) \\Big(-\\frac{d}{dp}\\Big) \\sum_{k=0}^\\infty (1 - p)^k &amp; \\\\ &amp;= p (1 - p) \\Big(-\\frac{d}{dp}\\Big) \\frac{1}{1 - (1 - p)} &amp; \\text{geometric series} \\\\ &amp;= \\frac{1 - p}{p} \\end{align*}\\] 7.2 Continuous random variables Exercise 7.5 (Gamma) Let \\(X \\sim \\text{Gamma}(\\alpha, \\beta)\\). Hint: \\(\\Gamma(z) = \\int_0^\\infty t^{z-1}e^{-t} dt\\) and \\(\\Gamma(z + 1) = z \\Gamma(z)\\). Find \\(E[X]\\). Find \\(Var[X]\\). R: Let \\(\\alpha = 10\\) and \\(\\beta = 2\\). Plot the density of \\(X\\). Add a horizontal line at the expected value that touches the density curve (geom_segment). Shade the area within a standard deviation of the expected value. Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^\\alpha e^{-\\beta x} dx &amp; \\\\ &amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) \\int_0^\\infty }x^\\alpha e^{-\\beta x} dx &amp; \\text{ (let $t = \\beta x$)} \\\\ &amp;= \\frac{\\beta^\\alpha}{\\Gamma(\\alpha) \\int_0^\\infty }\\frac{t^\\alpha}{\\beta^\\alpha} e^{-t} \\frac{dt}{\\beta} &amp; \\\\ &amp;= \\frac{1}{\\beta \\Gamma(\\alpha) \\int_0^\\infty }t^\\alpha e^{-t} dt &amp; \\\\ &amp;= \\frac{\\Gamma(\\alpha + 1)}{\\beta \\Gamma(\\alpha)} &amp; \\\\ &amp;= \\frac{\\alpha \\Gamma(\\alpha)}{\\beta \\Gamma(\\alpha)} &amp; \\\\ &amp;= \\frac{\\alpha}{\\beta}. &amp; \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\int_0^\\infty \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}x^{\\alpha+1} e^{-\\beta x} dx - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\Gamma(\\alpha + 2)}{\\beta^2 \\Gamma(\\alpha)} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{(\\alpha + 1)\\alpha\\Gamma(\\alpha)}{\\beta^2 \\Gamma(\\alpha)} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\alpha^2 + \\alpha}{\\beta^2} - \\frac{\\alpha^2}{\\beta^2} \\\\ &amp;= \\frac{\\alpha}{\\beta^2}. \\end{align*}\\] set.seed(1) x &lt;- seq(0, 25, by = 0.01) y &lt;- dgamma(x, shape = 10, rate = 2) df &lt;- data.frame(x = x, y = y) ggplot(df, aes(x = x, y = y)) + geom_line() + geom_segment(aes(x = 5, y = 0, xend = 5, yend = dgamma(5, shape = 10, rate = 2)), color = &quot;red&quot;) + stat_function(fun = dgamma, args = list(shape = 10, rate = 2), xlim = c(5 - sqrt(10/4), 5 + sqrt(10/4)), geom = &quot;area&quot;, fill = &quot;gray&quot;, alpha = 0.4) Exercise 7.6 (Beta) Let \\(X \\sim \\text{Beta}(\\alpha, \\beta)\\). Find \\(E[X]\\). Hint 1: \\(\\text{B}(x,y) = \\int_0^1 t^{x-1} (1 - t)^{y-1} dt\\). Hint 2: \\(\\text{B}(x + 1, y) = \\text{B}(x,y)\\frac{x}{x + y}\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^1 \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)} x dx \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)}\\int_0^1 x^{\\alpha} (1 - x)^{\\beta - 1} dx \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 1, \\beta) \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha, \\beta) \\frac{\\alpha}{\\alpha + \\beta} \\\\ &amp;= \\frac{\\alpha}{\\alpha + \\beta}. \\\\ \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 \\\\ &amp;= \\int_0^1 \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)} x^2 dx - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)}\\int_0^1 x^{\\alpha + 1} (1 - x)^{\\beta - 1} dx - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 2, \\beta) - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{1}{\\text{B}(\\alpha, \\beta)} \\text{B}(\\alpha + 1, \\beta) \\frac{\\alpha + 1}{\\alpha + \\beta + 1} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2} \\\\ &amp;= \\frac{\\alpha + 1}{\\alpha + \\beta + 1} \\frac{\\alpha}{\\alpha + \\beta} - \\frac{\\alpha^2}{(\\alpha + \\beta)^2}\\\\ &amp;= \\frac{\\alpha \\beta}{(\\alpha + \\beta)^2(\\alpha + \\beta + 1)}. \\end{align*}\\] Exercise 7.7 (Exponential) Let \\(X \\sim \\text{Exp}(\\lambda)\\). Find \\(E[X]\\). Hint: \\(\\Gamma(z + 1) = z\\Gamma(z)\\) and \\(\\Gamma(1) = 1\\). Find \\(Var[X]\\). Solution. \\[\\begin{align*} E[X] &amp;= \\int_0^\\infty \\lambda e^{-\\lambda x} x dx &amp; \\\\ &amp;= \\lambda \\int_0^\\infty x e^{-\\lambda x} dx &amp; \\\\ &amp;= \\lambda \\int_0^\\infty \\frac{t}{\\lambda} e^{-t} \\frac{dt}{\\lambda} &amp; \\text{$t = \\lambda x$}\\\\ &amp;= \\lambda \\lambda^{-2} \\Gamma(2) &amp; \\text{definition of gamma function} \\\\ &amp;= \\lambda^{-1}. \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[X^2] - E[X]^2 &amp; \\\\ &amp;= \\int_0^\\infty \\lambda e^{-\\lambda x} x^2 dx - \\lambda^{-2} &amp; \\\\ &amp;= \\lambda \\int_0^\\infty \\frac{t^2}{\\lambda^2} e^{-t} \\frac{dt}{\\lambda} - \\lambda^{-2} &amp; \\text{$t = \\lambda x$} \\\\ &amp;= \\lambda \\lambda^{-3} \\Gamma(3) - \\lambda^{-2} &amp; \\text{definition of gamma function} &amp; \\\\ &amp;= \\lambda^{-2} 2 \\Gamma(2) - \\lambda^{-2} &amp; \\\\ &amp;= 2 \\lambda^{-2} - \\lambda^{-2} &amp; \\\\ &amp;= \\lambda^{-2}. &amp; \\\\ \\end{align*}\\] Exercise 7.8 (Normal) Let \\(X \\sim \\text{N}(\\mu, \\sigma)\\). Show that \\(E[X] = \\mu\\). Hint: Use the error function \\(\\text{erf}(x) = \\frac{1}{\\sqrt(\\pi)} \\int_{-x}^x e^{-t^2} dt\\). The statistical interpretation of this function is that if \\(Y \\sim \\text{N}(0, 0.5)\\), then the error function describes the probability of \\(Y\\) falling between \\(-x\\) and \\(x\\). Show that \\(Var[X] = \\sigma^2\\). Hint: Start with the definition of variance. Solution. \\[\\begin{align*} E[X] &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} x dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty x e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty \\Big(t \\sqrt{2\\sigma^2} + \\mu\\Big)e^{-t^2} \\sqrt{2 \\sigma^2} dt &amp; t = \\frac{x - \\mu}{\\sqrt{2\\sigma}} \\\\ &amp;= \\frac{\\sqrt{2\\sigma^2}}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty t e^{-t^2} dt + \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty \\mu e^{-t^2} dt &amp; \\\\ \\end{align*}\\] Let us calculate these integrals separately. \\[\\begin{align*} \\int t e^{-t^2} dt &amp;= -\\frac{1}{2}\\int e^{s} ds &amp; s = -t^2 \\\\ &amp;= -\\frac{e^s}{2} + C \\\\ &amp;= -\\frac{e^{-t^2}}{2} + C &amp; \\text{undoing substitution}. \\end{align*}\\] Inserting the integration limits we get \\[\\begin{align*} \\int_{-\\infty}^\\infty t e^{-t^2} dt &amp;= 0, \\end{align*}\\] due to the integrated function being symmetric. Reordering the second integral we get \\[\\begin{align*} \\mu \\frac{1}{\\sqrt{\\pi}} \\int_{-\\infty}^\\infty e^{-t^2} dt &amp;= \\mu \\text{erf}(\\infty) &amp; \\text{definition of error function} \\\\ &amp;= \\mu &amp; \\text{probability of $Y$ falling between $-\\infty$ and $\\infty$}. \\end{align*}\\] Combining all of the above we get \\[\\begin{align*} E[X] &amp;= \\frac{\\sqrt{2\\sigma^2}}{\\sqrt{\\pi}} \\times 0 + \\mu &amp;= \\mu.\\\\ \\end{align*}\\] \\[\\begin{align*} Var[X] &amp;= E[(X - E[X])^2] \\\\ &amp;= E[(X - \\mu)^2] \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty (x - \\mu)^2 e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} dx \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\int_{-\\infty}^\\infty t^2 e^{-\\frac{t^2}{2}} dt \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\bigg(\\Big(- t e^{-\\frac{t^2}{2}} |_{-\\infty}^\\infty \\Big) + \\int_{-\\infty}^\\infty e^{-\\frac{t^2}{2}} \\bigg) dt &amp; \\text{integration by parts} \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\sqrt{2 \\pi} \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt(\\pi)}e^{-s^2} \\bigg) &amp; s = \\frac{t}{\\sqrt{2}} \\text{ and evaluating the left expression at the bounds} \\\\ &amp;= \\frac{\\sigma^2}{\\sqrt{2\\pi}} \\sqrt{2 \\pi} \\Big(\\text{erf}(\\infty) &amp; \\text{definition of error function} \\\\ &amp;= \\sigma^2. \\end{align*}\\] 7.3 Sums, functions, conditional expectations Exercise 1.6 (Expectation of transformations) Let \\(X\\) follow a normal distribution. Find \\(E[2X + 4]\\). Find \\(E[X^2]\\). Find \\(E[\\exp(X)]\\). R: Check your results numerically for \\(\\mu = 0.4\\) and \\(\\sigma^2 = 0.25\\) and plot the densities of all four distributions. Solution. \\[\\begin{align} E[2X + 4] &amp;= 2E[X] + 4 &amp; \\text{linearity of expectation} \\\\ &amp;= 2\\mu + 4. \\\\ \\end{align}\\] \\[\\begin{align} E[X^2] &amp;= E[X]^2 - Var[X] &amp; \\text{definition of variance} \\\\ &amp;= \\mu^2 + \\sigma^2. \\end{align}\\] \\[\\begin{align} E[\\exp(X)] &amp;= \\int_{-\\infty}^\\infty \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2\\sigma^2}} e^x dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty e^{\\frac{2 \\sigma^2 x}{2\\sigma^2} -\\frac{(x - \\mu)^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty e^{-\\frac{x^2 - 2x(\\mu + \\sigma^2) + \\mu^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} \\int_{-\\infty}^\\infty e^{-\\frac{(x - (\\mu + \\sigma^2))^2 + \\mu^2 - (\\mu + \\sigma^2)^2}{2\\sigma^2}} dx &amp; \\text{complete the square} \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{- \\mu^2 + (\\mu + \\sigma^2)^2}{2\\sigma^2}} \\int_{-\\infty}^\\infty e^{-\\frac{(x - (\\mu + \\sigma^2))^2}{2\\sigma^2}} dx &amp; \\\\ &amp;= \\frac{1}{\\sqrt{2\\pi \\sigma^2}} e^{\\frac{- \\mu^2 + (\\mu + \\sigma^2)^2}{2\\sigma^2}} \\sigma \\sqrt{2 \\pi} \\text{erf}(\\infty) &amp; \\text{using the same substitutions as in \\@ref(exr:normev)} \\\\ &amp;= e^{\\frac{2\\mu + \\sigma^2}{2}}. \\end{align}\\] set.seed(1) mu &lt;- 0.4 sigma &lt;- 0.5 x &lt;- rnorm(100000, mean = mu, sd = sigma) mean(2*x + 4) ## [1] 4.797756 2 * mu + 4 ## [1] 4.8 mean(x^2) ## [1] 0.4108658 mu^2 + sigma^2 ## [1] 0.41 mean(exp(x)) ## [1] 1.689794 exp((2 * mu + sigma^2) / 2) ## [1] 1.690459 Exercise 7.9 (Sum of independent random variables) Let \\(X_1, X_2,...,X_n\\) be IID random variables with expected value \\(E[X_i] = \\mu\\) and variance \\(Var[X_i] = \\sigma^2\\). Find the expected value and variance of \\(\\bar{X} = \\frac{1}{n} \\sum_{i=1}^n X_i\\). \\(\\bar{X}\\) is called a statistic (a function of the values in a sample). It is itself a random variable. R: Take \\(n = 5, 10, 100, 1000\\) samples from the N(\\(2\\), \\(6\\)) distribution 10000 times. Plot the theoretical density and the densities of \\(\\bar{X}\\) statistic for each \\(n\\). Intuitively, are the results in correspondence with your calculations? Check them numerically. Solution. Let us start with the expectation of \\(\\bar{X}\\). \\[\\begin{align} E[\\bar{X}] &amp;= E[\\frac{1}{n} \\sum_{i=1}^n X_i] &amp; \\\\ &amp;= \\frac{1}{n} E[\\sum_{i=1}^n X_i] &amp; \\text{ (multiplication with a scalar)} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n E[X_i] &amp; \\text{ (linearity)} \\\\ &amp;= \\frac{1}{n} n \\mu &amp; \\\\ &amp;= \\mu. \\end{align}\\] Now the variance \\[\\begin{align} Var[\\bar{X}] &amp;= Var[\\frac{1}{n} \\sum_{i=1}^n X_i] &amp; \\\\ &amp;= \\frac{1}{n^2} Var[\\sum_{i=1}^n X_i] &amp; \\text{ (multiplication with a scalar)} \\\\ &amp;= \\frac{1}{n} \\sum_{i=1}^n Var[X_i] &amp; \\text{ (independence of samples)} \\\\ &amp;= \\frac{1}{n^2} n \\sigma^2 &amp; \\\\ &amp;= \\frac{1}{n} \\sigma^2. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 mu &lt;- 2 sigma &lt;- sqrt(6) N &lt;- c(5, 10, 100, 500) X &lt;- matrix(data = NA, nrow = nsamps, ncol = length(N)) ind &lt;- 1 for (n in N) { for (i in 1:nsamps) { X[i,ind] &lt;- mean(rnorm(n, mu, sigma)) } ind &lt;- ind + 1 } colnames(X) &lt;- N X &lt;- melt(as.data.frame(X)) ggplot(data = X, aes(x = value, colour = variable)) + geom_density() + stat_function(data = data.frame(x = seq(-2, 6, by = 0.01)), aes(x = x), fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;black&quot;) Exercise 2.2 (Conditional expectation) Let \\(X \\in \\mathbb{R}_0^+\\) and \\(Y \\in \\mathbb{N}_0\\) be random variables with joint distribution \\(p_{XY}(X,Y) = \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}} 0.5^{y + 1}\\). Find \\(E[X | Y]\\) by first finding \\(p_Y\\) and then \\(p_{X|Y}\\). Find \\(E[X]\\). R: check your answers to a) and b) by drawing 10000 samples from \\(p_Y\\) and \\(p_{X|Y}\\). Solution. \\[\\begin{align} p(y) &amp;= \\int_0^\\infty \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}} 0.5^{y + 1} dx \\\\ &amp;= \\frac{0.5^{y + 1}}{y + 1} \\int_0^\\infty e^{-\\frac{x}{y + 1}} dx \\\\ &amp;= \\frac{0.5^{y + 1}}{y + 1} (y + 1) \\\\ &amp;= 0.5^{y + 1} \\\\ &amp;= 0.5(1 - 0.5)^y. \\end{align}\\] We recognize this as the geometric distribution. \\[\\begin{align} p(x|y) &amp;= \\frac{p(X,Y)}{P(Y)} \\\\ &amp;= \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}}. \\end{align}\\] We recognize this as the exponential distribution. \\[\\begin{align} E[X | Y] &amp;= \\int_0^\\infty x \\frac{1}{y + 1} e^{-\\frac{x}{y + 1}} dx \\\\ &amp;= y + 1 &amp; \\text{expected value of the exponential distribution} \\end{align}\\] Use the law of iterated expectation. \\[\\begin{align} E[X] &amp;= E[E[X | Y]] \\\\ &amp;= E[Y + 1] \\\\ &amp;= E[Y] + 1 \\\\ &amp;= \\frac{1 - 0.5}{0.5} + 1 \\\\ &amp;= 2. \\end{align}\\] set.seed(1) y &lt;- rgeom(100000, 0.5) x &lt;- rexp(100000, rate = 1 / (y + 1)) x2 &lt;- x[y == 3] mean(x2) ## [1] 4.048501 3 + 1 ## [1] 4 mean(x) ## [1] 2.007639 (1 - 0.5) / 0.5 + 1 ## [1] 2 Exercise 3.3 (Cauchy distribution) Let \\(p(x | x_0, \\gamma) = \\frac{1}{\\pi \\gamma \\Big(1 + \\big(\\frac{x - x_0}{\\gamma}\\big)^2\\Big)}\\). A random variable with this PDF follows a Cauchy distribution. This distribution is symmetric and has wider tails than the normal distribution. R: Draw \\(n = 1,...,1000\\) samples from a standard normal and \\(\\text{Cauchy}(0, 1)\\). For each \\(n\\) plot the mean and the median of the sample using facets. Interpret the results. To get a mathematical explanation of the results in a), evaluate the integral \\(\\int_0^\\infty \\frac{x}{1 + x^2} dx\\) and consider that \\(E[X] = \\int_{-\\infty}^\\infty \\frac{x}{1 + x^2}dx\\). set.seed(1) n &lt;- 1000 means_n &lt;- vector(mode = &quot;numeric&quot;, length = n) means_c &lt;- vector(mode = &quot;numeric&quot;, length = n) medians_n &lt;- vector(mode = &quot;numeric&quot;, length = n) medians_c &lt;- vector(mode = &quot;numeric&quot;, length = n) for (i in 1:n) { tmp_n &lt;- rnorm(i) tmp_c &lt;- rcauchy(i) means_n[i] &lt;- mean(tmp_n) means_c[i] &lt;- mean(tmp_c) medians_n[i] &lt;- median(tmp_n) medians_c[i] &lt;- median(tmp_c) } df &lt;- data.frame(&quot;distribution&quot; = c(rep(&quot;normal&quot;, 2 * n), rep(&quot;Cauchy&quot;, 2 * n)), &quot;type&quot; = c(rep(&quot;mean&quot;, n), rep(&quot;median&quot;, n), rep(&quot;mean&quot;, n), rep(&quot;median&quot;, n)), &quot;value&quot; = c(means_n, medians_n, means_c, medians_c), &quot;n&quot; = rep(1:n, times = 4)) ggplot(df, aes(x = n, y = value)) + geom_line(alpha = 0.5) + facet_wrap(~ type + distribution , scales = &quot;free&quot;) Solution. a. \\[\\begin{align} \\int_0^\\infty \\frac{x}{1 + x^2} dx &amp;= \\frac{1}{2} \\int_0^\\infty \\frac{1}{u} du &amp; u = 1 + x^2 \\\\ &amp;= \\frac{1}{2} \\ln(x) |_0^\\infty. \\end{align}\\] This integral is not finite. The same holds for the negative part. Therefore, the expectation is undefined. 7.4 Covariance Exercise 7.10 Below is a table of values for random variables \\(X\\) and \\(Y\\). X Y 2.1 8 -0.5 11 1 10 -2 12 4 9 Find sample covariance of \\(X\\) and \\(Y\\). Find sample variances of \\(X\\) and \\(Y\\). Find sample correlation of \\(X\\) and \\(Y\\). Find sample variance of \\(Z = 2X - 3Y\\). Solution. a. \\(\\bar{X} = 0.92\\) and \\(\\bar{Y} = 10\\). \\[\\begin{align} s(X, Y) &amp;= \\frac{1}{n - 1} \\sum_{i=1}^5 (X_i - 0.92) (Y_i - 10) \\\\ &amp;= -3.175. \\end{align}\\] \\[\\begin{align} s(X) &amp;= \\frac{\\sum_{i=1}^5(X_i - 0.92)^2}{5 - 1} \\\\ &amp;= 5.357. \\end{align}\\] \\[\\begin{align} s(Y) &amp;= \\frac{\\sum_{i=1}^5(Y_i - 10)^2}{5 - 1} \\\\ &amp;= 2.5. \\end{align}\\] \\[\\begin{align} r(X,Y) &amp;= \\frac{Cov(X,Y)}{\\sqrt{Var[X]Var[Y]}} \\\\ &amp;= \\frac{-3.175}{\\sqrt{5.357 \\times 2.5}} \\\\ &amp;= -8.68. \\end{align}\\] \\[\\begin{align} s(Z) &amp;= 2^2 s(X) + 3^2 s(Y) + 2 \\times 2 \\times 3 s(X, Y) \\\\ &amp;= 4 \\times 5.357 + 9 \\times 2.5 + 12 \\times 3.175 \\\\ &amp;= 82.028. \\end{align}\\] Exercise 7.11 Let \\(X \\sim \\text{Uniform}(0,1)\\) and \\(Y | X = x \\sim \\text{Uniform(0,x)}\\). Find the covariance of \\(X\\) and \\(Y\\). Find the correlation of \\(X\\) and \\(Y\\). R: check your answers to a) and b) with simulation. Plot \\(X\\) against \\(Y\\) on a scatterplot. Solution. a. The joint PDF is \\(p(x,y) = p(x)p(y|x) = \\frac{1}{x}\\). \\[\\begin{align} Cov(X,Y) &amp;= E[XY] - E[X]E[Y] \\\\ \\end{align}\\] Let us first evaluate the first term: \\[\\begin{align} E[XY] &amp;= \\int_0^1 \\int_0^x x y \\frac{1}{x} dy dx \\\\ &amp;= \\int_0^1 \\int_0^x y dy dx \\\\ &amp;= \\int_0^1 \\frac{x^2}{2} dx \\\\ &amp;= \\frac{1}{6}. \\end{align}\\] Now let us find \\(E[Y]\\), \\(E[X]\\) is trivial. \\[\\begin{align} E[Y] = E[E[Y | X]] = E[\\frac{X}{2}] = \\frac{1}{2} \\int_0^1 x dx = \\frac{1}{4}. \\end{align}\\] Combining all: \\[\\begin{align} Cov(X,Y) &amp;= \\frac{1}{6} - \\frac{1}{2} \\frac{1}{4} = \\frac{1}{24}. \\end{align}\\] \\[\\begin{align} \\rho(X,Y) &amp;= \\frac{Cov(X,Y)}{\\sqrt{Var[X]Var[Y]}} \\\\ \\end{align}\\] Let us calculate \\(Var[X]\\). \\[\\begin{align} Var[X] &amp;= E[X^2] - \\frac{1}{4} \\\\ &amp;= \\int_0^1 x^2 - \\frac{1}{4} \\\\ &amp;= \\frac{1}{3} - \\frac{1}{4} \\\\ &amp;= \\frac{1}{12}. \\end{align}\\] Let us calculate \\(E[E[Y^2|X]]\\). \\[\\begin{align} E[E[Y^2|X]] &amp;= E[\\frac{x^2}{3}] \\\\ &amp;= \\frac{1}{9}. \\end{align}\\] Then \\(Var[Y] = \\frac{1}{9} - \\frac{1}{16} = \\frac{7}{144}\\). Combining all \\[\\begin{align} \\rho(X,Y) &amp;= \\frac{\\frac{1}{24}}{\\sqrt{\\frac{1}{12}\\frac{5}{144}}} \\\\ &amp;= 0.65. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 x &lt;- runif(nsamps) y &lt;- runif(nsamps, 0, x) cov(x, y) ## [1] 0.04274061 1/24 ## [1] 0.04166667 cor(x, y) ## [1] 0.6629567 (1 / 24) / (sqrt(7 / (12 * 144))) ## [1] 0.6546537 ggplot(data.frame(x = x, y = y), aes(x = x, y = y)) + geom_point(alpha = 0.2) + geom_smooth(method = &quot;lm&quot;) "],
["mrv.html", "Chapter 8 Multivariate random variables 8.1 Multinomial random variables 8.2 Multivariate normal random variables 8.3 Transformations", " Chapter 8 Multivariate random variables This chapter deals with multivariate random variables. The students are expected to acquire the following knowledge: Theoretical Multivariate normal distribution. R Sampling from the multivariate normal distribution. 8.1 Multinomial random variables Exercise 8.1 Let \\(X_i\\), \\(i = 1,...,k\\) represent \\(k\\) events, and \\(p_i\\) the probabilities of these events happening in a trial. Let \\(n\\) be the number of trials, and \\(X\\) a multivariate random variable, the collection of \\(X_i\\). Then \\(p(x) = \\frac{n!}{x_1!x_2!...x_k!} p_1^{x_1} p_2^{x_2}...p_k^{x_k}\\) is the PMF of a multinomial distribution. Show that the marginal distribution of \\(X_i\\) is a binomial distribution. Take 1000 samples from the multinomial distribution with \\(n=4\\) and probabilities \\(p = (0.2, 0.2, 0.5, 0.1)\\). Then take 1000 samples from four binomial distributions with the same parameters. Inspect the results visually. Solution. We will approach this proof from the probabilistic point of view. W.L.O.G. let \\(x_1\\) be the marginal distribution we are interested in. The term \\(p^{x_1}\\) denotes the probability that event 1 happened \\(x_1\\) times. For this event not to happen, one of the other events needs to happen. So for each of the remaining trials, the probability of another event is \\(\\sum_{i=2}^k p_i = 1 - p_1\\), and there were \\(n - x_1\\) such trials. What is left to do is to calculate the number of permutations of event 1 happening and event 1 not happening. We choose \\(x_1\\) trials, from \\(n\\) trials. Therefore \\(p(x_1) = \\binom{n}{x_1} p_1^{x_1} (1 - p_1)^{n - x_1}\\), which is the binomial PMF. Interested students are encouraged to prove this mathematically. set.seed(1) nsamps &lt;- 1000 samps_mult &lt;- rmultinom(nsamps, 4, prob = c(0.2, 0.2, 0.5, 0.1)) samps_mult &lt;- as_tibble(t(samps_mult)) %&gt;% gather() samps &lt;- tibble( V1 = rbinom(nsamps, 4, 0.2), V2 = rbinom(nsamps, 4, 0.2), V3 = rbinom(nsamps, 4, 0.5), V4 = rbinom(nsamps, 4, 0.1) ) %&gt;% gather() %&gt;% bind_rows(samps_mult) %&gt;% bind_cols(&quot;dist&quot; = c(rep(&quot;binomial&quot;, 4*nsamps), rep(&quot;multinomial&quot;, 4*nsamps))) ggplot(samps, aes(x = value, fill = dist)) + geom_bar(position = &quot;dodge&quot;) + facet_wrap(~ key) Exercise 8.2 (Multinomial expected value) Find the expected value, variance and covariance of the multinomial distribution. Solution. The expected value… Use the differentiation trick. 8.2 Multivariate normal random variables Exercise 8.3 (Cholesky decomposition) Let \\(X\\) be a random vector of length \\(k\\) with \\(X_i \\sim \\text{N}(0, 1)\\) and \\(LL^*\\) the Cholesky decomposition of a Hermitian positive-definite matrix \\(A\\). Let \\(\\mu\\) be a vector of length \\(k\\). Find the distribution of the random vector \\(Y = \\mu + L X\\). Find the Cholesky decomposition of \\(A = \\begin{bmatrix} 2 &amp; 1.2 \\\\ 1.2 &amp; 1 \\end{bmatrix}\\). R: Use the results from a) and b) to sample from the MVN distribution \\(\\text{N}(\\mu, A)\\), where \\(\\mu = [1.5, -1]^T\\). Plot a scatterplot and compare it to direct samples from the multivariate normal distribution (rmvnorm). R: \\(L\\) is a linear map$. Plot 10 points from \\(X\\) and 10 points from the transformation \\(L X\\) (Hint: use color or shapes). Compare and discuss the results. Solution. \\(X\\) has an independent normal distribution of dimension \\(k\\). Then \\[\\begin{align} Y = \\mu + L X &amp;\\sim \\text{N}(\\mu, LL^T) \\\\ &amp;\\sim \\text{N}(\\mu, A). \\end{align}\\] \\[\\begin{align} \\begin{bmatrix} a &amp; 0 \\\\ b &amp; c \\end{bmatrix} \\begin{bmatrix} a &amp; b \\\\ 0 &amp; c \\end{bmatrix} = \\begin{bmatrix} 2 &amp; 1.2 \\\\ 1.2 &amp; 1 \\end{bmatrix} \\end{align}\\] # a set.seed(1) nsamps &lt;- 1000 X &lt;- matrix(data = rnorm(nsamps * 2), ncol = 2) mu &lt;- c(1.5, -1) L &lt;- matrix(data = c(sqrt(2), 0, 1.2 / sqrt(2), sqrt(1 - 1.2^2/2)), ncol = 2, byrow = TRUE) Y &lt;- t(mu + L %*% t(X)) plot_df &lt;- data.frame(rbind(X, Y), c(rep(&quot;X&quot;, nsamps), rep(&quot;Y&quot;, nsamps))) colnames(plot_df) &lt;- c(&quot;D1&quot;, &quot;D2&quot;, &quot;var&quot;) ggplot(data = plot_df, aes(x = D1, y = D2, colour = as.factor(var))) + geom_point() # b nsamps &lt;- 10 X &lt;- matrix(data = rnorm(nsamps * 2), ncol = 2) L &lt;- matrix(data = c(sqrt(2), 0, 1.2 / sqrt(2), sqrt(1 - 1.2^2/2)), ncol = 2, byrow = TRUE) Y &lt;- t(L %*% t(X)) plot_df &lt;- data.frame(rbind(X, Y), c(rep(&quot;X&quot;, nsamps), rep(&quot;Y&quot;, nsamps)), c(1:nsamps, 1:nsamps)) colnames(plot_df) &lt;- c(&quot;D1&quot;, &quot;D2&quot;, &quot;var&quot;, &quot;samp&quot;) ggplot(data = plot_df, aes(x = D1, y = D2, color = as.factor(var), shape = as.factor(samp))) + geom_point() + scale_shape_manual(values=1:10) Exercise 8.4 (Eigendecomposition) R: Let \\(\\Sigma = U \\Lambda U^T\\) be the eigendecomposition of covariance matrix \\(\\sigma\\). Follow the procdeure below, to sample from a multivariate normal with \\(\\mu = [-2, 1]^T\\) and \\(\\Sigma = \\begin{bmatrix} 0.3, -0.5 \\\\ -0.5, 1.6 \\end{bmatrix}\\): Sample from two independent standardized normal distributions to get \\(X\\). Find the eigen decomposition of \\(X\\) (eigen). Multiply \\(X\\) by \\(\\Lambda^{\\frac{1}{2}}\\) to get \\(X2\\). Consider how the eigendecomposition for \\(X2\\) changes compared to \\(X\\). Multiply \\(X2\\) by \\(U\\) to get \\(X3\\). Consider how the eigendecomposition for \\(X3\\) changes compared to \\(X2\\). Add \\(\\mu\\) to \\(X3\\). Consider how the eigendecomposition for \\(X4\\) changes compared to \\(X3\\). Plot the data and the eigenvectors (scaled with \\(\\Lambda^{\\frac{1}{2}}\\)) at each step. Hint: Use geom_segment for the eigenvectors. # a set.seed(1) sigma &lt;- matrix(data = c(0.3, -0.5, -0.5, 1.6), nrow = 2, byrow = TRUE) ed &lt;- eigen(sigma) e_val &lt;- ed$values e_vec &lt;- ed$vectors # b set.seed(1) nsamps &lt;- 1000 X &lt;- matrix(data = rnorm(nsamps * 2), ncol = 2) vec1 &lt;- matrix(c(1,0,0,1), nrow = 2) X2 &lt;- t(sqrt(diag(e_val)) %*% t(X)) vec2 &lt;- sqrt(diag(e_val)) %*% vec1 X3 &lt;- t(e_vec %*% t(X2)) vec3 &lt;- e_vec %*% vec2 X4 &lt;- t(c(-2, 1) + t(X3)) vec4 &lt;- c(-2, 1) + vec3 vec_mat &lt;- data.frame(matrix(c(0,0,0,0,0,0,0,0,0,0,0,0,-2,1,-2,1), ncol = 2, byrow = TRUE), t(cbind(vec1, vec2, vec3, vec4)), c(1,1,2,2,3,3,4,4)) df &lt;- data.frame(rbind(X, X2, X3, X4), c(rep(1, nsamps), rep(2, nsamps), rep(3, nsamps), rep(4, nsamps))) colnames(df) &lt;- c(&quot;D1&quot;, &quot;D2&quot;, &quot;wh&quot;) colnames(vec_mat) &lt;- c(&quot;D1&quot;, &quot;D2&quot;, &quot;E1&quot;, &quot;E2&quot;, &quot;wh&quot;) ggplot(data = df, aes(x = D1, y = D2)) + geom_point() + geom_segment(data = vec_mat, aes(xend = E1, yend = E2), color = &quot;red&quot;) + facet_wrap(~ wh) + coord_fixed() Exercise 8.5 (Marginal and conditional distributions) Let \\(X \\sim \\text{N}(\\mu, \\Sigma)\\), where $= [2, 0, -1]^T and \\(\\Sigma = \\begin{bmatrix} 1 &amp; -0.2 &amp; 0.5 \\\\ -0.2 &amp; 1.4 &amp; 0 \\\\ 0.5 &amp; 0 &amp; 2 \\\\ \\end{bmatrix}\\). R: For the calculation in the following points, you can use R. Find the distribution of \\(\\Sigma_A\\). Find the distribution of \\(\\Sigma_A | Sigma_B\\). Find the distribution of. Find the distribution of. Find the distribution of. R: Visually compare the distributions of a) and b), and c), d) and e) at different conditional values. mu &lt;- c(2, 0, -1) Sigma &lt;- matrix(c(1, -0.2, 0.5, -0.2, 1.4, -1.2, 0.5, -1.2, 2), nrow = 3, byrow = TRUE) mu_A &lt;- c(2, 0) mu_B &lt;- -1 Sigma_A &lt;- Sigma[1:2, 1:2] Sigma_B &lt;- Sigma[3, 3] Sigma_AB &lt;- Sigma[1:2, 3] # b tmp_b &lt;- Sigma_AB * (1 / Sigma_B) mu_b &lt;- mu_A - tmp_b * mu_B Sigma_b &lt;- Sigma_B - t(Sigma_AB) %*% solve(Sigma_A) %*% Sigma_AB tmp_b ## [1] 0.25 -0.60 mu_b ## [1] 2.25 -0.60 Sigma_b ## [,1] ## [1,] 0.8602941 # d Sigma_d &lt;- Sigma_A - (Sigma_AB * (1 / Sigma_B)) %*% t(Sigma_AB) Sigma_d ## [,1] [,2] ## [1,] 0.875 0.10 ## [2,] 0.100 0.68 Sigma_d ## [,1] [,2] ## [1,] 0.875 0.10 ## [2,] 0.100 0.68 Solution. \\(\\mu_A = [2, 0, -1]^T\\) and \\(\\Sigma_A = \\begin{bmatrix} 1 &amp; -0.2 &amp; \\\\ -0.2 &amp; 1.4 \\\\ \\end{bmatrix}\\). \\[\\begin{align} X_A | X_B &amp;= b \\sim \\text{N}(\\mu_t, \\Sigma_t), \\\\ \\mu_t &amp;= [2.25, -0.6]^T + [2.25, -0.6]^T b, \\\\ \\Sigma_t &amp;= \\begin{bmatrix} 0.875 &amp; 0.1 \\\\ 0.1 &amp; 0.68 \\\\ \\end{bmatrix} \\end{align}\\] 8.3 Transformations Exercise 1.5 Let \\((U,V)\\) be a random variable with PDF \\(p(u,v) = \\frac{1}{8 \\sqrt{u}}\\), \\(U \\in [0,4]\\) and \\(V \\in [\\sqrt{U}, \\sqrt{U} - 1]\\). Let \\(X = \\sqrt{U}\\) and \\(Y = V - \\sqrt{U}\\). Find PDF of \\((X,Y)\\). What can you tell about distributions of \\(X\\) and \\(Y\\)? This exercise shows how we can simplify a probabilistic problem with a clever use of transformations. R: Take 1000 samples from \\((X,Y)\\) and transform them with inverses of the above functions to get samples from \\((U,V)\\). Plot both sets of samples. Solution. First we need to find the inverse functions. Since \\(x = \\sqrt{u}\\) it follows that \\(u = x^2\\), and that \\(x \\in [-2,2]\\). Similarly \\(v = y + x\\) and \\(y \\in [0,1]\\). Let us first find the Jacobian. \\[\\renewcommand\\arraystretch{1.6} J(x,y) = \\begin{bmatrix} \\frac{\\partial u}{\\partial x} &amp; \\frac{\\partial v}{\\partial x} \\\\%[1ex] % &lt;-- 1ex more space between rows of matrix \\frac{\\partial u}{\\partial y} &amp; \\frac{\\partial v}{\\partial y} \\end{bmatrix} = \\begin{bmatrix} 2x &amp; 1 \\\\%[1ex] % &lt;-- 1ex more space between rows of matrix 0 &amp; 1 \\end{bmatrix}, \\] and the determinant is \\(|J(x,y)| = 2x\\). Putting everything together, we get \\[\\begin{align} p_{X,Y}(x,y) = p_{U,V}(x^2, y + x) |J(x,y)| = \\frac{1}{8 \\sqrt{x^2}} 2x = \\frac{1}{4}. \\end{align}\\] This reminds us of the Uniform distribution. Indeed we can see that \\(p_X(x) = \\frac{1}{4}\\) and \\(p_Y(y) = 1\\). So instead of dealing with an awkward PDF of \\((U,V)\\) and the corresponding dynamic bounds, we are now looking at two independent Uniform random variables. In practice, this could make modeling much easier. set.seed(1) nsamps &lt;- 1000 x &lt;- runif(nsamps, min = -2, max = 2) y &lt;- runif(nsamps) orig &lt;- tibble(x = x, y = y, vrs = &quot;original&quot;) u &lt;- x^2 v &lt;- y + x transf &lt;- tibble(x = u, y = v, vrs = &quot;transformed&quot;) df &lt;- bind_rows(orig, transf) ggplot(df, aes(x = x, y = y, color = vrs)) + geom_point(alpha = 0.3) Exercise 4.5 R: Write a function that will calculate the probability density of an arbitraty multivariate normal distribution, based on independent standardized normal PDFs. Compare? Compare? Solution. TODO: Inverse of the transformation (exists, because the inverse of Q exists). Jacobi (simplified?). set.seed(1) "],
["r-programming-language.html", "A R programming language A.1 Basic characteristics A.2 Why R? A.3 Setting up A.4 R basics A.5 Functions A.6 Other tips A.7 Further reading and references A.8 Learning outcomes", " A R programming language A.1 Basic characteristics R is free software for statistical computing and graphics. It is widely used by statisticians, scientists, and other professionals for software development and data analysis. It is an interpreted language and therefore the programs do not need compilation. A.2 Why R? R is one of the main two languages used for statistics and machine learning (the other being Python). Pros Libraries. Comprehensive collection of statistical and machine learning packages. Easy to code. Open source. Anyone can access R and develop new methods. Additionally, it is relatively simple to get source code of established methods. Large community. The use of R has been rising for some time, in industry and academia. Therefore a large collection of blogs and tutorials exists, along with people offering help on pages like StackExchange and CrossValidated. Integration with other languages and LaTeX. New methods. Many researchers develop R packages based on their research, therefore new methods are available soon after development. Cons Slow. Programs run slower than in other programming languages, however this can be somewhat ammended by effective coding or integration with other languages. Memory intensive. This can become a problem with large data sets, as they need to be stored in the memory, along with all the information the models produce. Some packages are not as good as they should be, or have poor documentation. Object oriented programming in R can be very confusing and complex. A.3 Setting up https://www.r-project.org/ A.3.1 RStudio RStudio is the most widely used IDE for R. It is completely free A.3.2 Libraries for data science dplyr efficient data manipulation ggplot2 plotting stats several statistical models rstan Bayesian inference. MCMCpack rmarkdown and knitr Dynamic reports. devtools Package development. A.4 R basics A.4.1 Variables and types Important information and tips: no type declaration define variables with &lt;- instead of = (although both work, there is a slight difference, additionally most of the packages use the arrow) for strings use \"\" for comments use # change types with as.type() functions no special type for single character like C++ for example n &lt;- 20 x &lt;- 2.7 m &lt;- n # m gets value 20 my_flag &lt;- TRUE student_name &lt;- &quot;Luka&quot; typeof(n) ## [1] &quot;double&quot; typeof(student_name) ## [1] &quot;character&quot; typeof(my_flag) ## [1] &quot;logical&quot; typeof(as.integer(n)) ## [1] &quot;integer&quot; typeof(as.character(n)) ## [1] &quot;character&quot; A.4.2 Basic operations n + x ## [1] 22.7 n - x ## [1] 17.3 diff &lt;- n - x # variable diff gets the difference between n and x diff ## [1] 17.3 n * x ## [1] 54 n / x ## [1] 7.407407 x^2 ## [1] 7.29 sqrt(x) ## [1] 1.643168 n &gt; 2 * n ## [1] FALSE n == n ## [1] TRUE n == 2 * n ## [1] FALSE n != n ## [1] FALSE paste(student_name, &quot;is&quot;, n, &quot;years old&quot;) ## [1] &quot;Luka is 20 years old&quot; A.4.3 Vectors use c() to combine elements into vectors can only contain one type of variable if different types are provided, all are transformed to the most basic type in the vector access elements by indexes or logical vectors of the same length a scalar value is regarded as a vector of length 1 1:4 # creates a vector of integers from 1 to 4 ## [1] 1 2 3 4 student_ages &lt;- c(20, 23, 21) student_names &lt;- c(&quot;Luke&quot;, &quot;Jen&quot;, &quot;Mike&quot;) passed &lt;- c(TRUE, TRUE, FALSE) length(student_ages) ## [1] 3 # access by index student_ages[2] ## [1] 23 student_ages[1:2] ## [1] 20 23 student_ages[2] &lt;- 24 # change values # access by logical vectors student_ages[passed == TRUE] # same as student_ages[passed] ## [1] 20 24 student_ages[student_names %in% c(&quot;Luke&quot;, &quot;Mike&quot;)] ## [1] 20 21 student_names[student_ages &gt; 20] ## [1] &quot;Jen&quot; &quot;Mike&quot; A.4.3.1 Operations with vectors most operations are element-wise if we operate on vectors of different lengths, the shorter vector periodically repeats its elements until it reaches the length of the longer one a &lt;- c(1, 3, 5) b &lt;- c(2, 2, 1) d &lt;- c(6, 7) a + b ## [1] 3 5 6 a * b ## [1] 2 6 5 a + d ## Warning in a + d: longer object length is not a multiple of shorter object ## length ## [1] 7 10 11 a + 2 * b ## [1] 5 7 7 a &gt; b ## [1] FALSE TRUE TRUE b == a ## [1] FALSE FALSE FALSE a %*% b # vector multiplication, not element-wise ## [,1] ## [1,] 13 A.4.4 Factors vectors of finite predetermined classes suitable for categorical variables ordinal (ordered) or nominal (unordered) car_brand &lt;- factor(c(&quot;Audi&quot;, &quot;BMW&quot;, &quot;Mercedes&quot;, &quot;BMW&quot;), ordered = FALSE) car_brand ## [1] Audi BMW Mercedes BMW ## Levels: Audi BMW Mercedes freq &lt;- factor(x = NA, levels = c(&quot;never&quot;,&quot;rarely&quot;,&quot;sometimes&quot;,&quot;often&quot;,&quot;always&quot;), ordered = TRUE) freq[1:3] &lt;- c(&quot;rarely&quot;, &quot;sometimes&quot;, &quot;rarely&quot;) freq ## [1] rarely sometimes rarely ## Levels: never &lt; rarely &lt; sometimes &lt; often &lt; always freq[4] &lt;- &quot;quite_often&quot; # non-existing level, returns NA ## Warning in `[&lt;-.factor`(`*tmp*`, 4, value = &quot;quite_often&quot;): invalid factor ## level, NA generated freq ## [1] rarely sometimes rarely &lt;NA&gt; ## Levels: never &lt; rarely &lt; sometimes &lt; often &lt; always A.4.5 Matrices two-dimensional generalizations of vectors my_matrix &lt;- matrix(c(1, 2, 1, 5, 4, 2), nrow = 2, byrow = TRUE) my_matrix ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 5 4 2 my_square_matrix &lt;- matrix(c(1, 3, 2, 3), nrow = 2) my_square_matrix ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 3 my_matrix[1,2] # first row, second column ## [1] 2 my_matrix[2, ] # second row ## [1] 5 4 2 my_matrix[ ,3] # third column ## [1] 1 2 A.4.5.1 Matrix functions and operations most operation element-wise mind the dimensions when using matrix multiplication %*% nrow(my_matrix) # number of matrix rows ## [1] 2 ncol(my_matrix) # number of matrix columns ## [1] 3 dim(my_matrix) # matrix dimension ## [1] 2 3 t(my_matrix) # transpose ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 4 ## [3,] 1 2 diag(my_matrix) # the diagonal of the matrix as vector ## [1] 1 4 diag(1, nrow = 3) # creates a diagonal matrix ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 det(my_square_matrix) # matrix determinant ## [1] -3 my_matrix + 2 * my_matrix ## [,1] [,2] [,3] ## [1,] 3 6 3 ## [2,] 15 12 6 my_matrix * my_matrix # element-wise multiplication ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 25 16 4 my_matrix %*% t(my_matrix) # matrix multiplication ## [,1] [,2] ## [1,] 6 15 ## [2,] 15 45 my_vec &lt;- as.vector(my_matrix) # transform to vector my_vec ## [1] 1 5 2 4 1 2 A.4.6 Arrays multi-dimensional generalizations of matrices my_array &lt;- array(c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2)) my_array[1, 1, 1] ## [1] 1 my_array[2, 2, 1] ## [1] 4 my_array[1, , ] ## [,1] [,2] ## [1,] 1 5 ## [2,] 3 7 dim(my_array) ## [1] 2 2 2 A.4.7 Data frames basic data structure for analysis differ from matrices as columns can be of different types student_data &lt;- data.frame(&quot;Name&quot; = student_names, &quot;Age&quot; = student_ages, &quot;Pass&quot; = passed) student_data ## Name Age Pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE ## 3 Mike 21 FALSE colnames(student_data) &lt;- c(&quot;name&quot;, &quot;age&quot;, &quot;pass&quot;) # change column names student_data[1, ] ## name age pass ## 1 Luke 20 TRUE student_data[ ,colnames(student_data) %in% c(&quot;name&quot;, &quot;pass&quot;)] ## name pass ## 1 Luke TRUE ## 2 Jen TRUE ## 3 Mike FALSE student_data$pass # access column by name ## [1] TRUE TRUE FALSE student_data[student_data$pass == TRUE, ] ## name age pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE A.4.8 Lists useful for storing different data structures access elements with double square brackets elements can be named first_list &lt;- list(student_ages, my_matrix, student_data) second_list &lt;- list(student_ages, my_matrix, student_data, first_list) first_list[[1]] ## [1] 20 24 21 second_list[[4]] ## [[1]] ## [1] 20 24 21 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 5 4 2 ## ## [[3]] ## name age pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE ## 3 Mike 21 FALSE second_list[[4]][[1]] # first element of the fourth element of second_list ## [1] 20 24 21 length(second_list) ## [1] 4 second_list[[length(second_list) + 1]] &lt;- &quot;add_me&quot; # append an element names(first_list) &lt;- c(&quot;Age&quot;, &quot;Matrix&quot;, &quot;Data&quot;) first_list$Age ## [1] 20 24 21 A.4.9 Loops mostly for loop for loop can iterate over an arbitrary vector # iterate over consecutive natural numbers my_sum &lt;- 0 for (i in 1:10) { my_sum &lt;- my_sum + i } my_sum ## [1] 55 # iterate over an arbirary vector my_sum &lt;- 0 some_numbers &lt;- c(2, 3.5, 6, 100) for (i in some_numbers) { my_sum &lt;- my_sum + i } my_sum ## [1] 111.5 A.5 Functions for help use ?function_name A.5.1 Writing functions We can write our own functions with function(). In the brackets, we define the parameters the function gets, and in curly brackets we define what the function does. We use return() to return values. sum_first_n_elements &lt;- function (n) { my_sum &lt;- 0 for (i in 1:n) { my_sum &lt;- my_sum + i } return (my_sum) } sum_first_n_elements(10) ## [1] 55 A.6 Other tips Use set.seed(arbitrary_number) at the beginning of a script to set the seed and ensure replication. To dynamically set the working directory in R Studio to the parent folder of a R script use setwd(dirname(rstudioapi::getSourceEditorContext()$path)). To avoid slow R loops use the apply family of functions. See ?apply and ?lapply. To make your data manipulation (and therefore your life) a whole lot easier, use the dplyr package. Use getAnywhere(function_name) to get the source code of any function. Use browser for debugging. See ?browser. A.7 Further reading and references Getting started with R Studio: https://www.youtube.com/watch?v=lVKMsaWju8w Official R manuals: https://cran.r-project.org/manuals.html Cheatsheets: https://www.rstudio.com/resources/cheatsheets/ Workshop on R, dplyr, ggplot2, and R Markdown: https://github.com/bstatcomp/Rworkshop A.8 Learning outcomes Data science students should work towards obtaining the knowledge and the skills that enable them to: Use the R programming language for common programming tasks, data manipulation, file I/0, etc. Find suitable R packages for the task at hand and use them. Recognize when R is and when it is not a suitable language to use. "],
["probability-distributions.html", "B Probability distributions", " B Probability distributions Name pdf/pmf cdf mean variance Use in R relationships Bernoulli \\(p^k (1 - p)^{1 - k}\\) 1.11 \\(p\\) 7.1 \\(p(1-p)\\) 7.1 binomial \\(\\binom{n}{k} p^k (1 - p)^{n - k}\\) 1.11 \\(np\\) 7.2 \\(np(1-p)\\) 7.2 Poisson \\(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\) 1.12 \\(\\lambda\\) 7.3 \\(\\lambda\\) 7.3 geometric \\(p(1-p)^k\\) 1.12 \\(1 - (1-p)^{k + 1}\\) 4.4 \\(\\frac{1 - p}{p}\\) 7.4 TODO: MGF Normal \\(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(x - \\mu)^2}{2 \\sigma^2}}\\) 4.11 \\(\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(t - \\mu)^2}{2 \\sigma^2}} dt\\) 4.11 MVN categorical dirichlet beta \\(\\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}\\) 4.9 gamma \\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\beta x}\\) 4.10 \\(\\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}\\) 4.10 \\(\\frac{\\alpha}{\\beta}\\) 7.5 \\(\\frac{\\alpha}{\\beta^2}\\) 7.5 exponential \\(\\lambda e^{-\\lambda x}\\) 4.7 \\(1 - e^{-\\lambda x}\\) 4.7 \\(\\lambda^{-1}\\) 7.7 \\(\\lambda^{-2}\\) 7.7 logistic \\(\\frac{e^{-\\frac{x - \\mu}{s}}}{(1 + e{-\\frac{x - \\mu}{s}})^2}\\) 4.12 \\(\\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}}\\) 4.12 rayleigh t F \\(\\chi^2\\) Wishart Pareto Weibull negative binomial \\(\\binom{k + r - 1}{k}(1-p)^r p^k\\) 4.6 multinomial \\(\\frac{n!}{x_1!x_2!...x_k!} p_1^{x_1} p_2^{x_2}...p_k^{x_k}\\) 8.1 "],
["references.html", "References", " References "]
]
