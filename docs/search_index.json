[
["rvs.html", "Chapter 4 Random variables 4.1 General properties and calculations 4.2 Discrete random variables 4.3 Continuous random variables 4.4 Transformations", " Chapter 4 Random variables This chapter deals with random variables. The students are expected to acquire the following knowledge: Theoretical R sampling from distributions calculating PDF, CDF, and quantile functions plotting results facet wrap 4.1 General properties and calculations Exercise 4.1 Which of the functions below are valid CDFs? Find their respective densities. R: Plot the three functions. \\[\\begin{equation} F(x) = \\begin{cases} 1 - e^{-x^2} &amp; x \\geq 0 \\\\ 0 &amp; x &lt; 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} e^{-\\frac{1}{x}} &amp; x &gt; 0 \\\\ 0 &amp; x \\leq 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x \\leq 0 \\\\ \\frac{1}{3} &amp; 0 &lt; x \\leq \\frac{1}{2} \\\\ 1 &amp; x &gt; \\frac{1}{2}. \\end{cases} \\end{equation}\\] Solution. Yes. First, let us check the limits. \\(\\lim_{x \\rightarrow -\\infty} (0) = 0\\). \\(\\lim_{x \\rightarrow \\infty} (1 - e^{-x^2}) = 1 - \\lim_{x \\rightarrow \\infty} e^{-x^2} = 1 - 0 = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(1 - e^{-x^2} \\geq 1 - e^{-y^2}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} 1 - e^{-\\epsilon^2} = 1 - \\lim_{\\epsilon \\downarrow 0} e^{-\\epsilon^2} = 1 - 1 = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} 1 - e^{-x^2} = 2xe^{-x^2}.\\) Students are encouraged to check that this is a proper PDF. Yes. First, let us check the limits. $_{x -} (0) = 0 and \\(\\lim_{x \\rightarrow \\infty} (e^{-\\frac{1}{x}}) = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(e^{-\\frac{1}{x}} \\geq e^{-\\frac{1}{y}}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} e^{-\\frac{1}{\\epsilon}} = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} e^{-\\frac{1}{x}} = \\frac{1}{x^2}e^{-\\frac{1}{x}}.\\) Students are encouraged to check that this is a proper PDF. No. The function is not right continuous as \\(F(\\frac{1}{2}) = \\frac{1}{3}\\), but \\(\\lim_{\\epsilon \\downarrow 0} F(\\frac{1}{2} + \\epsilon) = 1\\). f1 &lt;- function (x) { tmp &lt;- 1 - exp(-x^2) tmp[x &lt; 0] &lt;- 0 return(tmp) } f2 &lt;- function (x) { tmp &lt;- exp(-(1 / x)) tmp[x &lt;= 0] &lt;- 0 return(tmp) } f3 &lt;- function (x) { tmp &lt;- x tmp[x == x] &lt;- 1 tmp[x &lt;= 0.5] &lt;- 1/3 tmp[x &lt;= 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 20, by = 0.001), f1 = f1(x), f2 = f2(x), f3 = f3(x)) %&gt;% melt(id.vars = &quot;x&quot;) # geo_plot &lt;- ggplot(data = data.frame(x = seq(-1, 10, by = 0.01)), aes(x = x)) + # stat_function(aes(color = &quot;f1&quot;), fun = f1) + # stat_function(aes(color = &quot;f2&quot;), fun = f2) + # stat_function(aes(color = &quot;f3&quot;), fun = f3) + # geom_hline(yintercept = 1) # plot(geo_plot) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = value, color = variable)) + geom_hline(yintercept = 1) + geom_line() plot(cdf_plot) Exercise 4.2 Let \\(X\\) be a random variable with CDF \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\frac{x^2}{2} &amp; 0 \\leq x &lt; 1 \\\\ \\frac{1}{2} + \\frac{p}{2} &amp; 1 \\leq x &lt; 2 \\\\ \\frac{1}{2} + \\frac{p}{2} + \\frac{1 - p}{2} &amp; x \\geq 2 \\end{cases} \\end{equation}\\] R: Plot this CDF for \\(p = 0.3\\). Is it a discrete, continuous, or mixed random varible? Find the probability density/mass of \\(X\\). f1 &lt;- function (x, p) { tmp &lt;- x tmp[x &gt;= 2] &lt;- 0.5 + (p * 0.5) + ((1-p) * 0.5) tmp[x &lt; 2] &lt;- 0.5 + (p * 0.5) tmp[x &lt; 1] &lt;- (x[x &lt; 1])^2 / 2 tmp[x &lt; 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 5, by = 0.001), y = f1(x, 0.3)) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = y)) + geom_hline(yintercept = 1) + geom_line(color = &quot;blue&quot;) plot(cdf_plot) Solution. \\(X\\) is a mixed random variable. Since \\(X\\) is a mixed random variable, we have to find the PDF of the continuous part and the PMF of the discrete part. We get the continuous part by differentiating the corresponding CDF, \\(\\frac{d}{dx}\\frac{x^2}{2} = x\\). So the PDF, when \\(0 \\leq x &lt; 1\\), is \\(p(x) = x\\). Let us look at the discrete part now. It has two steps, so this is a discrete distribution with two outcomes – numbers two and three. The first happens with probability \\(\\frac{p}{2}\\), and the second with probability \\(\\frac{1 - p}{2}\\). This reminds us of the bernoulli distribution, the only difference is that the probabilities of outcomes are halved, as they need to be suitably normalized. So the PMF for the discrete part is \\(P(X = x) = (\\frac{p}{2})^{2 - \\lfloor x \\rfloor} (\\frac{1 - p}{2})^{\\lfloor x \\rfloor - 1}\\). Exercise 4.3 (Convolutions) Convolutions are probability distributions that correspond to sums of independent random variables. Let \\(X\\) and \\(Y\\) be independent discrete variables. Find the PMF of \\(Z = X + Y\\). Hint: Use the law of total probability. Let \\(X\\) and \\(Y\\) be independent continuous variables. Find the PDF of \\(Z = X + Y\\) Hint: Start with the CDF. Solution. \\[\\begin{align} P(Z = z) &amp;= P(X + Y = z) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + Y = z | Y = k) P(Y = k) &amp; \\text{ (law of total probability)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z | Y = k) P(Y = k) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z) P(Y = k) &amp; \\text{ (independence of $X$ and $Y$)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X = z - k) P(Y = k). &amp; \\end{align}\\] Let \\(f\\) and \\(g\\) be the PDFs of \\(X\\) and \\(Y\\) respectively. \\[\\begin{align} F(z) &amp;= P(Z &lt; z) \\\\ &amp;= P(X + Y &lt; z) \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + Y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X &lt; z - y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy \\end{align}\\] Now \\[\\begin{align} p(z) &amp;= \\frac{d}{dz} F(z) &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\frac{d}{dz}\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} f(z - y) g(y) dy &amp; \\text{ (fundamental theorem of calculus)}. \\end{align}\\] 4.2 Discrete random variables Exercise 4.4 (Binomial random variable) Let \\(X_k\\), \\(k = 1,...,n\\), be random variables with the Bernoulli measure as the PMF with \\(p = 0.4\\). Let \\(X = \\sum_{k=1}^n\\). We call \\(X_k\\) a Bernoulli random variable. Find the CDF of \\(X_k\\). Find PDF of \\(X\\). Find CDF of \\(X\\). R: Simulate from the binomial distribution with \\(n = 10\\) and \\(p = 0.5\\), and from \\(n\\) Bernoulli distributions with \\(p = 0.5\\). Visually compare the sum of Bernoullis and the binomial. Hint: there is no standard function like rpois for a Bernoulli random variable. Check exercise ?? to find out how to sample from a Bernoulli distribution. Solution. There are two outcomes – zero and one. Zero happens with probability \\(1 - p\\). Therefore \\[\\begin{equation} F(k) = \\begin{cases} 0 &amp; k &lt; 0 \\\\ 1 - p &amp; 0 \\leq k &lt; 1 \\\\ 1 &amp; k \\geq 1. \\end{cases} \\end{equation}\\] For the probability of \\(X\\) to be equal to some \\(k \\leq n\\), exactly \\(k\\) Bernoulli variables need to be one, and the others zero. So \\(p^k(1-p)^{n-k}\\). There are \\(\\binom{n}{k}\\) such possible arrangements. Therefore \\[\\begin{align} P(X = k) = P(\\sum X_k = 2) = \\binom{n}{k} p^k (1 - p)^{n-k}. \\end{align}\\] \\[\\begin{equation} F(k) = \\sum_{i = 1}^{\\lfloor k \\rfloor} \\binom{n}{i} p^i (1 - p)^{n - i} \\end{equation}\\] set.seed(1) nsamps &lt;- 10000 binom_samp &lt;- rbinom(nsamps, size = 10, prob = 0.5) bernoulli_mat &lt;- matrix(data = NA, nrow = nsamps, ncol = 10) for (i in 1:nsamps) { bernoulli_mat[i, ] &lt;- rbinom(10, size = 1, prob = 0.5) } bern_samp &lt;- apply(bernoulli_mat, 1, sum) b_data &lt;- tibble(x = c(binom_samp, bern_samp), type = c(rep(&quot;binomial&quot;, 10000), rep(&quot;Bernoulli_sum&quot;, 10000))) b_plot &lt;- ggplot(data = b_data, aes(x = x, fill = type)) + geom_bar(position = &quot;dodge&quot;) plot(b_plot) Exercise 4.5 (Geometric random variable) A variable with PMF \\[\\begin{equation} $p(1-p)^k$ \\end{equation}\\] is a geometric random variable with support in non-negative integers. It has one positive parameter \\(p\\). Derive the CDF of a geometric random variable. R: Draw 1000 samples from the geometric distribution with \\(p\\) = 0.3$ and compare their frequencies to theoretical values. Solution. \\[\\begin{align} P(X \\leq k) &amp;= \\sum_{i = 0}^k p(1-p)^i \\\\ &amp;= p \\sum_{i = 0}^k (1-p)^i \\\\ &amp;= p \\frac{1 - (1-p)^{k+1}}{1 - (1 - p)} \\\\ &amp;= 1 - (1-p)^{k + 1} \\end{align}\\] set.seed(1) geo_samp &lt;- rgeom(n = 1000, prob = 0.3) geo_samp &lt;- data.frame(x = geo_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dgeom(0:20, prob = 0.3), type = &quot;theoretical_measure&quot;)) geo_plot &lt;- ggplot(data = geo_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geo_plot) Exercise 4.6 (Poisson random variable) A variable with PMF \\(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\) is a Poisson random variable with support in non-negative integers. It has one positive parameter \\(\\lambda\\), which also represents its mean value and variance (a measure of the deviation of the values from the mean – more on mean and variance in the next chapter). This distribution is usually the default choice for modeling counts. We have already encountered a Poisson random variable in exercise ??, where we also sampled from this distribution. The CDF of a Poisson random variable is \\(P(X &lt;= x) = e^{-\\lambda} \\sum_{i=0}^x \\frac{\\lambda^{i}}{i!}\\). R: Draw 1000 samples from the Poisson distribution with \\(p\\) = 0.3$ and compare their empirical cumulative distribution function with the theoretical CDF. set.seed(1) pois_samp &lt;- rpois(n = 1000, lambda = 5) pois_samp &lt;- data.frame(x = pois_samp) pois_plot &lt;- ggplot(data = pois_samp, aes(x = x, colour = &quot;ECDF&quot;)) + stat_ecdf(geom = &quot;step&quot;) + geom_step(data = tibble(x = 0:17, y = ppois(x, 5)), aes(x = x, y = y, colour = &quot;CDF&quot;)) + # stat_function(data = data.frame(x = 0:17), aes(x = x, colour = &quot;CDF&quot;), geom = &quot;line&quot;, fun = ppois, args = list(lambda = 5)) + scale_colour_manual(&quot;Lgend title&quot;, values = c(&quot;black&quot;, &quot;red&quot;)) plot(pois_plot) Exercise 4.7 (Negative binomial random variable) A variable with PMF \\[\\begin{equation} $p(k) = \\binom{k + r - 1}{k}(1-p)^r p^k$ \\end{equation}\\] is a negative binomial random variable with support in non-negative integers. It has two parameters \\(r &gt; 0\\) and \\(p \\in (0,1)\\). Derive the CDF of a geometric random variable. R: Draw samples from … and plot the distributions of those samples using facet_wrap. Solution. \\[\\begin{align} P(X \\leq k) &amp;= \\sum_{i = 0}^k p(1-p)^i \\\\ &amp;= p \\sum_{i = 0}^k (1-p)^i \\\\ &amp;= p \\frac{1 - (1-p)^{k+1}}{1 - (1 - p)} \\\\ &amp;= 1 - (1-p)^{k + 1} \\end{align}\\] set.seed(1) geo_samp &lt;- rgeom(n = 1000, prob = 0.3) geo_samp &lt;- data.frame(x = geo_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dgeom(0:20, prob = 0.3), type = &quot;theoretical_measure&quot;)) geo_plot &lt;- ggplot(data = geo_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geo_plot) 4.3 Continuous random variables Exercise 4.8 (Exponential random variable) A variable \\(X\\) with PDF \\(\\lambda e^{-\\lambda x}\\) is an exponential random variable with support in non-negative real numbers. It has one positive parameter \\(\\lambda\\). \\[\\begin{equation} X | \\lambda \\sim \\text{Exp}(\\lambda) \\end{equation}\\] Derive the CDF of an exponential random variable. Derive the quantile function of an exponential random variable. Calculate the probability \\(P(1 \\leq X \\leq 3)\\), where \\(X \\sim \\text{Exp(1.5)}\\). R: Check your answer to c) with a simulation (rexp). Plot the probability in a meaningful way. R: Implement PDF, CDF, and the quantile function and compare their values with corresponding R functions visually. Hint: use the size parameter in geom_line to make one of the curves wider. Solution. \\[\\begin{align} F(x) &amp;= \\int_{0}^{x} \\lambda e^{-\\lambda t} dt \\\\ &amp;= \\lambda \\int_{0}^{x} e^{-\\lambda t} dt \\\\ &amp;= \\lambda (\\frac{1}{-\\lambda}e^{-\\lambda t} |_{0}^{x}) \\\\ &amp;= \\lambda(\\frac{1}{\\lambda} - \\frac{1}{\\lambda} e^{-\\lambda x}) \\\\ &amp;= 1 - e^{-\\lambda x}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(x)) &amp;= x \\\\ 1 - e^{-\\lambda F^{-1}(x)} &amp;= x \\\\ e^{-\\lambda F^{-1}(x)} &amp;= 1 - x \\\\ -\\lambda F^{-1}(x) &amp;= \\ln(1 - x) \\\\ F^{-1}(x) &amp;= - \\frac{ln(1 - x)}{\\lambda}. \\end{align}\\] \\[\\begin{align} P(1 \\leq X \\leq 3) &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= 1 - e^{-1.5 \\times 3} - 1 + e^{-1.5 \\times 1} \\\\ &amp;\\approx 0.212. \\end{align}\\] set.seed(1) nsamps &lt;- 1000 samps &lt;- rexp(nsamps, rate = 1.5) sum(samps &gt;= 1 &amp; samps &lt;= 3) / nsamps ## [1] 0.212 exp_plot &lt;- ggplot(data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5)) + stat_function(fun = dexp, args = list(rate = 1.5), xlim = c(1,3), geom = &quot;area&quot;, fill = &quot;red&quot;) plot(exp_plot) exp_pdf &lt;- function(x, lambda) { return (lambda * exp(-lambda * x)) } exp_cdf &lt;- function(x, lambda) { return (1 - exp(-lambda * x)) } exp_quant &lt;- function(q, lambda) { return (-(log(1 - q) / lambda)) } ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_pdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = pexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_cdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = qexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_quant, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) Exercise 4.9 (Uniform random variable) Continuous uniform random variable with parameters \\(a\\) and \\(b\\) has the PDF \\[\\begin{equation} p(x) = \\begin{cases} \\frac{1}{b - a} &amp; x \\in [a,b] \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation}\\] Derive the CDF of the uniform random variable. Derive the quantile function of the uniform random variable. Let \\(X \\sim \\text{Uniform}(a,b)\\). Derive the CDF of the variable \\(Y = \\frac{X - a}{b - a}\\). This is the standard uniform random variable. Let \\(X \\sim \\text{Uniform}(-1, 3)\\). Find such \\(z\\) that \\(P(X &lt; z + \\mu_x) = \\frac{1}{5}\\). R: Check your result from d) using simulation. Solution. \\[\\begin{align} F(x) &amp;= \\int_{a}^x \\frac{1}{b - a} dt \\\\ &amp;= \\frac{1}{b - a} \\int_{a}^x dt \\\\ &amp;= \\frac{x - a}{b - a}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(p)) &amp;= p \\\\ \\frac{F^{-1}(p) - a}{b - a} &amp;= p \\\\ F^{-1}(p) &amp;= p(b - a) + a. \\end{align}\\] \\[\\begin{align} F_Y(y) &amp;= P(Y &lt; y) \\\\ &amp;= P(\\frac{X - a}{b - a} &lt; y) \\\\ &amp;= P(X &lt; y(b - a) + a) \\\\ &amp;= F_X(y(b - a) + a) \\\\ &amp;= \\frac{(y(b - a) + a) - a}{b - a} \\\\ &amp;= y. \\end{align}\\] \\[\\begin{align} P(X &lt; z + 1) &amp;= \\frac{1}{5} \\\\ F(z + 1) &amp;= \\frac{1}{5} \\\\ z + 1 &amp;= F^{-1}(\\frac{1}{5}) \\\\ z &amp;= \\frac{1}{5}4 - 1 - 1 \\\\ z &amp;= -1.2. \\end{align}\\] set.seed(1) a &lt;- -1 b &lt;- 3 nsamps &lt;- 10000 unif_samp &lt;- runif(nsamps, a, b) mu_x &lt;- mean(unif_samp) new_samp &lt;- unif_samp - mu_x quantile(new_samp, probs = 1/5) ## 20% ## -1.203192 punif(-0.2, -1, 3) ## [1] 0.2 Exercise 4.10 (Beta random variable) A variable \\(X\\) with PDF \\[\\begin{equation} p(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}, \\end{equation}\\] where \\(\\text{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) and \\(\\Gamma(x) = \\int_0^{\\infty} x^{z - 1} e^{-x} dx\\) is a Beta random variable with support on \\([0,1]\\). It has two positive parameters \\(\\alpha\\) and \\(\\beta\\). Notation: \\[\\begin{equation} X | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta) \\end{equation}\\] It is often used in modeling rates. The CDF of a Beta random variable is \\[\\begin{equation} F(x) = \\frac{\\text{B}(x; \\alpha, \\beta)}{\\text{B}(\\alpha, \\beta)}, \\end{equation}\\] where \\(\\text{B}(x; \\alpha, \\beta) = \\int_0^x t^{\\alpha+1} (1 - t)^{\\beta - 1} dt\\). Calculate the PDF for \\(\\alpha = 1\\) and \\(\\beta = 1\\). What do you notice? R: Plot densities of the beta distribution for \\(\\alpha = 0.5\\), \\(\\beta = 0.5\\), \\(\\alpha = 4\\), \\(\\beta = 1\\), \\(\\alpha = 1\\), \\(\\beta = 4\\), \\(alpha = 0.1\\), \\(\\beta = 0.1\\). R: Sample from \\(X \\sim \\text{Beta}(2, 5)\\) and compare the histogram with Beta PDF. Solution. \\[\\begin{equation} p(x) = \\frac{x^{1 - 1} (1 - x)^{1 - 1}}{\\text{B}(1, 1)} = 1. \\end{equation}\\] This is the standard uniform distribution. set.seed(1) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), aes(color = &quot;alpha = 0.5&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 4, shape2 = 1), aes(color = &quot;alpha = 4&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 4), aes(color = &quot;alpha = 1&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5), aes(color = &quot;alpha = 25&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 0.1, shape2 = 0.1), aes(color = &quot;alpha = 0.1&quot;)) set.seed(1) nsamps &lt;- 1000 samps &lt;- rbeta(nsamps, 2, 5) ggplot(data = data.frame(x = samps), aes(x = x)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;) + stat_function(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x), fun = dbeta, args = list(shape1 = 2, shape2 = 5), color = &quot;red&quot;, size = 1.2) Exercise 4.11 (Gamma random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\beta x} \\end{equation}\\] is a Gamma random variable with parameters shape \\(\\alpha &gt; 0\\) and rate \\(\\beta &gt; 0\\). We write \\[\\begin{equation} X \\sim \\text{Gamma}(\\alpha, \\beta) \\end{equation}\\] and it’s CDF is \\[\\begin{equation} \\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}, \\end{equation}\\] where \\(\\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt\\). It is usually used in modeling positive phenomena (for example insurance claims and rainfalls). Let \\(X \\sim \\text{Gamma}(1, \\beta)\\). Find the PDF of \\(X\\). Do you recognize this PDF? Let \\(k = \\alpha\\) and \\(\\theta = \\frac{1}{\\beta}\\). Find the PDF of \\(X | k, \\theta \\sim \\text{Gamma}(k, \\theta)\\). Most (?ALL) random variables can be reparameterized, and sometimes a reparameterized distribution is more suitable for certain calculations. The first parameterization is for example usually used in Bayesian statistics, while this parameterization is more common in econometrics and some other applied fields. Note that you also need to pay attention to the parameters in statistical software, so diligently read the help files when using functions like rgamma to see how the function is parameterized. R: Plot gamma CDF for random variables with shape and rate parameters (1,1), (10,1), (1,10). Solution. \\[\\begin{align} p(x) &amp;= \\frac{\\beta^1}{\\Gamma(1)} x^{1 - 1}e^{-\\beta x} \\\\ &amp;= \\beta e^{-\\beta x} \\end{align}\\] This is the PDF of the exponential distribution with parameter \\(\\beta\\). \\[\\begin{align} p(x) &amp;= \\frac{1}{\\Gamma(k)\\beta^k} x^{k - 1}e^{-\\frac{x}{\\theta}}. \\end{align}\\] set.seed(1) ggplot(data = data.frame(x = seq(0, 25, by = 0.01)), aes(x = x)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 1), aes(color = &quot;Gamma(1,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 10, rate = 1), aes(color = &quot;Gamma(10,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 10), aes(color = &quot;Gamma(1,10)&quot;)) Exercise 4.12 (Normal random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\end{equation}\\] is a normal random variable with parameters \\(\\mu\\) in reals and \\(\\sigma^2 &gt; 0\\). The first is the mean parameter and the second is the variance parameter. Many statistical methods assume a normal distribution. We write \\[\\begin{equation} X \\sim \\text{N}(\\mu, \\sigma^2), \\end{equation}\\] and it’s CDF is \\[\\begin{equation} F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(t - \\mu)^2}{2 \\sigma^2}} dt, \\end{equation}\\] which is intractable and is usually approximated. Due to it’s flexibility it is also one of the most researched distributions. For that reason statisticians often use transformations of variables or approximate distributions with the normal distribution. Show that a variable \\(\\frac{X - \\mu}{\\sigma} \\sim \\text{N}(0,1)\\). This transformation is called standardization, and \\(\\text{N}(0,1)\\) is a standardized normal distribution. R: Plot the normal distribution with \\(\\mu = 0\\) and different values for the \\(\\sigma\\) parameter. R: The normal distribution provides a good approximation for the Poisson distribution with a large \\(\\lambda\\). Let \\(X \\sim \\text{Poisson}(50)\\). Approximate \\(X\\) with the normal distribution and compare it’s density with the Poisson histogram. What are the values of \\(\\mu\\) and \\(\\sigma^2\\) that should provide the best approximation? Note that R function rnorm takes standard deviation (\\(\\sigma\\)) as a parameter and not variance. Solution. \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= P(X &lt; \\sigma x + \\mu) \\\\ &amp;= F(\\sigma x + \\mu) \\\\ &amp;= \\int_{-\\infty}^{\\sigma x + \\mu} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(t - \\mu)^2}{2\\sigma^2}} dt \\end{align}\\] Now let \\(s = f(t) = \\frac{t - \\mu}{\\sigma}\\), then \\(ds = \\frac{dt}{\\sigma}\\) and \\(f(\\sigma x + \\mu) = x\\), so \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2 \\pi}} e^{\\frac{s^2}{2}} ds. \\end{align}\\] There is no need to evaluate this integral, as we recognize it as the CDF of a normal distribution with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). set.seed(1) # a ggplot(data = data.frame(x = seq(-15, 15, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = &quot;sd = 1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 0.4), aes(color = &quot;sd = 0.1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;sd = 2&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 5), aes(color = &quot;sd = 5&quot;)) # b mean_par &lt;- 50 nsamps &lt;- 100000 pois_samps &lt;- rpois(nsamps, lambda = mean_par) norm_samps &lt;- rnorm(nsamps, mean = mean_par, sd = sqrt(mean_par)) my_plot &lt;- ggplot() + geom_bar(data = tibble(x = pois_samps), aes(x = x, y = (..count..)/sum(..count..))) + geom_density(data = tibble(x = norm_samps), aes(x = x), color = &quot;red&quot;) plot(my_plot) Exercise 4.13 (Logistic random variable) A logistic random variable has CDF \\[\\begin{equation} F(x) = \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}}, \\end{equation}\\] where \\(\\mu\\) is real and \\(s &gt; 0\\). We write \\[\\begin{equation} X \\sim \\text{Logistic}(\\mu, s). \\end{equation}\\] The distribution of the logistic random variable resembles a normal random variable, however it has heavier tails. Find the PDF of a logistic random variable. R: Implement logistic PDF and CDF and visually compare both for \\(X \\sim \\text{N}(0, 1)\\) and \\(Y \\sim \\text{logit}(0, \\sqrt{\\frac{3}{\\pi^2}})\\). These distributions have the same mean and variance. Additionally, plot the same plot on the interval [5,10], to better see the difference in the tails. R: For the distributions in b) find the probability \\(P(|X| &gt; 4)\\) and interpret the result. Solution. \\[\\begin{align} p(x) &amp;= \\frac{d}{dx} \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}} \\\\ &amp;= \\frac{- \\frac{d}{dx} (1 + e^{-\\frac{x - \\mu}{s}})}{(1 + e{-\\frac{x - \\mu}{s}})^2} \\\\ &amp;= \\frac{e^{-\\frac{x - \\mu}{s}}}{(1 + e{-\\frac{x - \\mu}{s}})^2}. \\end{align}\\] # a set.seed(1) logit_pdf &lt;- function (x, mu, s) { return ((exp(-(x - mu)/(s))) / (s * (1 + exp(-(x - mu)/(s)))^2)) } nl_plot &lt;- ggplot(data = data.frame(x = seq(-12, 12, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(12/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) nl_plot &lt;- ggplot(data = data.frame(x = seq(5, 10, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(12/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) # b logit_cdf &lt;- function (x, mu, s) { return (1 / (1 + exp(-(x - mu) / s))) } p_logistic &lt;- 1 - logit_cdf(4, 0, sqrt(12/pi^2)) + logit_cdf(-4, 0, sqrt(12/pi^2)) p_norm &lt;- 1 - pnorm(4, 0, 2) + pnorm(-4, 0, 2) p_logistic ## [1] 0.05178347 p_norm ## [1] 0.04550026 # Logistic distribution has wider tails, therefore the probability of larger # absolute values is higher. 4.4 Transformations "]
]
