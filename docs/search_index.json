[
["index.html", "Principles of Uncertainty – exercises Preface", " Principles of Uncertainty – exercises Gregor Pirš and Erik Štrumbelj 2019-11-05 Preface These are the exercises for the Principles of Uncertainty course of the Data Science Master’s at University of Ljubljana, Faculty of Computer and Information Science. This document will be extended each week as the course progresses. At the end of each exercise session, we will post the solutions to the exercises worked in class and select exercises for homework. Students are also encouraged to solve the remaining exercises to further extend their knowledge. Some exercises require the use of R. Those exercises (or parts of) are coloured blue. Students that are not familiar with R programming language should study A to learn the basics. As the course progresses, we will cover more relevant uses of R for data science. "],
["introduction.html", "Chapter 1 Probability spaces 1.1 Measure and probability spaces 1.2 Properties of probability measures 1.3 Discrete probability spaces", " Chapter 1 Probability spaces This chapter deals with measures and probability spaces. At the end of the chapter, we look more closely at discrete probability spaces. The students are expected to acquire the following knowledge: Theoretical Use properties of probability to calculate probabilities. Combinatorics. Understanding of continuity of probability. R Vectors and vector operations. For loop. Estimating probability with simulation. sample function. Matrices and matrix operations. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 1.1 Measure and probability spaces Exercise 1.1 (Completing a set to a sigma algebra) Let \\(\\Omega = \\{1,2,...,10\\}\\) and let \\(A = \\{\\emptyset, \\{1\\}, \\{2\\}, \\Omega \\}\\). Show that \\(A\\) is not a sigma algebra of \\(\\Omega\\). Find the minimum number of elements to complete A to a sigma algebra of \\(\\Omega\\). Solution. \\(1^c = \\{2,3,...,10\\} \\notin A \\implies\\) \\(A\\) is not sigma algebra. First we need the complements of all elements, so we need to add sets \\(\\{2,3,...,10\\}\\) and \\(\\{1,3,4,...,10\\}\\). Next we need unions of all sets – we add the set \\(\\{1,2\\}\\). Again we need the complement of this set, so we add \\(\\{3,4,...,10\\}\\). So the minimum number of elements we need to add is 4. Exercise 1.2 (Diversity of sigma algebras) Let \\(\\Omega\\) be a set. Find the smallest sigma algebra of \\(\\Omega\\). Find the largest sigma algebra of \\(\\Omega\\). Solution. \\(A = \\{\\emptyset, \\Omega\\}\\) \\(2^{\\Omega}\\) Exercise 1.3 Find all sigma algebras for \\(\\Omega = \\{0, 1, 2\\}\\). Solution. \\(A = \\{\\emptyset, \\Omega\\}\\) \\(A = 2^{\\Omega}\\) \\(A = \\{\\emptyset, \\{0\\}, \\{1,2\\}, \\Omega\\}\\) \\(A = \\{\\emptyset, \\{1\\}, \\{0,2\\}, \\Omega\\}\\) \\(A = \\{\\emptyset, \\{2\\}, \\{0,1\\}, \\Omega\\}\\) Exercise 1.4 (Difference between algebra and sigma algebra) Let \\(\\Omega = \\mathbb{N}\\) and \\(\\mathcal{A} = \\{A \\subseteq \\mathbb{N}: A \\text{ is finite or } A^c \\text{ is finite.} \\}\\). Show that \\(\\mathcal{A}\\) is an algebra but not a sigma algebra. Solution. \\(\\emptyset\\) is finite so \\(\\emptyset \\in \\mathcal{A}\\). Let \\(A \\in \\mathcal{A}\\) and \\(B \\in \\mathcal{A}\\). If both are finite, then their union is also finite and therefore in \\(\\mathcal{A}\\). Let at least one of them not be finite. Then their union is not finite. But \\((A \\cup B)^c = A^c \\cap B^c\\). And since at least one is infinite, then its complement is finite and the intersection is too. So finite unions are in \\(\\mathcal{A}\\). Let us look at numbers \\(2n\\). For any \\(n\\), \\(2n \\in \\mathcal{A}\\) as it is finite. But \\(\\bigcup_{k = 1}^{\\infty} 2n \\notin \\mathcal{A}\\). Exercise 1.5 (Intro to measure) Take the measurable space \\(\\Omega = \\{1,2\\}\\), \\(F = 2^{\\Omega}\\). Which of the following is a measure? Which is a probability measure? \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 5\\), \\(\\mu(\\{2\\}) = 6\\), \\(\\mu(\\{1,2\\}) = 11\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 0\\), \\(\\mu(\\{1,2\\}) = 1\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 0\\), \\(\\mu(\\{1,2\\}) = 0\\) \\(\\mu(\\emptyset) = 0\\), \\(\\mu(\\{1\\}) = 0\\), \\(\\mu(\\{2\\}) = 1\\), \\(\\mu(\\{1,2\\}) = 1\\) \\(\\mu(\\emptyset)=0\\), \\(\\mu(\\{1\\})=0\\), \\(\\mu(\\{2\\})=\\infty\\), \\(\\mu(\\{1,2\\})=\\infty\\) Solution. Measure. Not probability measure since \\(\\mu(\\Omega) &gt; 1\\). Neither due to countable additivity. Measure. Not probability measure since \\(\\mu(\\Omega) = 0\\). Probability measure. Measure. Not probability measure since \\(\\mu(\\Omega) &gt; 1\\). Exercise 1.6 Define a probability space that could be used to model the outcome of throwing two fair 6-sided dice. Solution. \\(\\Omega = \\{\\{i,j\\}, i = 1,...,6, j = 1,...,6\\}\\) \\(F = 2^{\\Omega}\\) \\(\\forall \\omega \\in \\Omega\\), \\(P(\\omega) = \\frac{1}{6} \\times \\frac{1}{6} = \\frac{1}{36}\\) 1.2 Properties of probability measures Exercise 1.7 A standard deck (52 cards) is distributed to two persons: 26 cards to each person. All partitions are equally likely. Find the probability that: The first person gets 4 Queens. The first person gets at least 2 Queens. R: Use simulation (sample) to check the above answers. Solution. \\(\\frac{\\binom{48}{22}}{\\binom{52}{26}}\\) 1 - \\(\\frac{\\binom{48}{26} + 4 \\times \\binom{48}{25}}{\\binom{52}{26}}\\) For the simulation, let us represent cards with numbers from 1 to 52, and let 1 through 4 represent Queens. set.seed(1) cards &lt;- 1:52 n &lt;- 10000 q4 &lt;- vector(mode = &quot;logical&quot;, length = n) q2 &lt;- vector(mode = &quot;logical&quot;, length = n) tmp &lt;- vector(mode = &quot;logical&quot;, length = n) for (i in 1:n) { p1 &lt;- sample(1:52, 26) q4[i] &lt;- sum(1:4 %in% p1) == 4 q2[i] &lt;- sum(1:4 %in% p1) &gt;= 2 } sum(q4) / n ## [1] 0.0556 sum(q2) / n ## [1] 0.6995 Exercise 1.8 Let \\(A\\) and \\(B\\) be events with probabilities \\(P(A) = \\frac{2}{3}\\) and \\(P(B) = \\frac{1}{2}\\). Show that \\(\\frac{1}{6} \\leq P(A\\cap B) \\leq \\frac{1}{2}\\), and give examples to show that both extremes are possible. Find corresponding bounds for \\(P(A\\cup B)\\). R: Draw samples from the examples and show the probability bounds of \\(P(A \\cap B)\\) . Solution. - From the properties of probability we have \\[\\begin{equation} P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\leq 1. \\end{equation}\\] From this follows \\[\\begin{align} P(A \\cap B) &amp;\\geq P(A) + P(B) - 1 \\\\ &amp;= \\frac{2}{3} + \\frac{1}{2} - 1 \\\\ &amp;= \\frac{1}{6}, \\end{align}\\] which is the lower bound for the intersection. Conversely, we have \\[\\begin{equation} P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\geq P(A). \\end{equation}\\] From this follows \\[\\begin{align} P(A \\cap B) &amp;\\leq P(B) \\\\ &amp;= \\frac{1}{2}, \\end{align}\\] which is the upper bound for the intersection. For an example take a fair die. To achieve the lower bound let \\(A = \\{1,2,3\\}\\) and \\(B = \\{3,4,5,6\\}\\), then their intersection is \\(A \\cap B = \\{3\\}\\). To achieve the upper bound take \\(A = \\{1,2,3\\}\\) and \\(B = \\{1,2,3,4\\}\\). For the bounds of the union we will use the results from the first part. Again from the properties of probability we have \\[\\begin{align} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;\\geq P(A) + P(B) - \\frac{1}{2} \\\\ &amp;= \\frac{2}{3}. \\end{align}\\] Conversely \\[\\begin{align} P(A \\cup B) &amp;= P(A) + P(B) - P(A \\cap B) \\\\ &amp;\\leq P(A) + P(B) - \\frac{1}{6} \\\\ &amp;= 1. \\end{align}\\] Therefore \\(\\frac{2}{3} \\leq P(A \\cup B) \\leq 1\\). We use sample in R: set.seed(1) n &lt;- 10000 samps &lt;- sample(1:6, n, replace = TRUE) # lower bound lb &lt;- vector(mode = &quot;logical&quot;, length = n) A &lt;- c(1,2,3) B &lt;- c(3,4,5,6) for (i in 1:n) { lb[i] &lt;- samps[i] %in% A &amp; samps[i] %in% B } sum(lb) / n ## [1] 0.1724 # upper bound ub &lt;- vector(mode = &quot;logical&quot;, length = n) A &lt;- c(1,2,3) B &lt;- c(1,2,3,4) for (i in 1:n) { ub[i] &lt;- samps[i] %in% A &amp; samps[i] %in% B } sum(ub) / n ## [1] 0.5047 Exercise 1.9 A fair coin is tossed repeatedly. Show that, with probability one, a head turns up sooner or later. Show similarly that any given finite sequence of heads and tails occurs eventually with probability one. Solution. \\[\\begin{align} P(\\text{no heads}) &amp;= \\lim_{n \\rightarrow \\infty} P(\\text{no heads in first }n \\text{ tosses}) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\frac{1}{2^n} \\\\ &amp;= 0. \\end{align}\\] For the second part, let us fix the given sequence of heads and tails of length \\(k\\) as \\(s\\). A probability that this happens in \\(k\\) tosses is \\(\\frac{1}{2^k}\\). \\[\\begin{align} P(s \\text{ occurs}) &amp;= \\lim_{n \\rightarrow \\infty} P(s \\text{ occurs in first } nk \\text{ tosses}) \\end{align}\\] The right part of the upper equation is greater than if \\(s\\) occurs either in the first \\(k\\) tosses, second \\(k\\) tosses,…, \\(n\\)-th \\(k\\) tosses. Therefore \\[\\begin{align} P(s \\text{ occurs}) &amp;\\geq \\lim_{n \\rightarrow \\infty} P(s \\text{ occurs in first } n \\text{ disjoint sequences of length } k) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} (1 - P(s \\text{ does not occur in first } n \\text{ disjoint sequences})) \\\\ &amp;= 1 - \\lim_{n \\rightarrow \\infty} P(s \\text{ does not occur in first } n \\text{ disjoint sequences}) \\\\ &amp;= 1 - \\lim_{n \\rightarrow \\infty} (1 - \\frac{1}{2^k})^n \\\\ &amp;= 1. \\end{align}\\] Exercise 1.10 An Erdos-Renyi random graph \\(G(n,p)\\) is a model with \\(n\\) nodes, where each pair of nodes is connected with probability \\(p\\). Calculate the probability that there exists a node that is not connected to any other node in \\(G(4,0.6)\\). Show that the upper bound for the probability that there exist 2 nodes that are not connected to any other node for an arbitrary \\(G(n,p)\\) is \\(\\binom{n}{2} (1-p)^{2n - 3}\\). R: Estimate the probability from the first point using simulation. Solution. Let \\(A_i\\) be the event that the \\(i\\)-th node is not connected to any other node. Then our goal is to calculate \\(P(\\cup_{i=1}^n A_i)\\). Using the inclusion-exclusion principle, we get \\[\\begin{align} P(\\cup_{i=1}^n A_i) &amp;= \\sum_i A_i - \\sum_{i&lt;j} P(A_i \\cap A_j) + \\sum_{i&lt;j&lt;k} P(A_i \\cap A_j \\cap A_k) - P(A_1 \\cap A_2 \\cap A_3 \\cap A_4) \\\\ &amp;=4 (1 - p)^3 - \\binom{4}{2} (1 - p)^5 + \\binom{4}{3} (1 - p)^6 - (1 - p)^6 \\\\ &amp;\\approx 0.21. \\end{align}\\] Let \\(A_{ij}\\) be the event that nodes \\(i\\) and \\(j\\) are not connected to any other node. We are interested in \\(P(\\cup_{i&lt;j}A_{ij})\\). By using Boole`s inequality, we get \\[\\begin{align} P(\\cup_{i&lt;j}A_{ij}) \\leq \\sum_{i&lt;j} P(A_{ij}). \\end{align}\\] What is the probability of \\(A_{ij}\\)? There need to be no connections to the \\(i\\)-th node to the remaining nodes (excluding \\(j\\)), the same for the \\(j\\)-th node, and there can be no connection between them. Therefore \\[\\begin{align} P(\\cup_{i&lt;j}A_{ij}) &amp;\\leq \\sum_{i&lt;j} (1 - p)^{2(n-2) + 1} \\\\ &amp;= \\binom{n}{2} (1 - p)^{2n - 3}. \\end{align}\\] set.seed(1) n_samp &lt;- 100000 n &lt;- 4 p &lt;- 0.6 conn_samp &lt;- vector(mode = &quot;logical&quot;, length = n_samp) for (i in 1:n_samp) { tmp_mat &lt;- matrix(data = 0, nrow = n, ncol = n) samp_conn &lt;- sample(c(0,1), choose(4,2), replace = TRUE, prob = c(1 - p, p)) tmp_mat[lower.tri(tmp_mat)] &lt;- samp_conn tmp_mat[upper.tri(tmp_mat)] &lt;- t(tmp_mat)[upper.tri(t(tmp_mat))] not_conn &lt;- apply(tmp_mat, 1, sum) if (any(not_conn == 0)) { conn_samp[i] &lt;- TRUE } else { conn_samp[i] &lt;- FALSE } } sum(conn_samp) / n_samp ## [1] 0.20565 1.3 Discrete probability spaces Exercise 1.11 Show that the standard measurable space on \\(\\Omega = \\{0,1,...,n\\}\\) equipped with binomial measure is a discrete probability space. Define another probability measure on this measurable space. Show that for \\(n=1\\) the binomial measure is the same as the Bernoulli measure. R: Draw 1000 samples from the binomial distribution \\(p=0.5\\), \\(n=20\\) (rbinom) and compare relative frequencies with theoretical probability measure. Solution. We need to show that the terms of \\(\\sum_{k=0}^n \\binom{n}{k} p^k (1 - p)^{n - k}\\) sum to 1. For that we use the binomial theorem \\(\\sum_{k=0}^n \\binom{n}{k} x^k y^{n-k} = (x + y)^n\\). So \\[\\begin{equation} \\sum_{k=0}^n \\binom{n}{k} p^k (1 - p)^{n - k} = (p + 1 - p)^n = 1. \\end{equation}\\] \\(P(\\{k\\}) = \\frac{1}{n + 1}\\). When \\(n=1\\) then \\(k \\in \\{0,1\\}\\). Inserting \\(n=1\\) into the binomial measure, we get \\(\\binom{1}{k}p^k (1-p)^{1 - k}\\). Now \\(\\binom{1}{1} = \\binom{1}{0} = 1\\), so the measure is \\(p^k (1-p)^{1 - k}\\), which is the Bernoulli measure. set.seed(1) library(ggplot2) library(dplyr) bin_samp &lt;- rbinom(n = 1000, size = 20, prob = 0.5) bin_samp &lt;- data.frame(x = bin_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dbinom(0:20, size = 20, prob = 0.5), type = &quot;theoretical_measure&quot;)) bin_plot &lt;- ggplot(data = bin_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(bin_plot) Exercise 1.12 Show that the standard measurable space on \\(\\Omega = \\{0,1,...,\\infty\\}\\) equipped with geometric measure is a discrete probability space, equipped with Poisson measure is a discrete probability space. Define another probability measure on this measurable space. R: Draw 1000 samples from the Poisson distribution \\(\\lambda = 10\\) (rpois) and compare relative frequencies with theoretical probability measure. Solution. \\(\\sum_{k = 0}^{\\infty} p(1 - p)^k = p \\sum_{k = 0}^{\\infty} (1 - p)^k = p \\frac{1}{1 - 1 + p} = 1\\). We used the formula for geometric series. \\(\\sum_{k = 0}^{\\infty} \\frac{\\lambda^k e^{-\\lambda}}{k!} = e^{-\\lambda} \\sum_{k = 0}^{\\infty} \\frac{\\lambda^k}{k!} = e^{-\\lambda} e^{\\lambda} = 1.\\) We used the Taylor expansion of the exponential function. Since we only have to define a probability measure, we could only assign probabilities that sum to one to a finite number of events in \\(\\Omega\\), and probability zero to the other infinite number of events. However to make this solution more educational, we will try to find a measure that assigns a non-zero probability to all events in \\(\\Omega\\). A good start for this would be to find a converging infinite series, as the probabilities will have to sum to one. One simple converging series is the geometric series \\(\\sum_{k=0}^{\\infty} p^k\\) for \\(|p| &lt; 1\\). Let us choose an arbitrary \\(p = 0.5\\). Then \\(\\sum_{k=0}^{\\infty} p^k = \\frac{1}{1 - 0.5} = 2\\). To complete the measure, we have to normalize it, so it sums to one, therefore \\(P(\\{k\\}) = \\frac{0.5^k}{2}\\) is a probability measure on \\(\\Omega\\). We could make it even more difficult by making this measure dependent on some parameter \\(\\alpha\\), but this is out of the scope of this introductory chapter. set.seed(1) pois_samp &lt;- rpois(n = 1000, lambda = 10) pois_samp &lt;- data.frame(x = pois_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:25, n = dpois(0:25, lambda = 10), type = &quot;theoretical_measure&quot;)) pois_plot &lt;- ggplot(data = pois_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(pois_plot) Exercise 1.13 Define a probability measure on \\((\\Omega = \\mathbb{Z}, 2^{\\mathbb{Z}})\\). Define a probability measure such that \\(P(\\omega) &gt; 0, \\forall \\omega \\in \\Omega\\). R: Implement a random generator that will generate samples with the relative frequency that corresponds to your probability measure. Compare relative frequencies with theoretical probability measure . Solution. \\(P(0) = 1, P(\\omega) = 0, \\forall \\omega \\neq 0\\). \\(P(\\{k\\}) = \\sum_{k = -\\infty}^{\\infty} \\frac{p(1 - p)^{|k|}}{2^{1 - 1_0(k)}}\\), where \\(1_0(k)\\) is the indicator function, which equals to one if \\(k\\) is 0, and equals to zero in every other case. n &lt;- 1000 geom_samps &lt;- rgeom(n, prob = 0.5) sign_samps &lt;- sample(c(FALSE, TRUE), size = n, replace = TRUE) geom_samps[sign_samps] &lt;- -geom_samps[sign_samps] my_pmf &lt;- function (k, p) { indic &lt;- rep(1, length(k)) indic[k == 0] &lt;- 0 return ((p * (1 - p)^(abs(k))) / 2^indic) } geom_samps &lt;- data.frame(x = geom_samps) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = -10:10, n = my_pmf(-10:10, 0.5), type = &quot;theoretical_measure&quot;)) geom_plot &lt;- ggplot(data = geom_samps, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geom_plot) Exercise 1.14 Define a probability measure on \\(\\Omega = \\{1,2,3,4,5,6\\}\\) with parameter \\(m \\in \\{1,2,3,4,5,6\\}\\), so that the probability of outcome at distance \\(1\\) from \\(m\\) is half of the probability at distance \\(0\\), at distance \\(2\\) is half of the probability at distance \\(1\\), etc. R: Implement a random generator that will generate samples with the relative frequency that corresponds to your probability measure. Compare relative frequencies with theoretical probability measure . Solution. \\(P(\\{k\\}) = \\frac{\\frac{1}{2}^{|m - k|}}{\\sum_{i=1}^6 \\frac{1}{2}^{|m - i|}}\\) n &lt;- 10000 m &lt;- 4 my_pmf &lt;- function (k, m) { denom &lt;- sum(0.5^abs(m - 1:6)) return (0.5^abs(m - k) / denom) } samps &lt;- c() for (i in 1:n) { a &lt;- sample(1:6, 1) a_val &lt;- my_pmf(a, m) prob &lt;- runif(1) if (prob &lt; a_val) { samps &lt;- c(samps, a) } } samps &lt;- data.frame(x = samps) %&gt;% count(x) %&gt;% mutate(n = n / length(samps), type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 1:6, n = my_pmf(1:6, m), type = &quot;theoretical_measure&quot;)) my_plot &lt;- ggplot(data = samps, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(my_plot) "],
["uprobspaces.html", "Chapter 2 Uncountable probability spaces 2.1 Borel sets 2.2 Lebesgue measure", " Chapter 2 Uncountable probability spaces This chapter deals with uncountable probability spaces. The students are expected to acquire the following knowledge: Theoretical Understand Borel sets and identify them. Estimate Lebesgue measure for different sets. Know when sets are Borel-measurable. Understanding of countable and uncountable sets. R Uniform sampling. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 2.1 Borel sets Exercise 2.1 Prove that the intersection of two sigma algebras on \\(\\Omega\\) is a sigma algebra. Prove that the collection of all open subsets \\((a,b)\\) on \\((0,1]\\) is not a sigma algebra of \\((0,1]\\). Solution. Empty set: \\[\\begin{equation} \\emptyset \\in \\mathcal{A} \\wedge \\emptyset \\in \\mathcal{B} \\Rightarrow \\emptyset \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Complement: \\[\\begin{equation} \\text{Let } A \\in \\mathcal{A} \\cap \\mathcal{B} \\Rightarrow A \\in \\mathcal{A} \\wedge A \\in \\mathcal{B} \\Rightarrow A^c \\in \\mathcal{A} \\wedge A^c \\in \\mathcal{B} \\Rightarrow A^c \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Countable additivity: Let \\(\\{A_i\\}\\) be a countable sequence of subsets in \\(\\mathcal{A} \\cap \\mathcal{B}\\). \\[\\begin{equation} \\forall i: A_i \\in \\mathcal{A} \\cap \\mathcal{B} \\Rightarrow A_i \\in \\mathcal{A} \\wedge A_i \\in \\mathcal{B} \\Rightarrow \\cup A_i \\in \\mathcal{A} \\wedge \\cup A_i \\in \\mathcal{B} \\Rightarrow \\cup A_i \\in \\mathcal{A} \\cap \\mathcal{B} \\end{equation}\\] Let \\(A\\) denote the collection of all open subsets \\((a,b)\\) on \\((0,1]\\). Then \\((0,1) \\in A\\). But \\((0,1)^c = 1 \\notin A\\). Exercise 2.2 Show that \\(\\mathcal{C} = \\sigma(\\mathcal{C})\\) if and only if \\(\\mathcal{C}\\) is a sigma algebra. Solution. “\\(\\Rightarrow\\)” This follows from the definition of a generated sigma algebra. “\\(\\Leftarrow\\)” Let \\(\\mathcal{F} = \\cap_i F_i\\) be the intersection of all sigma algebras that contain \\(\\mathcal{C}\\). Then \\(\\sigma(\\mathcal{C}) = \\mathcal{F}\\). Additionally, \\(\\forall i: \\mathcal{C} \\in F_i\\). So each \\(F_i\\) can be written as \\(F_i = \\mathcal{C} \\cup D\\), where \\(D\\) are the rest of the elements in the sigma algebra. In other words, each sigma algebra in the collection contains at least \\(\\mathcal{C}\\), but can contain other elements. Now for some \\(j\\), \\(F_j = \\mathcal{C}\\) as \\(\\{F_i\\}\\) contains all sigma algebras that contain \\(\\mathcal{C}\\) and \\(\\mathcal{C}\\) is such a sigma algebra. Since this is the smallest subset in the intersection it follows that \\(\\sigma(\\mathcal{C}) = \\mathcal{F} = \\mathcal{C}\\). Exercise 2.3 Let \\(\\mathcal{C}\\) and \\(\\mathcal{D}\\) be two collections of subsets on \\(\\Omega\\) such that \\(\\mathcal{C} \\subset \\mathcal{D}\\). Prove that \\(\\sigma(\\mathcal{C}) \\subseteq \\sigma(\\mathcal{D})\\). Solution. \\(\\sigma(\\mathcal{D})\\) is a sigma algebra that contains \\(\\mathcal{D}\\). It follows that \\(\\sigma(\\mathcal{D})\\) is a sigma algebra that contains \\(\\mathcal{C}\\). Let us write \\(\\sigma(\\mathcal{C}) = \\cap_i F_i\\), where \\(\\{F_i\\}\\) is the collection of all sigma algebras that contain \\(\\mathcal{C}\\). Since \\(\\sigma(\\mathcal{D})\\) is such a sigma algebra, there exists an index \\(j\\), so that \\(F_j = \\sigma(\\mathcal{D})\\). Then we can write \\[\\begin{align} \\sigma(\\mathcal{C}) &amp;= (\\cap_{i \\neq j} F_i) \\cap \\sigma(\\mathcal{D}) \\\\ &amp;\\subset \\sigma(\\mathcal{D}). \\end{align}\\] Exercise 2.4 Prove that the following subsets of \\((0,1]\\) are Borel-measurable by finding their measure. Any countable set. The set of numbers in (0,1] whose decimal expansion does not contain 7. Solution. This follows directly from the fact that every countable set is a union of singletons, whose measure is 0. Let us first look at numbers which have a 7 as the first decimal numbers. Their measure is 0.1. Then we take all the numbers with a 7 as the second decimal number (excluding those who already have it as the first). These have the measure 0.01, and there are 9 of them, so their total measure is 0.09. We can continue to do so infinitely many times. At each \\(n\\), we have the measure of the intervals which is \\(10^n\\) and the number of those intervals is \\(9^{n-1}\\). Now \\[\\begin{align} \\lambda(A) &amp;= 1 - \\sum_{n = 0}^{\\infty} \\frac{9^n}{10^{n+1}} \\\\ &amp;= 1 - \\frac{1}{10} \\sum_{n = 0}^{\\infty} (\\frac{9}{10})^n \\\\ &amp;= 1 - \\frac{1}{10} \\frac{10}{1} \\\\ &amp;= 0. \\end{align}\\] Since we have shown that the measure of the set is \\(0\\), we have also shown that the set is measurable. Exercise 2.5 Let \\(\\Omega = [0,1]\\), and let \\(\\mathcal{F}_3\\) consist of all countable subsets of \\(\\Omega\\), and all subsets of \\(\\Omega\\) having a countable complement. Show that \\(\\mathcal{F}_3\\) is a sigma algebra. Let us define \\(P(A)=0\\) if \\(A\\) is countable, and \\(P(A) = 1\\) if \\(A\\) has a countable complement. Is \\((\\Omega, \\mathcal{F}_3, P)\\) a legitimate probability space? Solution. The empty set is countable, therefore it is in \\(\\mathcal{F}_3\\). For any \\(A \\in \\mathcal{F}_3\\). If \\(A\\) is countable, then \\(A^c\\) has a countable complement and is in \\(\\mathcal{F}_3\\). If \\(A\\) is uncountable, then it has a countable complement \\(A^c\\) which is therefore also in \\(\\mathcal{F}_3\\). We are left with showing countable additivity. Let \\(\\{A_i\\}\\) be an arbitrary collection of sets in \\(\\mathcal{F}_3\\). We will look at two possibilities. First let all \\(A_i\\) be countable. A countable union of countable sets is countable, and therefore in \\(\\mathcal{F}_3\\). Second, let at least one \\(A_i\\) be uncountable. It follows that it has a countable complement. We can write \\[\\begin{equation} (\\cup_{i=1}^{\\infty} A_i)^c = \\cap_{i=1}^{\\infty} A_i^c. \\end{equation}\\] Since at least one \\(A_i^c\\) on the right side is countable, the whole intersection is countable, and therefore the union has a countable complement. It follows that the union is in \\(\\mathcal{F}_3\\). The tuple \\((\\Omega, \\mathcal{F}_3)\\) is a measurable space. Therefore, we only need to check whether \\(P\\) is a probability measure. The measure of the empty set is zero as it is countable. We have to check for countable additivity. Let us look at three situations. Let \\(A_i\\) be disjoint sets. First, let all \\(A_i\\) be countable. \\[\\begin{equation} P(\\cup_{i=1}^{\\infty} A_i) = \\sum_{i=1}^{\\infty}P( A_i)) = 0. \\end{equation}\\] Since the union is countable, the above equation holds. Second, let exactly one \\(A_i\\) be uncountable. W.L.O.G. let that be \\(A_1\\). Then \\[\\begin{equation} P(\\cup_{i=1}^{\\infty} A_i) = 1 + \\sum_{i=2}^{\\infty}P( A_i)) = 1. \\end{equation}\\] Since the union is uncountable, the above equation holds. Third, let at least two \\(A_i\\) be uncountable. We have to check whether it is possible for two uncountable sets in \\(\\mathcal{F}_3\\) to be disjoint. If that is possible, then their measures would sum to more than one and \\(P\\) would not be a probability measure. W.L.O.G. let \\(A_1\\) and \\(A_2\\) be uncountable. Then we have \\[\\begin{equation} A_1 \\cap A_2 = (A_1^c \\cup A_2^c)^c. \\end{equation}\\] Now \\(A_1^c\\) and \\(A_2^c\\) are countable and their union is therefore countable. Let \\(B = A_1^c \\cup A_2^c\\). So the intersection of \\(A_1\\) and \\(A_2\\) equals the complement of \\(B\\), which is countable. For the intersection to be the empty set, \\(B\\) would have to equal to \\(\\Omega\\). But \\(\\Omega\\) is uncountable and therefore \\(B\\) can not equal to \\(\\Omega\\). It follows that two uncountable sets in \\(\\mathcal{F}_3\\) can not have an empty intersection. Therefore the tuple is a legitimate probability space. 2.2 Lebesgue measure Exercise 2.6 Show that the Lebesgue measure of rational numbers on \\([0,1]\\) is 0. R: Implement a random number generator, which generates uniform samples of irrational numbers in \\([0,1]\\) by uniformly sampling from \\([0,1]\\) and rejecting a sample if it is rational. Solution. There are a countable number of rational numbers. Therefore, we can write \\[\\begin{align} \\lambda(\\mathbb{Q}) &amp;= \\lambda(\\cup_{i = 1}^{\\infty} q_i) &amp;\\\\ &amp;= \\sum_{i = 1}^{\\infty} \\lambda(q_i) &amp;\\text{ (countable additivity)} \\\\ &amp;= \\sum_{i = 1}^{\\infty} 0 &amp;\\text{ (Lebesgue measure of a singleton)} \\\\ &amp;= 0. \\end{align}\\] Exercise 2.7 Prove that the Lebesgue measure of \\(\\mathbb{R}\\) is infinity. Paradox. Show that the cardinality of \\(\\mathbb{R}\\) and \\((0,1)\\) is the same, while their Lebesgue measures are infinity and one respectably. Solution. Let \\(a_i\\) be the \\(i\\)-th integer for \\(i \\in \\mathbb{Z}\\). We can write \\(\\mathbb{R} = \\cup_{-\\infty}^{\\infty} (a_i, a_{i + 1}]\\). \\[\\begin{align} \\lambda(\\mathbb{R}) &amp;= \\lambda(\\cup_{i = -\\infty}^{\\infty} (a_i, a_{i + 1}]) \\\\ &amp;= \\lambda(\\lim_{n \\rightarrow \\infty} \\cup_{i = -n}^{n} (a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\lambda(\\cup_{i = -n}^{n} (a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\sum_{i = -n}^{n} \\lambda((a_i, a_{i + 1}]) \\\\ &amp;= \\lim_{n \\rightarrow \\infty} \\sum_{i = -n}^{n} 1 \\\\ &amp;= \\lim_{n \\rightarrow \\infty} 2n \\\\ &amp;= \\infty. \\end{align}\\] We need to find a bijection between \\(\\mathbb{R}\\) and \\((0,1)\\). A well-known function that maps from a bounded interval to \\(\\mathbb{R}\\) is the tangent. To make the bijection easier to achieve, we will take the inverse, which maps from \\(\\mathbb{R}\\) to \\((-\\frac{\\pi}{2}, \\frac{\\pi}{2})\\). However, we need to change the function so it maps to \\((0,1)\\). First we add \\(\\frac{\\pi}{2}\\), so that we move the function above zero. Then we only have to divide by the max value, which in this case is \\(\\pi\\). So our bijection is \\[\\begin{equation} f(x) = \\frac{\\tan^{-1}(x) + \\frac{\\pi}{2}}{\\pi}. \\end{equation}\\] Exercise 2.8 Take the measure space \\((\\Omega_1 = (0,1], B_{(0,1], \\lambda})\\) (we know that this is a probability space on \\((0,1]\\)). Define a map (function) from \\(\\Omega_1\\) to \\(\\Omega_2 = \\{1,2,3,4,5,6\\}\\) such that the measure space \\((\\Omega_2, 2^{\\Omega_2}, \\lambda(f^{-1}()))\\) will be a discrete probability space with uniform probabilities (\\(P(\\omega) = \\frac{1}{6}, \\forall \\omega \\in \\Omega_2)\\). Is the map that you defined in (a) the only such map? How would you in the same fashion define a map that would result in a probability space that can be interpreted as a coin toss with probability \\(p\\) of heads? R: Use the map in (a) as a basis for a random generator for this fair die. Solution. In other words, we have to assign disjunct intervals of the same size to each element of \\(\\Omega_2\\). Therefore \\[\\begin{equation} f(x) = \\lceil 6x \\rceil. \\end{equation}\\] No, we could for example rearrange the order in which the intervals are mapped to integers. Additionally, we could have several disjoint intervals that mapped to the same integer, as long as the Lebesgue measure of their union would be \\(\\frac{1}{6}\\) and the function would remain injective. We have \\(\\Omega_3 = \\{0,1\\}\\), where zero represents heads and one represents tails. Then \\[\\begin{equation} f(x) = 0^{I_{A}(x)}, \\end{equation}\\] where \\(A = \\{y \\in (0,1] : y &lt; p\\}\\). set.seed(1) unif_s &lt;- runif(1000) die_s &lt;- ceiling(6 * unif_s) summary(as.factor(die_s)) ## 1 2 3 4 5 6 ## 166 154 200 146 166 168 "],
["condprob.html", "Chapter 3 Conditional probability 3.1 Calculating conditional probabilities 3.2 Conditional independence 3.3 Monty Hall problem", " Chapter 3 Conditional probability This chapter deals with conditional probability. The students are expected to acquire the following knowledge: Theoretical Identify whether variables are independent. Calculation of conditional probabilities. Understanding of conditional dependence and independence. How to apply Bayes’ theorem to solve difficult probabilistic questions. R Simulating conditional probabilities. cumsum. apply. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 3.1 Calculating conditional probabilities Exercise 3.1 A military officer is in charge of identifying enemy aircraft and shooting them down. He is able to positively identify an enemy airplane 95% of the time and positively identify a friendly airplane 90% of the time. Furthermore, 99% of the airplanes are friendly. When the officer identifies an airplane as an enemy airplane, what is the probability that it is not and they will shoot at a friendly airplane? Solution. Let \\(E = 0\\) denote that the observed plane is friendly and \\(E=1\\) that it is an enemy. Let \\(I = 0\\) denote that the officer identified it as friendly and \\(I = 1\\) as enemy. Then \\[\\begin{align} P(E = 0 | I = 1) &amp;= \\frac{P(I = 1 | E = 0)P(E = 0)}{P(I = 1)} \\\\ &amp;= \\frac{P(I = 1 | E = 0)P(E = 0)}{P(I = 1 | E = 0)P(E = 0) + P(I = 1 | E = 1)P(E = 1)} \\\\ &amp;= \\frac{0.1 \\times 0.99}{0.1 \\times 0.99 + 0.95 \\times 0.01} \\\\ &amp;= 0.91. \\end{align}\\] Exercise 3.2 R: Consider tossing a fair die. Let \\(A = \\{2,4,6\\}\\) and \\(B = \\{1,2,3,4\\}\\). Then \\(P(A) = \\frac{1}{2}\\), \\(P(B) = \\frac{2}{3}\\) and \\(P(AB) = \\frac{1}{3}\\). Since \\(P(AB) = P(A)P(B)\\), the events \\(A\\) and \\(B\\) are independent. Simulate draws from the sample space and verify that the proportions are the same. Then find two events \\(C\\) and \\(D\\) that are not independent and repeat the simulation. set.seed(1) nsamps &lt;- 10000 tosses &lt;- sample(1:6, nsamps, replace = TRUE) PA &lt;- sum(tosses %in% c(2,4,6)) / nsamps PB &lt;- sum(tosses %in% c(1,2,3,4)) / nsamps PA * PB ## [1] 0.3283998 sum(tosses %in% c(2,4)) / nsamps ## [1] 0.3217 # Let C = {1,2} and D = {2,3} PC &lt;- sum(tosses %in% c(1,2)) / nsamps PD &lt;- sum(tosses %in% c(2,3)) / nsamps PC * PD ## [1] 0.1114867 sum(tosses %in% c(2)) / nsamps ## [1] 0.1631 Exercise 3.3 A machine reports the true value of a thrown 12-sided die 5 out of 6 times. If the machine reports a 1 has been tossed, what is the probability that it is actually a 1? Now let the machine only report whether a 1 has been tossed or not. Does the probability change? R: Use simulation to check your answers to a) and b). Solution. Let \\(T = 1\\) denote that the toss is 1 and \\(M = 1\\) that the machine reports a 1. \\[\\begin{align} P(T = 1 | M = 1) &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{P(M = 1)} \\\\ &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{\\sum_{k=1}^{12} P(M = 1 | T = k)P(T = k)} \\\\ &amp;= \\frac{\\frac{5}{6}\\frac{1}{12}}{\\frac{5}{6}\\frac{1}{12} + 11 \\frac{1}{6} \\frac{1}{11} \\frac{1}{12}} \\\\ &amp;= \\frac{5}{6}. \\end{align}\\] Yes. \\[\\begin{align} P(T = 1 | M = 1) &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{P(M = 1)} \\\\ &amp;= \\frac{P(M = 1 | T = 1)P(T = 1)}{\\sum_{k=1}^{12} P(M = 1 | T = k)P(T = k)} \\\\ &amp;= \\frac{\\frac{5}{6}\\frac{1}{12}}{\\frac{5}{6}\\frac{1}{12} + 11 \\frac{1}{6} \\frac{1}{12}} \\\\ &amp;= \\frac{5}{16}. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 report_a &lt;- vector(mode = &quot;numeric&quot;, length = nsamps) report_b &lt;- vector(mode = &quot;logical&quot;, length = nsamps) truths &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:10000) { toss &lt;- sample(1:12, size = 1) truth &lt;- sample(c(TRUE, FALSE), size = 1, prob = c(5/6, 1/6)) truths[i] &lt;- truth if (truth) { report_a[i] &lt;- toss report_b[i] &lt;- toss == 1 } else { remaining &lt;- (1:12)[1:12 != toss] report_a[i] &lt;- sample(remaining, size = 1) report_b[i] &lt;- toss != 1 } } truth_a1 &lt;- truths[report_a == 1] sum(truth_a1) / length(truth_a1) ## [1] 0.8384075 truth_b1 &lt;- truths[report_b] sum(truth_b1) / length(truth_b1) ## [1] 0.3110339 Exercise 3.4 A coin is tossed independently \\(n\\) times. The probability of heads at each toss is \\(p\\). At each time \\(k\\), \\((k = 2,3,...,n)\\) we get a reward at time \\(k+1\\) if \\(k\\)-th toss was a head and the previous toss was a tail. Let \\(A_k\\) be the evebt that a reward is obtained at time \\(k\\). Are events \\(A_k\\) and \\(A_{k+1}\\) independent? Are events \\(A_k\\) and \\(A_{k+2}\\) independent? R: simulate 10 tosses 10000 times, where \\(p = 0.7\\). Check your answers to a) and b) by counting the frequencies of the events \\(A_5\\), \\(A_6\\), and \\(A_7\\). Solution. For \\(A_k\\) to happen, we need the tosses \\(k-2\\) and \\(k-1\\) be tails and heads respectively. For \\(A_{k+1}\\) to happen, we need tosses \\(k-1\\) and \\(k\\) be tails and heads respectively. As the toss \\(k-1\\) need to be heads for one and tails for the other, these two events can not happen simultaneously. Therefore the probability of their intersection is 0. But the probability of each of them separately is \\(p(1-p) &gt; 0\\). Therefore, they are not independent. For \\(A_k\\) to happen, we need the tosses \\(k-2\\) and \\(k-1\\) be tails and heads respectively. For \\(A_{k+2}\\) to happen, we need tosses \\(k\\) and \\(k+1\\) be tails and heads respectively. So the probability of intersection is \\(p^2(1-p)^2\\). And the probability of each separately is again \\(p(1-p)\\). Therefore, they are independent. set.seed(1) nsamps &lt;- 10000 p &lt;- 0.7 rewardA_5 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_6 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_7 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_56 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) rewardA_57 &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:nsamps) { samps &lt;- sample(c(0,1), size = 10, replace = TRUE, prob = c(0.7, 0.3)) rewardA_5[i] &lt;- (samps[4] == 0 &amp; samps[3] == 1) rewardA_6[i] &lt;- (samps[5] == 0 &amp; samps[4] == 1) rewardA_7[i] &lt;- (samps[6] == 0 &amp; samps[5] == 1) rewardA_56[i] &lt;- (rewardA_5[i] &amp; rewardA_6[i]) rewardA_57[i] &lt;- (rewardA_5[i] &amp; rewardA_7[i]) } sum(rewardA_5) / nsamps ## [1] 0.2141 sum(rewardA_6) / nsamps ## [1] 0.2122 sum(rewardA_7) / nsamps ## [1] 0.2107 sum(rewardA_56) / nsamps ## [1] 0 sum(rewardA_57) / nsamps ## [1] 0.0454 Exercise 3.5 A drawer contains two coins. One is an unbiased coin, the other is a biased coin, which will turn up heads with probability \\(p\\) and tails with probability \\(1-p\\). One coin is selected uniformly at random. The selected coin is tossed \\(n\\) times. The coin turns up heads \\(k\\) times and tails \\(n-k\\) times. What is the probability that the coin is biased? The selected coin is tossed repeatedly until it turns up heads \\(k\\) times. Given that it is tossed \\(n\\) times in total, what is the probability that the coin is biased? Solution. Let \\(B = 1\\) denote that the coin is biased and let \\(H = k\\) denote that we’ve seen \\(k\\) heads. \\[\\begin{align} P(B = 1 | H = k) &amp;= \\frac{P(H = k | B = 1)P(B = 1)}{P(H = k)} \\\\ &amp;= \\frac{P(H = k | B = 1)P(B = 1)}{P(H = k | B = 1)P(B = 1) + P(H = k | B = 0)P(B = 0)} \\\\ &amp;= \\frac{p^k(1-p)^{n-k} 0.5}{p^k(1-p)^{n-k} 0.5 + 0.5^{n+1}} \\\\ &amp;= \\frac{p^k(1-p)^{n-k}}{p^k(1-p)^{n-k} + 0.5^n}. \\end{align}\\] The same results as in a). The only difference between these two scenarios is that in b) the last throw must be heads. However, this holds for the biased and the unbiased coin and therefore does not affect the probability of the coin being biased. Exercise 3.6 Judy goes around the company for Women’s day and shares flowers. In every office she leaves a flower, if there is at least one woman inside. The probability that there’s a woman in the office is \\(\\frac{3}{5}\\). What is the probability that Judy leaves her first flower in the fourth office? Given that she has given away exactly three flowers in the first four offices, what is the probability that she gives her fourth flower in the eighth office? What is the probability that she leaves the second flower in the fifth office? What is the probability that she leaves the second flower in the fifth office, given that she did not leave the second flower in the second office? Judy needs a new supply of flowers immediately after the office, where she gives away her last flower. What is the probability that she visits at least five offices, if she starts with two flowers? R: simulate Judy’s walk 10000 times to check your answers a) - e). Solution. Let \\(X_i = k\\) denote the event that … \\(i\\)-th sample on the \\(k\\)-th run. Since the events are independent, we can multiply their probabilities to get \\[\\begin{equation} P(X_1 = 4) = 0.4^3 \\times 0.6 = 0.0384. \\end{equation}\\] Same as in a) as we have a fresh start after first four offices. For this to be possible, she had to leave the first flower in one of the first four offices. Therefore there are four possibilities, and for each of those the probability is \\(0.4^3 \\times 0.6\\). Additionally, the probability that she leaves a flower in the fifth office is \\(0.6\\). So \\[\\begin{equation} P(X_2 = 5) = \\binom{4}{1} \\times 0.4^3 \\times 0.6^2 = 0.09216. \\end{equation}\\] We use Bayes’ theorem. \\[\\begin{align} P(X_2 = 5 | X_2 \\neq 2) &amp;= \\frac{P(X_2 \\neq 2 | X_2 = 5)P(X_2 = 5)}{P(X_2 \\neq 2)} \\\\ &amp;= \\frac{0.09216}{0.64} \\\\ &amp;= 0.144. \\end{align}\\] The denominator in the second equation can be calculated as follows. One of three things has to happen for the second not to be dealt in the second round. First, both are zero, so \\(0.4^2\\). Second, first is zero, and second is one, so \\(0.4 \\times 0.6\\). Third, the first is one and the second one zero, so \\(0.6 \\times 0.4\\). Summing these values we get \\(0.64\\). We will look at the complement, so the events that she gave away exactly two flowers after two, three and four offices. \\[\\begin{equation} P(X_2 \\geq 5) = 1 - 0.6^2 - 2 \\times 0.4 \\times 0.6^2 - 3 \\times 0.4^2 \\times 0.6^2 = 0.1792. \\end{equation}\\] The multiplying parts represent the possibilities of the first flower. set.seed(1) nsamps &lt;- 100000 Judyswalks &lt;- matrix(data = NA, nrow = nsamps, ncol = 8) for (i in 1:nsamps) { thiswalk &lt;- sample(c(0,1), size = 8, replace = TRUE, prob = c(0.4, 0.6)) Judyswalks[i, ] &lt;- thiswalk } csJudy &lt;- t(apply(Judyswalks, 1, cumsum)) # a sum(csJudy[ ,4] == 1 &amp; csJudy[ ,3] == 0) / nsamps ## [1] 0.03848 # b csJsubset &lt;- csJudy[csJudy[ ,4] == 3 &amp; csJudy[ ,3] == 2, ] sum(csJsubset[ ,8] == 4 &amp; csJsubset[ ,7] == 3) / nrow(csJsubset) ## [1] 0.03665893 # c sum(csJudy[ ,5] == 2 &amp; csJudy[ ,4] == 1) / nsamps ## [1] 0.09117 # d sum(csJudy[ ,5] == 2 &amp; csJudy[ ,4] == 1) / sum(csJudy[ ,2] != 2) ## [1] 0.1422398 # e sum(csJudy[ ,4] &lt; 2) / nsamps ## [1] 0.17818 3.2 Conditional independence Exercise 3.7 Describe: A real-world example of two events \\(A\\) and \\(B\\) that are dependent but become conditionally independent if conditioned on a third event \\(C\\). A real-world example of two events \\(A\\) and \\(B\\) that are independent, but become dependent if conditioned on some third event \\(C\\). Solution. Let \\(A\\) be the height of a person and let \\(B\\) be the person’s knowledge of the Dutch language. These events are dependent since the Dutch are known to be taller than average. However if \\(C\\) is the nationality of the person, then \\(A\\) and \\(B\\) are independent given \\(C\\). Let \\(A\\) be the event that Mary passes the exam and let \\(B\\) be the event that John passes the exam. These events are independent. However, if the event \\(C\\) is that Mary and John studied together, then \\(A\\) and \\(B\\) are conditionally dependent given \\(C\\). Exercise 3.8 We have two coins of identical appearance. We know that one is a fair coin and the other flips heads 80% of the time. We choose one of the two coins uniformly at random. We discard the coin that was not chosen. We now flip the chosen coin independently 10 times, producing a sequence \\(Y_1 = y_1\\), \\(Y_2 = y_2\\), …, \\(Y_{10} = y_{10}\\). Intuitively, without doing and computation, are these random variables independent? Compute the probability \\(P(Y_1 = 1)\\). Compute the probabilities \\(P(Y_2 = 1 | Y_1 = 1)\\) and P(Y_{10} = 1 | Y_1 = 1,…,Y_9 = 1)$. Given your answers to b) and c), would you now change your answer to a)? If so, discuss why your intuition had failed. Solution. \\(P(Y_1 = 1) = 0.5 * 0.8 + 0.5 * 0.5 = 0.65\\). Since we know that \\(Y_1 = 1\\) this should change our view of the probability of the coin being biased or not. Let \\(B = 1\\) denote the event that the coin is biased and let \\(B = 0\\) denote that the coin is unbiased. By using marginal probability, we can write \\[\\begin{align} P(Y_2 = 1 | Y_1 = 1) &amp;= P(Y_2 = 1, B = 1 | Y_1 = 1) + P(Y_2 = 1, B = 0 | Y_1 = 1) \\\\ &amp;= \\sum_{k=1}^2 P(Y_2 = 1 | B = k, Y_1 = 1)P(B = k | Y_1 = 1) \\\\ &amp;= 0.8 \\frac{P(Y_1 = 1 | B = 1)P(B = 1)}{P(Y_1 = 1)} + 0.5 \\frac{P(Y_1 = 1 | B = 0)P(B = 0)}{P(Y_1 = 1)} \\\\ &amp;= 0.8 \\frac{0.8 \\times 0.5}{0.65} + 0.5 \\frac{0.5 \\times 0.5}{0.65} \\\\ &amp;\\approx 0.68. \\end{align}\\] For the other calculation we follow the same procedure. Let \\(X = 1\\) denote that first nine tosses are all heads (equivalent to \\(Y_1 = 1\\),…, \\(Y_9 = 1\\)). \\[\\begin{align} P(Y_{10} = 1 | X = 1) &amp;= P(Y_2 = 1, B = 1 | X = 1) + P(Y_2 = 1, B = 0 | X = 1) \\\\ &amp;= \\sum_{k=1}^2 P(Y_2 = 1 | B = k, X = 1)P(B = k | X = 1) \\\\ &amp;= 0.8 \\frac{P(X = 1 | B = 1)P(B = 1)}{P(X = 1)} + 0.5 \\frac{P(X = 1 | B = 0)P(B = 0)}{P(X = 1)} \\\\ &amp;= 0.8 \\frac{0.8^9 \\times 0.5}{0.5 \\times 0.8^9 + 0.5 \\times 0.5^9} + 0.5 \\frac{0.5^9 \\times 0.5}{0.5 \\times 0.8^9 + 0.5 \\times 0.5^9} \\\\ &amp;\\approx 0.8. \\end{align}\\] 3.3 Monty Hall problem The Monty Hall problem is a famous probability puzzle with non-intuitive outcome. Many established mathematicians and statisticians had problems solving it and many even disregarded the correct solution until they’ve seen the proof by simulation. Here we will show how it can be solved relatively simply with the use of Bayes’ theorem if we select the variables in a smart way. Exercise 3.9 (Monty Hall problem) A prize is placed at random behind one of three doors. You pick a door. Now Monty Hall chooses one of the other two doors, opens it and shows you that it is empty. He then gives you the opportunity to keep your door or switch to the other unopened door. Should you stay or switch? Use Bayes’ theorem to calculate the probability of winning if you switch and if you do not. R: Check your answers in R. Solution. W.L.O.G. assume we always pick the first door. The host can only open door 2 or door 3, as he can not open the door we picked. Let \\(k \\in \\{2,3\\}\\). Let us first look at what happens if we do not change. Then we have \\[\\begin{align} P(\\text{car in 1} | \\text{open $k$}) &amp;= \\frac{P(\\text{open $k$} | \\text{car in 1})P(\\text{car in 1})}{P(\\text{open $k$})} \\\\ &amp;= \\frac{P(\\text{open $k$} | \\text{car in 1})P(\\text{car in 1})}{\\sum_{n=1}^3 P(\\text{open $k$} | \\text{car in $n$})P(\\text{car in $n$)}}. \\end{align}\\] The probability that he opened \\(k\\) if the car is in 1 is \\(\\frac{1}{2}\\), as he can choose between door 2 and 3 as both have a goat behind it. Let us look at the normalization constant. When \\(n = 1\\) we get the value in the nominator. When \\(n=k\\), we get 0, as he will not open the door if there’s a prize behind. The remaining option is that we select 1, the car is behind \\(k\\) and he opens the only door left. Since he can’t open 1 due to it being our pick and \\(k\\) due to having the prize, the probability of opening the remaining door is 1, and the prior probability of the car being behind this door is \\(\\frac{1}{3}\\). So we have \\[\\begin{align} P(\\text{car in 1} | \\text{open $k$}) &amp;= \\frac{\\frac{1}{2}\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} \\\\ &amp;= \\frac{1}{3}. \\end{align}\\] Now let us look at what happens if we do change. Let \\(k&#39; \\in \\{2,3\\}\\) be the door that is not opened. If we change, we select this door, so we have \\[\\begin{align} P(\\text{car in $k&#39;$} | \\text{open $k$}) &amp;= \\frac{P(\\text{open $k$} | \\text{car in $k&#39;$})P(\\text{car in $k&#39;$})}{P(\\text{open $k$})} \\\\ &amp;= \\frac{P(\\text{open $k$} | \\text{car in $k&#39;$})P(\\text{car in $k&#39;$})}{\\sum_{n=1}^3 P(\\text{open $k$} | \\text{car in $n$})P(\\text{car in $n$)}}. \\end{align}\\] The denominator stays the same, the only thing that is different from before is \\(P(\\text{open $k$} | \\text{car in $k&#39;$})\\). We have a situation where we initially selected door 1 and the car is in door \\(k&#39;\\). The probability that the host will open door \\(k\\) is then 1, as he can not pick any other door. So we have \\[\\begin{align} P(\\text{car in $k&#39;$} | \\text{open $k$}) &amp;= \\frac{\\frac{1}{3}}{\\frac{1}{2}\\frac{1}{3} + \\frac{1}{3}} \\\\ &amp;= \\frac{2}{3}. \\end{align}\\] Therefore it makes sense to change the door. set.seed(1) nsamps &lt;- 1000 ifchange &lt;- vector(mode = &quot;logical&quot;, length = nsamps) ifstay &lt;- vector(mode = &quot;logical&quot;, length = nsamps) for (i in 1:nsamps) { where_car &lt;- sample(c(1:3), 1) where_player &lt;- sample(c(1:3), 1) open_samp &lt;- (1:3)[where_car != (1:3) &amp; where_player != (1:3)] if (length(open_samp) == 1) { where_open &lt;- open_samp } else { where_open &lt;- sample(open_samp, 1) } ifstay[i] &lt;- where_car == where_player where_ifchange &lt;- (1:3)[where_open != (1:3) &amp; where_player != (1:3)] ifchange[i] &lt;- where_ifchange == where_car } sum(ifstay) / nsamps ## [1] 0.333 sum(ifchange) / nsamps ## [1] 0.667 "],
["rvs.html", "Chapter 4 Random variables 4.1 General properties and calculations 4.2 Discrete random variables 4.3 Continuous random variables 4.4 Singular random variables 4.5 Transformations", " Chapter 4 Random variables This chapter deals with random variables and their distributions. The students are expected to acquire the following knowledge: Theoretical Identification of random variables. Convolutions of random variables. Derivation of PDF, PMF, CDF, and quantile function. Definitions and properties of common discrete random variables. Definitions and properties of common continuous random variables. Transforming univariate random variables. R Familiarize with PDF, PMF, CDF, and quantile functions for several distributions. Visual inspection of probability distributions. Analytical and empirical calculation of probabilities based on distributions. New R functions for plotting (for example, facet_wrap). Creating random number generators based on the Uniform distribution. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 4.1 General properties and calculations Exercise 4.1 Which of the functions below are valid CDFs? Find their respective densities. R: Plot the three functions. \\[\\begin{equation} F(x) = \\begin{cases} 1 - e^{-x^2} &amp; x \\geq 0 \\\\ 0 &amp; x &lt; 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} e^{-\\frac{1}{x}} &amp; x &gt; 0 \\\\ 0 &amp; x \\leq 0. \\end{cases} \\end{equation}\\] \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x \\leq 0 \\\\ \\frac{1}{3} &amp; 0 &lt; x \\leq \\frac{1}{2} \\\\ 1 &amp; x &gt; \\frac{1}{2}. \\end{cases} \\end{equation}\\] Solution. Yes. First, let us check the limits. \\(\\lim_{x \\rightarrow -\\infty} (0) = 0\\). \\(\\lim_{x \\rightarrow \\infty} (1 - e^{-x^2}) = 1 - \\lim_{x \\rightarrow \\infty} e^{-x^2} = 1 - 0 = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(1 - e^{-x^2} \\geq 1 - e^{-y^2}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} 1 - e^{-\\epsilon^2} = 1 - \\lim_{\\epsilon \\downarrow 0} e^{-\\epsilon^2} = 1 - 1 = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} 1 - e^{-x^2} = 2xe^{-x^2}.\\) Students are encouraged to check that this is a proper PDF. Yes. First, let us check the limits. $_{x -} (0) = 0 and \\(\\lim_{x \\rightarrow \\infty} (e^{-\\frac{1}{x}}) = 1\\). Second, let us check whether the function is increasing. Let \\(x &gt; y \\geq 0\\). Then \\(e^{-\\frac{1}{x}} \\geq e^{-\\frac{1}{y}}\\). We only have to check right continuity for the point zero. \\(F(0) = 0\\) and \\(\\lim_{\\epsilon \\downarrow 0}F (0 + \\epsilon) = \\lim_{\\epsilon \\downarrow 0} e^{-\\frac{1}{\\epsilon}} = 0\\). We get the density by differentiating the CDF. \\(p(x) = \\frac{d}{dx} e^{-\\frac{1}{x}} = \\frac{1}{x^2}e^{-\\frac{1}{x}}.\\) Students are encouraged to check that this is a proper PDF. No. The function is not right continuous as \\(F(\\frac{1}{2}) = \\frac{1}{3}\\), but \\(\\lim_{\\epsilon \\downarrow 0} F(\\frac{1}{2} + \\epsilon) = 1\\). f1 &lt;- function (x) { tmp &lt;- 1 - exp(-x^2) tmp[x &lt; 0] &lt;- 0 return(tmp) } f2 &lt;- function (x) { tmp &lt;- exp(-(1 / x)) tmp[x &lt;= 0] &lt;- 0 return(tmp) } f3 &lt;- function (x) { tmp &lt;- x tmp[x == x] &lt;- 1 tmp[x &lt;= 0.5] &lt;- 1/3 tmp[x &lt;= 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 20, by = 0.001), f1 = f1(x), f2 = f2(x), f3 = f3(x)) %&gt;% melt(id.vars = &quot;x&quot;) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = value, color = variable)) + geom_hline(yintercept = 1) + geom_line() plot(cdf_plot) Exercise 4.2 Let \\(X\\) be a random variable with CDF \\[\\begin{equation} F(x) = \\begin{cases} 0 &amp; x &lt; 0 \\\\ \\frac{x^2}{2} &amp; 0 \\leq x &lt; 1 \\\\ \\frac{1}{2} + \\frac{p}{2} &amp; 1 \\leq x &lt; 2 \\\\ \\frac{1}{2} + \\frac{p}{2} + \\frac{1 - p}{2} &amp; x \\geq 2 \\end{cases} \\end{equation}\\] R: Plot this CDF for \\(p = 0.3\\). Is it a discrete, continuous, or mixed random varible? Find the probability density/mass of \\(X\\). f1 &lt;- function (x, p) { tmp &lt;- x tmp[x &gt;= 2] &lt;- 0.5 + (p * 0.5) + ((1-p) * 0.5) tmp[x &lt; 2] &lt;- 0.5 + (p * 0.5) tmp[x &lt; 1] &lt;- (x[x &lt; 1])^2 / 2 tmp[x &lt; 0] &lt;- 0 return(tmp) } cdf_data &lt;- tibble(x = seq(-1, 5, by = 0.001), y = f1(x, 0.3)) cdf_plot &lt;- ggplot(data = cdf_data, aes(x = x, y = y)) + geom_hline(yintercept = 1) + geom_line(color = &quot;blue&quot;) plot(cdf_plot) Solution. \\(X\\) is a mixed random variable. Since \\(X\\) is a mixed random variable, we have to find the PDF of the continuous part and the PMF of the discrete part. We get the continuous part by differentiating the corresponding CDF, \\(\\frac{d}{dx}\\frac{x^2}{2} = x\\). So the PDF, when \\(0 \\leq x &lt; 1\\), is \\(p(x) = x\\). Let us look at the discrete part now. It has two steps, so this is a discrete distribution with two outcomes – numbers 1 and 2. The first happens with probability \\(\\frac{p}{2}\\), and the second with probability \\(\\frac{1 - p}{2}\\). This reminds us of the Bernoulli distribution. The PMF for the discrete part is \\(P(X = x) = (\\frac{p}{2})^{2 - x} (\\frac{1 - p}{2})^{x - 1}\\). Exercise 4.3 (Convolutions) Convolutions are probability distributions that correspond to sums of independent random variables. Let \\(X\\) and \\(Y\\) be independent discrete variables. Find the PMF of \\(Z = X + Y\\). Hint: Use the law of total probability. Let \\(X\\) and \\(Y\\) be independent continuous variables. Find the PDF of \\(Z = X + Y\\). Hint: Start with the CDF. Solution. \\[\\begin{align} P(Z = z) &amp;= P(X + Y = z) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + Y = z | Y = k) P(Y = k) &amp; \\text{ (law of total probability)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z | Y = k) P(Y = k) &amp; \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X + k = z) P(Y = k) &amp; \\text{ (independence of $X$ and $Y$)} \\\\ &amp;= \\sum_{k = -\\infty}^\\infty P(X = z - k) P(Y = k). &amp; \\end{align}\\] Let \\(f\\) and \\(g\\) be the PDFs of \\(X\\) and \\(Y\\) respectively. \\[\\begin{align} F(z) &amp;= P(Z &lt; z) \\\\ &amp;= P(X + Y &lt; z) \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + Y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z | Y = y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X + y &lt; z)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} P(X &lt; z - y)P(Y = y)dy \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy \\end{align}\\] Now \\[\\begin{align} p(z) &amp;= \\frac{d}{dz} F(z) &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} (\\frac{d}{dz}\\int_{-\\infty}^{z - y} f(x) dx) g(y) dy &amp; \\\\ &amp;= \\int_{-\\infty}^{\\infty} f(z - y) g(y) dy &amp; \\text{ (fundamental theorem of calculus)}. \\end{align}\\] 4.2 Discrete random variables Exercise 4.4 (Binomial random variable) Let \\(X_k\\), \\(k = 1,...,n\\), be random variables with the Bernoulli measure as the PMF. Let \\(X = \\sum_{k=1}^n X_k\\). We call \\(X_k\\) a Bernoulli random variable with parameter \\(p \\in (0,1)\\). Find the CDF of \\(X_k\\). Find PMF of \\(X\\). This is a Binomial random variable with support in \\(\\{0,1,2,...,n\\}\\) and parameters \\(p \\in (0,1)\\) and \\(n \\in \\mathbb{N}_0\\). We denote \\[\\begin{equation} X | n,p \\sim \\text{binomial}(n,p). \\end{equation}\\] Find CDF of \\(X\\). R: Simulate from the binomial distribution with \\(n = 10\\) and \\(p = 0.5\\), and from \\(n\\) Bernoulli distributions with \\(p = 0.5\\). Visually compare the sum of Bernoullis and the binomial. Hint: there is no standard function like rpois for a Bernoulli random variable. Check exercise 1.11 to find out how to sample from a Bernoulli distribution. Solution. There are two outcomes – zero and one. Zero happens with probability \\(1 - p\\). Therefore \\[\\begin{equation} F(k) = \\begin{cases} 0 &amp; k &lt; 0 \\\\ 1 - p &amp; 0 \\leq k &lt; 1 \\\\ 1 &amp; k \\geq 1. \\end{cases} \\end{equation}\\] For the probability of \\(X\\) to be equal to some \\(k \\leq n\\), exactly \\(k\\) Bernoulli variables need to be one, and the others zero. So \\(p^k(1-p)^{n-k}\\). There are \\(\\binom{n}{k}\\) such possible arrangements. Therefore \\[\\begin{align} P(X = k) = \\binom{n}{k} p^k (1 - p)^{n-k}. \\end{align}\\] \\[\\begin{equation} F(k) = \\sum_{i = 0}^{\\lfloor k \\rfloor} \\binom{n}{i} p^i (1 - p)^{n - i} \\end{equation}\\] set.seed(1) nsamps &lt;- 10000 binom_samp &lt;- rbinom(nsamps, size = 10, prob = 0.5) bernoulli_mat &lt;- matrix(data = NA, nrow = nsamps, ncol = 10) for (i in 1:nsamps) { bernoulli_mat[i, ] &lt;- rbinom(10, size = 1, prob = 0.5) } bern_samp &lt;- apply(bernoulli_mat, 1, sum) b_data &lt;- tibble(x = c(binom_samp, bern_samp), type = c(rep(&quot;binomial&quot;, 10000), rep(&quot;Bernoulli_sum&quot;, 10000))) b_plot &lt;- ggplot(data = b_data, aes(x = x, fill = type)) + geom_bar(position = &quot;dodge&quot;) plot(b_plot) Exercise 4.5 (Geometric random variable) A variable with PMF \\[\\begin{equation} P(k) = p(1-p)^k \\end{equation}\\] is a geometric random variable with support in non-negative integers. It has one parameter \\(p \\in (0,1]\\). We denote \\[\\begin{equation} X | p \\sim \\text{geometric}(p) \\end{equation}\\] Derive the CDF of a geometric random variable. R: Draw 1000 samples from the geometric distribution with \\(p = 0.3\\) and compare their frequencies to theoretical values. Solution. \\[\\begin{align} P(X \\leq k) &amp;= \\sum_{i = 0}^k p(1-p)^i \\\\ &amp;= p \\sum_{i = 0}^k (1-p)^i \\\\ &amp;= p \\frac{1 - (1-p)^{k+1}}{1 - (1 - p)} \\\\ &amp;= 1 - (1-p)^{k + 1} \\end{align}\\] set.seed(1) geo_samp &lt;- rgeom(n = 1000, prob = 0.3) geo_samp &lt;- data.frame(x = geo_samp) %&gt;% count(x) %&gt;% mutate(n = n / 1000, type = &quot;empirical_frequencies&quot;) %&gt;% bind_rows(data.frame(x = 0:20, n = dgeom(0:20, prob = 0.3), type = &quot;theoretical_measure&quot;)) geo_plot &lt;- ggplot(data = geo_samp, aes(x = x, y = n, fill = type)) + geom_bar(stat=&quot;identity&quot;, position = &quot;dodge&quot;) plot(geo_plot) Exercise 4.6 (Poisson random variable) A variable with PMF \\[\\begin{equation} P(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!} \\end{equation}\\] is a Poisson random variable with support in non-negative integers. It has one positive parameter \\(\\lambda\\), which also represents its mean value and variance (a measure of the deviation of the values from the mean – more on mean and variance in the next chapter). We denote \\[\\begin{equation} X | \\lambda \\sim \\text{Poisson}(\\lambda). \\end{equation}\\] This distribution is usually the default choice for modeling counts. We have already encountered a Poisson random variable in exercise 1.12, where we also sampled from this distribution. The CDF of a Poisson random variable is \\(P(X &lt;= x) = e^{-\\lambda} \\sum_{i=0}^x \\frac{\\lambda^{i}}{i!}\\). R: Draw 1000 samples from the Poisson distribution with \\(\\lambda = 5\\) and compare their empirical cumulative distribution function with the theoretical CDF. set.seed(1) pois_samp &lt;- rpois(n = 1000, lambda = 5) pois_samp &lt;- data.frame(x = pois_samp) pois_plot &lt;- ggplot(data = pois_samp, aes(x = x, colour = &quot;ECDF&quot;)) + stat_ecdf(geom = &quot;step&quot;) + geom_step(data = tibble(x = 0:17, y = ppois(x, 5)), aes(x = x, y = y, colour = &quot;CDF&quot;)) + scale_colour_manual(&quot;Lgend title&quot;, values = c(&quot;black&quot;, &quot;red&quot;)) plot(pois_plot) Exercise 4.7 (Negative binomial random variable) A variable with PMF \\[\\begin{equation} p(k) = \\binom{k + r - 1}{k}(1-p)^r p^k \\end{equation}\\] is a negative binomial random variable with support in non-negative integers. It has two parameters \\(r &gt; 0\\) and \\(p \\in (0,1)\\). We denote \\[\\begin{equation} X | r,p \\sim \\text{NB}(r,p). \\end{equation}\\] Let us reparameterize the negative binomial distribution with \\(q = 1 - p\\). Find the PMF of \\(X \\sim \\text{NB}(1, q)\\). Do you recognize this distribution? Show that the sum of two negative binomial random variables with the same \\(p\\) is also a negative binomial random variable. Hint: Use the fact that the number of ways to place \\(n\\) indistinct balls into \\(k\\) boxes is \\(\\binom{n + k - 1}{n}\\). R: Draw samples from \\(X \\sim \\text{NB}(5, 0.4)\\) and \\(Y \\sim \\text{NB}(3, 0.4)\\). Draw samples from \\(Z = X + Y\\), where you use the parameters calculated in b). Plot both distributions, their sum, and \\(Z\\) using facet_wrap. Be careful, as R uses a different parameterization size=\\(r\\) and prob=\\(1 - p\\). Solution. \\[\\begin{align} P(X = k) &amp;= \\binom{k + 1 - 1}{k}q^1 (1-q)^k \\\\ &amp;= q(1-q)^k. \\end{align}\\] This is the geometric distribution. Let \\(X \\sim \\text{NB}(r_1, p)\\) and \\(Y \\sim \\text{NB}(r_2, p)\\). Let \\(Z = X + Y\\). \\[\\begin{align} P(Z = z) &amp;= \\sum_{k = 0}^{\\infty} P(X = z - k)P(Y = k), \\text{ if k &lt; 0, then the probabilities are 0} \\\\ &amp;= \\sum_{k = 0}^{z} P(X = z - k)P(Y = k), \\text{ if k &gt; z, then the probabilities are 0} \\\\ &amp;= \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k}(1 - p)^{r_1} p^{z - k} \\binom{k + r_2 - 1}{k}(1 - p)^{r_2} p^{k} &amp; \\\\ &amp;= \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}(1 - p)^{r_1 + r_2} p^{z} &amp; \\\\ &amp;= (1 - p)^{r_1 + r_2} p^{z} \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}&amp; \\end{align}\\] The part before the sum reminds us of the negative binomial distribution with parameters \\(r_1 + r_2\\) and \\(p\\). To complete this term to the negative binomial PMF we need \\(\\binom{z + r_1 + r_2 -1}{z}\\). So the only thing we need to prove is that the sum equals this term. Both terms in the sum can be interpreted as numbers of ways to place a number of balls into boxes. For the left term it is \\(z-k\\) balls into \\(r_1\\) boxes, and for the right \\(k\\) balls into \\(r_2\\) boxes. For each \\(k\\) we are distributing \\(z\\) balls in total. By summing over all \\(k\\), we actually get all the possible placements of \\(z\\) balls into \\(r_1 + r_2\\) boxes. Therefore \\[\\begin{align} P(Z = z) &amp;= (1 - p)^{r_1 + r_2} p^{z} \\sum_{k = 0}^{z} \\binom{z - k + r_1 - 1}{z - k} \\binom{k + r_2 - 1}{k}&amp; \\\\ &amp;= \\binom{z + r_1 + r_2 -1}{z} (1 - p)^{r_1 + r_2} p^{z}. \\end{align}\\] From this it also follows that the sum of geometric distributions with the same parameter is a negative binomial distribution. \\(Z \\sim \\text{NB}(8, 0.4)\\). set.seed(1) nsamps &lt;- 10000 x &lt;- rnbinom(nsamps, size = 5, prob = 0.6) y &lt;- rnbinom(nsamps, size = 3, prob = 0.6) xpy &lt;- x + y z &lt;- rnbinom(nsamps, size = 8, prob = 0.6) samps &lt;- tibble(x, y, xpy, z) samps &lt;- melt(samps) ggplot(data = samps, aes(x = value)) + geom_bar() + facet_wrap(~ variable) 4.3 Continuous random variables Exercise 4.8 (Exponential random variable) A variable \\(X\\) with PDF \\(\\lambda e^{-\\lambda x}\\) is an exponential random variable with support in non-negative real numbers. It has one positive parameter \\(\\lambda\\). We denote \\[\\begin{equation} X | \\lambda \\sim \\text{Exp}(\\lambda). \\end{equation}\\] Find the CDF of an exponential random variable. Find the quantile function of an exponential random variable. Calculate the probability \\(P(1 \\leq X \\leq 3)\\), where \\(X \\sim \\text{Exp(1.5)}\\). R: Check your answer to c) with a simulation (rexp). Plot the probability in a meaningful way. R: Implement PDF, CDF, and the quantile function and compare their values with corresponding R functions visually. Hint: use the size parameter to make one of the curves wider. Solution. \\[\\begin{align} F(x) &amp;= \\int_{0}^{x} \\lambda e^{-\\lambda t} dt \\\\ &amp;= \\lambda \\int_{0}^{x} e^{-\\lambda t} dt \\\\ &amp;= \\lambda (\\frac{1}{-\\lambda}e^{-\\lambda t} |_{0}^{x}) \\\\ &amp;= \\lambda(\\frac{1}{\\lambda} - \\frac{1}{\\lambda} e^{-\\lambda x}) \\\\ &amp;= 1 - e^{-\\lambda x}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(x)) &amp;= x \\\\ 1 - e^{-\\lambda F^{-1}(x)} &amp;= x \\\\ e^{-\\lambda F^{-1}(x)} &amp;= 1 - x \\\\ -\\lambda F^{-1}(x) &amp;= \\ln(1 - x) \\\\ F^{-1}(x) &amp;= - \\frac{ln(1 - x)}{\\lambda}. \\end{align}\\] \\[\\begin{align} P(1 \\leq X \\leq 3) &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= P(X \\leq 3) - P(X \\leq 1) \\\\ &amp;= 1 - e^{-1.5 \\times 3} - 1 + e^{-1.5 \\times 1} \\\\ &amp;\\approx 0.212. \\end{align}\\] set.seed(1) nsamps &lt;- 1000 samps &lt;- rexp(nsamps, rate = 1.5) sum(samps &gt;= 1 &amp; samps &lt;= 3) / nsamps ## [1] 0.212 exp_plot &lt;- ggplot(data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5)) + stat_function(fun = dexp, args = list(rate = 1.5), xlim = c(1,3), geom = &quot;area&quot;, fill = &quot;red&quot;) plot(exp_plot) exp_pdf &lt;- function(x, lambda) { return (lambda * exp(-lambda * x)) } exp_cdf &lt;- function(x, lambda) { return (1 - exp(-lambda * x)) } exp_quant &lt;- function(q, lambda) { return (-(log(1 - q) / lambda)) } ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = dexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_pdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 5, by = 0.01)), aes(x = x)) + stat_function(fun = pexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_cdf, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = qexp, args = list(rate = 1.5), aes(color = &quot;R&quot;), size = 2.5) + stat_function(fun = exp_quant, args = list(lambda = 1.5), aes(color = &quot;Mine&quot;), size = 1.2) + scale_color_manual(values = c(&quot;red&quot;, &quot;black&quot;)) Exercise 4.9 (Uniform random variable) Continuous uniform random variable with parameters \\(a\\) and \\(b\\) has the PDF \\[\\begin{equation} p(x) = \\begin{cases} \\frac{1}{b - a} &amp; x \\in [a,b] \\\\ 0 &amp; \\text{otherwise}. \\end{cases} \\end{equation}\\] Find the CDF of the uniform random variable. Find the quantile function of the uniform random variable. Let \\(X \\sim \\text{Uniform}(a,b)\\). Find the CDF of the variable \\(Y = \\frac{X - a}{b - a}\\). This is the standard uniform random variable. Let \\(X \\sim \\text{Uniform}(-1, 3)\\). Find such \\(z\\) that \\(P(X &lt; z + \\mu_x) = \\frac{1}{5}\\). R: Check your result from d) using simulation. Solution. a. \\[\\begin{align} F(x) &amp;= \\int_{a}^x \\frac{1}{b - a} dt \\\\ &amp;= \\frac{1}{b - a} \\int_{a}^x dt \\\\ &amp;= \\frac{x - a}{b - a}. \\end{align}\\] \\[\\begin{align} F(F^{-1}(p)) &amp;= p \\\\ \\frac{F^{-1}(p) - a}{b - a} &amp;= p \\\\ F^{-1}(p) &amp;= p(b - a) + a. \\end{align}\\] \\[\\begin{align} F_Y(y) &amp;= P(Y &lt; y) \\\\ &amp;= P(\\frac{X - a}{b - a} &lt; y) \\\\ &amp;= P(X &lt; y(b - a) + a) \\\\ &amp;= F_X(y(b - a) + a) \\\\ &amp;= \\frac{(y(b - a) + a) - a}{b - a} \\\\ &amp;= y. \\end{align}\\] \\[\\begin{align} P(X &lt; z + 1) &amp;= \\frac{1}{5} \\\\ F(z + 1) &amp;= \\frac{1}{5} \\\\ z + 1 &amp;= F^{-1}(\\frac{1}{5}) \\\\ z &amp;= \\frac{1}{5}4 - 1 - 1 \\\\ z &amp;= -1.2. \\end{align}\\] set.seed(1) a &lt;- -1 b &lt;- 3 nsamps &lt;- 10000 unif_samp &lt;- runif(nsamps, a, b) mu_x &lt;- mean(unif_samp) new_samp &lt;- unif_samp - mu_x quantile(new_samp, probs = 1/5) ## 20% ## -1.203192 punif(-0.2, -1, 3) ## [1] 0.2 Exercise 4.10 (Beta random variable) A variable \\(X\\) with PDF \\[\\begin{equation} p(x) = \\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}, \\end{equation}\\] where \\(\\text{B}(\\alpha, \\beta) = \\frac{\\Gamma(\\alpha) \\Gamma(\\beta)}{\\Gamma(\\alpha + \\beta)}\\) and \\(\\Gamma(x) = \\int_0^{\\infty} x^{z - 1} e^{-x} dx\\) is a Beta random variable with support on \\([0,1]\\). It has two positive parameters \\(\\alpha\\) and \\(\\beta\\). Notation: \\[\\begin{equation} X | \\alpha, \\beta \\sim \\text{Beta}(\\alpha, \\beta) \\end{equation}\\] It is often used in modeling rates. Calculate the PDF for \\(\\alpha = 1\\) and \\(\\beta = 1\\). What do you notice? R: Plot densities of the beta distribution for parameter pairs (2, 2), (4, 1), (1, 4), (2, 5), and (0.1, 0.1). R: Sample from \\(X \\sim \\text{Beta}(2, 5)\\) and compare the histogram with Beta PDF. Solution. \\[\\begin{equation} p(x) = \\frac{x^{1 - 1} (1 - x)^{1 - 1}}{\\text{B}(1, 1)} = 1. \\end{equation}\\] This is the standard uniform distribution. set.seed(1) ggplot(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), aes(color = &quot;alpha = 0.5&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 4, shape2 = 1), aes(color = &quot;alpha = 4&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 1, shape2 = 4), aes(color = &quot;alpha = 1&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 5), aes(color = &quot;alpha = 25&quot;)) + stat_function(fun = dbeta, args = list(shape1 = 0.1, shape2 = 0.1), aes(color = &quot;alpha = 0.1&quot;)) set.seed(1) nsamps &lt;- 1000 samps &lt;- rbeta(nsamps, 2, 5) ggplot(data = data.frame(x = samps), aes(x = x)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;) + stat_function(data = data.frame(x = seq(0, 1, by = 0.01)), aes(x = x), fun = dbeta, args = list(shape1 = 2, shape2 = 5), color = &quot;red&quot;, size = 1.2) Exercise 4.11 (Gamma random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\beta x} \\end{equation}\\] is a Gamma random variable with support on the positive numbers and parameters shape \\(\\alpha &gt; 0\\) and rate \\(\\beta &gt; 0\\). We write \\[\\begin{equation} X | \\alpha, \\beta \\sim \\text{Gamma}(\\alpha, \\beta) \\end{equation}\\] and it’s CDF is \\[\\begin{equation} \\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}, \\end{equation}\\] where \\(\\gamma(s, x) = \\int_0^x t^{s-1} e^{-t} dt\\). It is usually used in modeling positive phenomena (for example insurance claims and rainfalls). Let \\(X \\sim \\text{Gamma}(1, \\beta)\\). Find the PDF of \\(X\\). Do you recognize this PDF? Let \\(k = \\alpha\\) and \\(\\theta = \\frac{1}{\\beta}\\). Find the PDF of \\(X | k, \\theta \\sim \\text{Gamma}(k, \\theta)\\). Random variables can be reparameterized, and sometimes a reparameterized distribution is more suitable for certain calculations. The first parameterization is for example usually used in Bayesian statistics, while this parameterization is more common in econometrics and some other applied fields. Note that you also need to pay attention to the parameters in statistical software, so diligently read the help files when using functions like rgamma to see how the function is parameterized. R: Plot gamma CDF for random variables with shape and rate parameters (1,1), (10,1), (1,10). Solution. \\[\\begin{align} p(x) &amp;= \\frac{\\beta^1}{\\Gamma(1)} x^{1 - 1}e^{-\\beta x} \\\\ &amp;= \\beta e^{-\\beta x} \\end{align}\\] This is the PDF of the exponential distribution with parameter \\(\\beta\\). \\[\\begin{align} p(x) &amp;= \\frac{1}{\\Gamma(k)\\beta^k} x^{k - 1}e^{-\\frac{x}{\\theta}}. \\end{align}\\] set.seed(1) ggplot(data = data.frame(x = seq(0, 25, by = 0.01)), aes(x = x)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 1), aes(color = &quot;Gamma(1,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 10, rate = 1), aes(color = &quot;Gamma(10,1)&quot;)) + stat_function(fun = pgamma, args = list(shape = 1, rate = 10), aes(color = &quot;Gamma(1,10)&quot;)) Exercise 4.12 (Normal random variable) A random variable with PDF \\[\\begin{equation} p(x) = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(x - \\mu)^2}{2 \\sigma^2}} \\end{equation}\\] is a normal random variable with support on the real axis and parameters \\(\\mu\\) in reals and \\(\\sigma^2 &gt; 0\\). The first is the mean parameter and the second is the variance parameter. Many statistical methods assume a normal distribution. We denote \\[\\begin{equation} X | \\mu, \\sigma \\sim \\text{N}(\\mu, \\sigma^2), \\end{equation}\\] and it’s CDF is \\[\\begin{equation} F(x) = \\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(t - \\mu)^2}{2 \\sigma^2}} dt, \\end{equation}\\] which is intractable and is usually approximated. Due to its flexibility it is also one of the most researched distributions. For that reason statisticians often use transformations of variables or approximate distributions with the normal distribution. Show that a variable \\(\\frac{X - \\mu}{\\sigma} \\sim \\text{N}(0,1)\\). This transformation is called standardization, and \\(\\text{N}(0,1)\\) is a standard normal distribution. R: Plot the normal distribution with \\(\\mu = 0\\) and different values for the \\(\\sigma\\) parameter. R: The normal distribution provides a good approximation for the Poisson distribution with a large \\(\\lambda\\). Let \\(X \\sim \\text{Poisson}(50)\\). Approximate \\(X\\) with the normal distribution and compare its density with the Poisson histogram. What are the values of \\(\\mu\\) and \\(\\sigma^2\\) that should provide the best approximation? Note that R function rnorm takes standard deviation (\\(\\sigma\\)) as a parameter and not variance. Solution. \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= P(X &lt; \\sigma x + \\mu) \\\\ &amp;= F(\\sigma x + \\mu) \\\\ &amp;= \\int_{-\\infty}^{\\sigma x + \\mu} \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(t - \\mu)^2}{2\\sigma^2}} dt \\end{align}\\] Now let \\(s = f(t) = \\frac{t - \\mu}{\\sigma}\\), then \\(ds = \\frac{dt}{\\sigma}\\) and \\(f(\\sigma x + \\mu) = x\\), so \\[\\begin{align} P(\\frac{X - \\mu}{\\sigma} &lt; x) &amp;= \\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2 \\pi}} e^{-\\frac{s^2}{2}} ds. \\end{align}\\] There is no need to evaluate this integral, as we recognize it as the CDF of a normal distribution with \\(\\mu = 0\\) and \\(\\sigma^2 = 1\\). set.seed(1) # b ggplot(data = data.frame(x = seq(-15, 15, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = &quot;sd = 1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 0.4), aes(color = &quot;sd = 0.1&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 2), aes(color = &quot;sd = 2&quot;)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 5), aes(color = &quot;sd = 5&quot;)) # c mean_par &lt;- 50 nsamps &lt;- 100000 pois_samps &lt;- rpois(nsamps, lambda = mean_par) norm_samps &lt;- rnorm(nsamps, mean = mean_par, sd = sqrt(mean_par)) my_plot &lt;- ggplot() + geom_bar(data = tibble(x = pois_samps), aes(x = x, y = (..count..)/sum(..count..))) + geom_density(data = tibble(x = norm_samps), aes(x = x), color = &quot;red&quot;) plot(my_plot) Exercise 4.13 (Logistic random variable) A logistic random variable has CDF \\[\\begin{equation} F(x) = \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}}, \\end{equation}\\] where \\(\\mu\\) is real and \\(s &gt; 0\\). The support is on the real axis. We denote \\[\\begin{equation} X | \\mu, s \\sim \\text{Logistic}(\\mu, s). \\end{equation}\\] The distribution of the logistic random variable resembles a normal random variable, however it has heavier tails. Find the PDF of a logistic random variable. R: Implement logistic PDF and CDF and visually compare both for \\(X \\sim \\text{N}(0, 1)\\) and \\(Y \\sim \\text{logit}(0, \\sqrt{\\frac{3}{\\pi^2}})\\). These distributions have the same mean and variance. Additionally, plot the same plot on the interval [5,10], to better see the difference in the tails. R: For the distributions in b) find the probability \\(P(|X| &gt; 4)\\) and interpret the result. Solution. \\[\\begin{align} p(x) &amp;= \\frac{d}{dx} \\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}} \\\\ &amp;= \\frac{- \\frac{d}{dx} (1 + e^{-\\frac{x - \\mu}{s}})}{(1 + e^{-\\frac{x - \\mu}{s}})^2} \\\\ &amp;= \\frac{e^{-\\frac{x - \\mu}{s}}}{s(1 + e^{-\\frac{x - \\mu}{s}})^2}. \\end{align}\\] # b set.seed(1) logit_pdf &lt;- function (x, mu, s) { return ((exp(-(x - mu)/(s))) / (s * (1 + exp(-(x - mu)/(s)))^2)) } nl_plot &lt;- ggplot(data = data.frame(x = seq(-12, 12, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(3/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) nl_plot &lt;- ggplot(data = data.frame(x = seq(5, 10, by = 0.01)), aes(x = x)) + stat_function(fun = dnorm, args = list(mean = 0, sd = 1), aes(color = &quot;normal&quot;)) + stat_function(fun = logit_pdf, args = list(mu = 0, s = sqrt(3/pi^2)), aes(color = &quot;logit&quot;)) plot(nl_plot) # c logit_cdf &lt;- function (x, mu, s) { return (1 / (1 + exp(-(x - mu) / s))) } p_logistic &lt;- 1 - logit_cdf(4, 0, sqrt(3/pi^2)) + logit_cdf(-4, 0, sqrt(3/pi^2)) p_norm &lt;- 1 - pnorm(4, 0, 1) + pnorm(-4, 0, 1) p_logistic ## [1] 0.001411988 p_norm ## [1] 6.334248e-05 # Logistic distribution has wider tails, therefore the probability of larger # absolute values is higher. 4.4 Singular random variables Exercise 4.14 (Cantor distribution) The Cantor set is a subset of \\([0,1]\\), which we create by iteratively deleting the middle third of the interval. For example, in the first iteration, we get the sets \\([0,\\frac{1}{3}]\\) and \\([\\frac{2}{3},1]\\). In the second iteration, we get \\([0,\\frac{1}{9}]\\), \\([\\frac{2}{9},\\frac{1}{3}]\\), \\([\\frac{2}{3}, \\frac{7}{9}]\\), and \\([\\frac{8}{9}, 1]\\). On the \\(n\\)-th iteration, we have \\[\\begin{equation} C_n = \\frac{C_{n-1}}{3} \\cup \\bigg(\\frac{2}{3} + \\frac{C_{n-1}}{3} \\bigg), \\end{equation}\\] where \\(C_0 = [0,1]\\). The Cantor set is then defined as the intersection of these sets \\[\\begin{equation} C = \\cap_{n=1}^{\\infty} C_n. \\end{equation}\\] It has the same cardinality as \\([0,1]\\). Another way to define the Cantor set is the set of all numbers on \\([0,1]\\), that do not have a 1 in the ternary representation \\(x = \\sum_{n=1}^\\infty \\frac{x_i}{3^i}, x_i \\in \\{0,1,2\\}\\). A random variable follows the Cantor distribution, if its CDF is the Cantor function (below). You can find the implementations of random number generator, CDF, and quantile functions for the Cantor distributions at https://github.com/Henrygb/CantorDist.R. Show that the Lebesgue measure of the Cantor set is 0. (Jagannathan) Let us look at an infinite sequence of independent fair-coin tosses. If the outcome is heads, let \\(x_i = 2\\) and \\(x_i = 0\\), when tails. Then use these to create \\(x = \\sum_{n=1}^\\infty \\frac{x_i}{3^i}\\). This is a random variable with the Cantor distribution. Show that \\(X\\) has a singular distribution. Solution. \\[\\begin{align} \\lambda(C) &amp;= 1 - \\lambda(C^c) \\\\ &amp;= 1 - \\frac{1}{3}\\sum_{k = 0}^\\infty (\\frac{2}{3})^k \\\\ &amp;= 1 - \\frac{\\frac{1}{3}}{1 - \\frac{2}{3}} \\\\ &amp;= 0. \\end{align}\\] First, for every \\(x\\), the probability of observing it is \\(\\lim_{n \\rightarrow \\infty} \\frac{1}{2^n} = 0\\). Second, the probability that we observe one of all the possible sequences is 1. Therefore \\(P(C) = 1\\). So this is a singular variable. The CDF only increments on the elements of the Cantor set. 4.5 Transformations Exercise 4.15 Let \\(X\\) be a random variable that is uniformly distributed on \\(\\{-2, -1, 0, 1, 2\\}\\). Find the PMF of \\(Y = X^2\\). Solution. \\[\\begin{align} P_Y(y) = \\sum_{x \\in \\sqrt(y)} P_X(x) = \\begin{cases} 0 &amp; y \\notin \\{0,1,4\\} \\\\ \\frac{1}{5} &amp; y = 0 \\\\ \\frac{2}{5} &amp; y \\in \\{1,4\\} \\end{cases} \\end{align}\\] Exercise 4.16 (Lognormal random variable) A lognormal random variable is a variable whose logarithm is normally distributed. In practice, we often encounter skewed data. Usually using a log transformation on such data makes it more symmetric and therefore more suitable for modeling with the normal distribution (more on why we wish to model data with the normal distribution in the following chapters). Let \\(X \\sim \\text{N}(\\mu,\\sigma)\\). Find the PDF of \\(Y: \\log(Y) = X\\). R: Sample from the lognormal distribution with parameters \\(\\mu = 5\\) and \\(\\sigma = 2\\). Plot a histogram of the samples. Then log-transform the samples and plot a histogram along with the theoretical normal PDF. Solution. \\[\\begin{align} p_Y(y) &amp;= p_X(\\log(y)) \\frac{d}{dy} \\log(y) \\\\ &amp;= \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(\\log(y) - \\mu)^2}{2 \\sigma^2}} \\frac{1}{y} \\\\ &amp;= \\frac{1}{y \\sqrt{2 \\pi \\sigma^2}} e^{-\\frac{(\\log(y) - \\mu)^2}{2 \\sigma^2}}. \\end{align}\\] set.seed(1) nsamps &lt;- 10000 mu &lt;- 5 sigma &lt;- 2 ln_samps &lt;- rlnorm(nsamps, mu, sigma) ln_plot &lt;- ggplot(data = data.frame(x = ln_samps), aes(x = x)) + geom_histogram(color = &quot;black&quot;) plot(ln_plot) norm_samps &lt;- log(ln_samps) n_plot &lt;- ggplot(data = data.frame(x = norm_samps), aes(x = x)) + geom_histogram(aes(y = ..density..), color = &quot;black&quot;) + stat_function(fun = dnorm, args = list(mean = mu, sd = sigma), color = &quot;red&quot;) plot(n_plot) Exercise 4.17 (Probability integral transform) This exercise is borrowed from Wasserman. Let \\(X\\) have a continuous, strictly increasing CDF \\(F\\). Let \\(Y = F(X)\\). Find the density of \\(Y\\). This is called the probability integral transform. Let \\(U \\sim \\text{Uniform}(0,1)\\) and let \\(X = F^{-1}(U)\\). Show that \\(X \\sim F\\). R: Implement a program that takes Uniform(0,1) random variables and generates random variables from an exponential(\\(\\beta\\)) distribution. Compare your implemented function with function rexp in R. Solution. \\[\\begin{align} F_Y(y) &amp;= P(Y &lt; y) \\\\ &amp;= P(F(X) &lt; y) \\\\ &amp;= P(X &lt; F_X^{-1}(y)) \\\\ &amp;= F_X(F_X^{-1}(y)) \\\\ &amp;= y. \\end{align}\\] From the above it follows that \\(p(y) = 1\\). Note that we need to know the inverse CDF to be able to apply this procedure. \\[\\begin{align} P(X &lt; x) &amp;= P(F^{-1}(U) &lt; x) \\\\ &amp;= P(U &lt; F(x)) \\\\ &amp;= F_U(F(x)) \\\\ &amp;= F(x). \\end{align}\\] set.seed(1) nsamps &lt;- 10000 beta &lt;- 4 generate_exp &lt;- function (n, beta) { tmp &lt;- runif(n) X &lt;- qexp(tmp, beta) return (X) } df &lt;- tibble(&quot;R&quot; = rexp(nsamps, beta), &quot;myGenerator&quot; = generate_exp(nsamps, beta)) %&gt;% gather() ggplot(data = df, aes(x = value, fill = key)) + geom_histogram(position = &quot;dodge&quot;) "],
["mrvs.html", "Chapter 5 Multiple random variables 5.1 General", " Chapter 5 Multiple random variables This chapter deals with multiple random variables and their distributions. The students are expected to acquire the following knowledge: Theoretical Find CDF and PDF of multiple random variables. Find CDF and PDF of transformed multiple random variables. Find conditional and marginal distributions of multiple random variables. R Scatterplots of bivariate random variables. New R functions (for example, expand.grid). .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } 5.1 General Exercise 5.1 Let \\(X \\sim \\text{N}(0,1)\\) and \\(Y \\sim \\text{N}(0,1)\\) be independent random variables. Draw 1000 samples from \\((X,Y)\\) and plot a scatterplot. Now let \\(X \\sim \\text{N}(0,1)\\) and \\(Y | X = x \\sim \\text{N}(ax, 1)\\). Draw 1000 samples from \\((X,Y)\\) for \\(a = 1\\), \\(a=0\\), and \\(a=-0.5\\). Plot the scatterplots. How would you interpret parameter \\(a\\)? Plot the marginal distribution of \\(Y\\) for cases \\(a=1\\), \\(a=0\\), and \\(a=-0.5\\). Can you guess which distribution it is? "],
["integ.html", "Chapter 6 Integration 6.1 Monte Carlo integration 6.2 Lebesgue integrals", " Chapter 6 Integration This chapter deals with abstract and Monte Carlo integration. The students are expected to acquire the following knowledge: Theoretical How to calculate Lebesgue integrals for non-simple functions. R Monte Carlo integration. 6.1 Monte Carlo integration Exercise 6.1 Let \\(X\\) and \\(Y\\) be continuous random variables on the unit interval and \\(p(x,y) = 6(x - y)^2\\). Use Monte Carlo integration to estimate the probability \\(P(0.2 \\leq X \\leq 0.5, \\: 0.1 \\leq Y \\leq 0.2)\\). Can you find the exact value? 6.2 Lebesgue integrals Exercise 6.2 (from Jagannathan) Find the Lebesgue integral of the following functions on (\\(\\mathbb{R}\\), \\(\\mathcal{B}(\\mathbb{R})\\), \\(\\lambda\\)). \\[\\begin{align} f(\\omega) = \\begin{cases} \\omega, &amp; \\text{for } \\omega = 0,1,...,n \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] \\[\\begin{align} f(\\omega) = \\begin{cases} 1, &amp; \\text{for } \\omega = \\mathbb{Q}^c \\cap [0,1] \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] \\[\\begin{align} f(\\omega) = \\begin{cases} n, &amp; \\text{for } \\omega = \\mathbb{Q}^c \\cap [0,n] \\\\ 0, &amp; \\text{elsewhere} \\end{cases} \\end{align}\\] Exercise 6.3 (from Jagannathan) Let \\(c \\in \\mathbb{R}\\) be fixed and (\\(\\mathbb{R}\\), \\(\\mathcal{B}(\\mathbb{R})\\)) a measurable space. If for any Borel set \\(A\\), \\(\\delta_c (A) = 1\\) if \\(c \\in A\\), and \\(\\delta_c (A) = 0\\) otherwise, then \\(\\delta_c\\) is called a Dirac measure. Let \\(g\\) be a non-negative, measurable function. Show that \\(\\int g d \\delta_c = g(c)\\). "],
["A1.html", "A R programming language A.1 Basic characteristics A.2 Why R? A.3 Setting up A.4 R basics A.5 Functions A.6 Other tips A.7 Further reading and references", " A R programming language A.1 Basic characteristics R is free software for statistical computing and graphics. It is widely used by statisticians, scientists, and other professionals for software development and data analysis. It is an interpreted language and therefore the programs do not need compilation. A.2 Why R? R is one of the main two languages used for statistics and machine learning (the other being Python). Pros Libraries. Comprehensive collection of statistical and machine learning packages. Easy to code. Open source. Anyone can access R and develop new methods. Additionally, it is relatively simple to get source code of established methods. Large community. The use of R has been rising for some time, in industry and academia. Therefore a large collection of blogs and tutorials exists, along with people offering help on pages like StackExchange and CrossValidated. Integration with other languages and LaTeX. New methods. Many researchers develop R packages based on their research, therefore new methods are available soon after development. Cons Slow. Programs run slower than in other programming languages, however this can be somewhat ammended by effective coding or integration with other languages. Memory intensive. This can become a problem with large data sets, as they need to be stored in the memory, along with all the information the models produce. Some packages are not as good as they should be, or have poor documentation. Object oriented programming in R can be very confusing and complex. A.3 Setting up https://www.r-project.org/. A.3.1 RStudio RStudio is the most widely used IDE for R. It is free, you can download it from https://rstudio.com/. While console R is sufficient for the requirements of this course, we recommend the students install RStudio for its better user interface. A.3.2 Libraries for data science Listed below are some of the more useful libraries (packages) for data science. Students are also encouraged to find other useful packages. dplyr Efficient data manipulation. Part of the wider package collection called tidyverse. ggplot2 Plotting based on grammar of graphics. stats Several statistical models. rstan Bayesian inference using Hamiltonian Monte Carlo. Very flexible model building. MCMCpack Bayesian inference. rmarkdown, knitr, and bookdown Dynamic reports (for example such as this one). devtools Package development. A.4 R basics A.4.1 Variables and types Important information and tips: no type declaration define variables with &lt;- instead of = (although both work, there is a slight difference, additionally most of the packages use the arrow) for strings use \"\" for comments use # change types with as.type() functions no special type for single character like C++ for example n &lt;- 20 x &lt;- 2.7 m &lt;- n # m gets value 20 my_flag &lt;- TRUE student_name &lt;- &quot;Luka&quot; typeof(n) ## [1] &quot;double&quot; typeof(student_name) ## [1] &quot;character&quot; typeof(my_flag) ## [1] &quot;logical&quot; typeof(as.integer(n)) ## [1] &quot;integer&quot; typeof(as.character(n)) ## [1] &quot;character&quot; A.4.2 Basic operations n + x ## [1] 22.7 n - x ## [1] 17.3 diff &lt;- n - x # variable diff gets the difference between n and x diff ## [1] 17.3 n * x ## [1] 54 n / x ## [1] 7.407407 x^2 ## [1] 7.29 sqrt(x) ## [1] 1.643168 n &gt; 2 * n ## [1] FALSE n == n ## [1] TRUE n == 2 * n ## [1] FALSE n != n ## [1] FALSE paste(student_name, &quot;is&quot;, n, &quot;years old&quot;) ## [1] &quot;Luka is 20 years old&quot; A.4.3 Vectors use c() to combine elements into vectors can only contain one type of variable if different types are provided, all are transformed to the most basic type in the vector access elements by indexes or logical vectors of the same length a scalar value is regarded as a vector of length 1 1:4 # creates a vector of integers from 1 to 4 ## [1] 1 2 3 4 student_ages &lt;- c(20, 23, 21) student_names &lt;- c(&quot;Luke&quot;, &quot;Jen&quot;, &quot;Mike&quot;) passed &lt;- c(TRUE, TRUE, FALSE) length(student_ages) ## [1] 3 # access by index student_ages[2] ## [1] 23 student_ages[1:2] ## [1] 20 23 student_ages[2] &lt;- 24 # change values # access by logical vectors student_ages[passed == TRUE] # same as student_ages[passed] ## [1] 20 24 student_ages[student_names %in% c(&quot;Luke&quot;, &quot;Mike&quot;)] ## [1] 20 21 student_names[student_ages &gt; 20] ## [1] &quot;Jen&quot; &quot;Mike&quot; A.4.3.1 Operations with vectors most operations are element-wise if we operate on vectors of different lengths, the shorter vector periodically repeats its elements until it reaches the length of the longer one a &lt;- c(1, 3, 5) b &lt;- c(2, 2, 1) d &lt;- c(6, 7) a + b ## [1] 3 5 6 a * b ## [1] 2 6 5 a + d ## Warning in a + d: longer object length is not a multiple of shorter object ## length ## [1] 7 10 11 a + 2 * b ## [1] 5 7 7 a &gt; b ## [1] FALSE TRUE TRUE b == a ## [1] FALSE FALSE FALSE a %*% b # vector multiplication, not element-wise ## [,1] ## [1,] 13 A.4.4 Factors vectors of finite predetermined classes suitable for categorical variables ordinal (ordered) or nominal (unordered) car_brand &lt;- factor(c(&quot;Audi&quot;, &quot;BMW&quot;, &quot;Mercedes&quot;, &quot;BMW&quot;), ordered = FALSE) car_brand ## [1] Audi BMW Mercedes BMW ## Levels: Audi BMW Mercedes freq &lt;- factor(x = NA, levels = c(&quot;never&quot;,&quot;rarely&quot;,&quot;sometimes&quot;,&quot;often&quot;,&quot;always&quot;), ordered = TRUE) freq[1:3] &lt;- c(&quot;rarely&quot;, &quot;sometimes&quot;, &quot;rarely&quot;) freq ## [1] rarely sometimes rarely ## Levels: never &lt; rarely &lt; sometimes &lt; often &lt; always freq[4] &lt;- &quot;quite_often&quot; # non-existing level, returns NA ## Warning in `[&lt;-.factor`(`*tmp*`, 4, value = &quot;quite_often&quot;): invalid factor ## level, NA generated freq ## [1] rarely sometimes rarely &lt;NA&gt; ## Levels: never &lt; rarely &lt; sometimes &lt; often &lt; always A.4.5 Matrices two-dimensional generalizations of vectors my_matrix &lt;- matrix(c(1, 2, 1, 5, 4, 2), nrow = 2, byrow = TRUE) my_matrix ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 5 4 2 my_square_matrix &lt;- matrix(c(1, 3, 2, 3), nrow = 2) my_square_matrix ## [,1] [,2] ## [1,] 1 2 ## [2,] 3 3 my_matrix[1,2] # first row, second column ## [1] 2 my_matrix[2, ] # second row ## [1] 5 4 2 my_matrix[ ,3] # third column ## [1] 1 2 A.4.5.1 Matrix functions and operations most operation element-wise mind the dimensions when using matrix multiplication %*% nrow(my_matrix) # number of matrix rows ## [1] 2 ncol(my_matrix) # number of matrix columns ## [1] 3 dim(my_matrix) # matrix dimension ## [1] 2 3 t(my_matrix) # transpose ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 4 ## [3,] 1 2 diag(my_matrix) # the diagonal of the matrix as vector ## [1] 1 4 diag(1, nrow = 3) # creates a diagonal matrix ## [,1] [,2] [,3] ## [1,] 1 0 0 ## [2,] 0 1 0 ## [3,] 0 0 1 det(my_square_matrix) # matrix determinant ## [1] -3 my_matrix + 2 * my_matrix ## [,1] [,2] [,3] ## [1,] 3 6 3 ## [2,] 15 12 6 my_matrix * my_matrix # element-wise multiplication ## [,1] [,2] [,3] ## [1,] 1 4 1 ## [2,] 25 16 4 my_matrix %*% t(my_matrix) # matrix multiplication ## [,1] [,2] ## [1,] 6 15 ## [2,] 15 45 my_vec &lt;- as.vector(my_matrix) # transform to vector my_vec ## [1] 1 5 2 4 1 2 A.4.6 Arrays multi-dimensional generalizations of matrices my_array &lt;- array(c(1, 2, 3, 4, 5, 6, 7, 8), dim = c(2, 2, 2)) my_array[1, 1, 1] ## [1] 1 my_array[2, 2, 1] ## [1] 4 my_array[1, , ] ## [,1] [,2] ## [1,] 1 5 ## [2,] 3 7 dim(my_array) ## [1] 2 2 2 A.4.7 Data frames basic data structure for analysis differ from matrices as columns can be of different types student_data &lt;- data.frame(&quot;Name&quot; = student_names, &quot;Age&quot; = student_ages, &quot;Pass&quot; = passed) student_data ## Name Age Pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE ## 3 Mike 21 FALSE colnames(student_data) &lt;- c(&quot;name&quot;, &quot;age&quot;, &quot;pass&quot;) # change column names student_data[1, ] ## name age pass ## 1 Luke 20 TRUE student_data[ ,colnames(student_data) %in% c(&quot;name&quot;, &quot;pass&quot;)] ## name pass ## 1 Luke TRUE ## 2 Jen TRUE ## 3 Mike FALSE student_data$pass # access column by name ## [1] TRUE TRUE FALSE student_data[student_data$pass == TRUE, ] ## name age pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE A.4.8 Lists useful for storing different data structures access elements with double square brackets elements can be named first_list &lt;- list(student_ages, my_matrix, student_data) second_list &lt;- list(student_ages, my_matrix, student_data, first_list) first_list[[1]] ## [1] 20 24 21 second_list[[4]] ## [[1]] ## [1] 20 24 21 ## ## [[2]] ## [,1] [,2] [,3] ## [1,] 1 2 1 ## [2,] 5 4 2 ## ## [[3]] ## name age pass ## 1 Luke 20 TRUE ## 2 Jen 24 TRUE ## 3 Mike 21 FALSE second_list[[4]][[1]] # first element of the fourth element of second_list ## [1] 20 24 21 length(second_list) ## [1] 4 second_list[[length(second_list) + 1]] &lt;- &quot;add_me&quot; # append an element names(first_list) &lt;- c(&quot;Age&quot;, &quot;Matrix&quot;, &quot;Data&quot;) first_list$Age ## [1] 20 24 21 A.4.9 Loops mostly for loop for loop can iterate over an arbitrary vector # iterate over consecutive natural numbers my_sum &lt;- 0 for (i in 1:10) { my_sum &lt;- my_sum + i } my_sum ## [1] 55 # iterate over an arbirary vector my_sum &lt;- 0 some_numbers &lt;- c(2, 3.5, 6, 100) for (i in some_numbers) { my_sum &lt;- my_sum + i } my_sum ## [1] 111.5 A.5 Functions for help use ?function_name A.5.1 Writing functions We can write our own functions with function(). In the brackets, we define the parameters the function gets, and in curly brackets we define what the function does. We use return() to return values. sum_first_n_elements &lt;- function (n) { my_sum &lt;- 0 for (i in 1:n) { my_sum &lt;- my_sum + i } return (my_sum) } sum_first_n_elements(10) ## [1] 55 A.6 Other tips Use set.seed(arbitrary_number) at the beginning of a script to set the seed and ensure replication. To dynamically set the working directory in R Studio to the parent folder of a R script use setwd(dirname(rstudioapi::getSourceEditorContext()$path)). To avoid slow R loops use the apply family of functions. See ?apply and ?lapply. To make your data manipulation (and therefore your life) a whole lot easier, use the dplyr package. Use getAnywhere(function_name) to get the source code of any function. Use browser for debugging. See ?browser. A.7 Further reading and references Getting started with R Studio: https://www.youtube.com/watch?v=lVKMsaWju8w Official R manuals: https://cran.r-project.org/manuals.html Cheatsheets: https://www.rstudio.com/resources/cheatsheets/ Workshop on R, dplyr, ggplot2, and R Markdown: https://github.com/bstatcomp/Rworkshop "],
["probability-distributions.html", "B Probability distributions", " B Probability distributions Name parameters pdf/pmf cdf mean variance Bernoulli \\(p\\) \\(p^k (1 - p)^{1 - k}\\) 1.11 binomial \\(n\\), \\(p\\) \\(\\binom{n}{k} p^k (1 - p)^{n - k}\\) 4.4 Poisson \\(\\lambda\\) \\(\\frac{\\lambda^k e^{-\\lambda}}{k!}\\) 4.6 geometric \\(p\\) \\(p(1-p)^k\\) 4.5 \\(1 - (1-p)^{k + 1}\\) 4.5 normal \\(\\mu\\), \\(\\sigma\\) \\(\\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(x - \\mu)^2}{2 \\sigma^2}}\\) 4.12 \\(\\int_{-\\infty}^x \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} e^{\\frac{(t - \\mu)^2}{2 \\sigma^2}} dt\\) 4.12 beta \\(\\alpha\\), \\(\\beta\\) \\(\\frac{x^{\\alpha - 1} (1 - x)^{\\beta - 1}}{\\text{B}(\\alpha, \\beta)}\\) 4.10 gamma \\(\\alpha\\), \\(\\beta\\) \\(\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)} x^{\\alpha - 1}e^{-\\beta x}\\) 4.11 \\(\\frac{\\gamma(\\alpha, \\beta x)}{\\Gamma(\\alpha)}\\) 4.11 exponential \\(\\lambda\\) \\(\\lambda e^{-\\lambda x}\\) 4.8 \\(1 - e^{-\\lambda x}\\) 4.8 logistic \\(\\mu\\), \\(s\\) \\(\\frac{e^{-\\frac{x - \\mu}{s}}}{(1 + e{-\\frac{x - \\mu}{s}})^2}\\) 4.13 \\(\\frac{1}{1 + e^{-\\frac{x - \\mu}{s}}}\\) 4.13 negative binomial \\(r\\), \\(p\\) \\(\\binom{k + r - 1}{k}(1-p)^r p^k\\) 4.7 "],
["references.html", "References", " References "]
]
