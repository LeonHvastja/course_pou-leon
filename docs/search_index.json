[
["ard.html", "Chapter 9 Multivariate random variables 9.1 Probability generating functions (PGFs) 9.2 Moment generating functions (MGFs)", " Chapter 9 Multivariate random variables This chapter deals with multivariate random variables. The students are expected to acquire the following knowledge: Theoretical Multivariate normal distribution. R Sampling from the multivariate normal distribution. 9.1 Probability generating functions (PGFs) Exercise 9.1 Show that the sum of independent Poisson random variables is itself a Poisson random variable. R: Let \\(X\\) be a sum of three Poisson distributions with \\(\\lambda_i \\in {2, 5.2, 10}\\). Take 1000 samples and plot the three distributions and the sum. Then take 1000 samples from the theoretical distribution of \\(X\\) and compare them to the sum. Solution. Let \\(X_i \\sim \\text{Poisson}(\\lambda_i)\\) for \\(i = 1,...,n\\), and let \\(X = \\sum_{i=1}^n X_i\\). \\[\\begin{align} \\alpha_X(t) &amp;= \\prod_{i=1}^n \\alpha_{X_i}(t) \\\\ &amp;= \\prod_{i=1}^n \\bigg( \\sum_{j=0}^\\infty t^j \\frac{\\lambda_i^j e^{-\\lambda_i}}{j!} \\bigg) \\\\ &amp;= \\prod_{i=1}^n \\bigg( e^{-\\lambda_i} \\sum_{j=0}^\\infty \\frac{(t\\lambda_i)^j }{j!} \\bigg) \\\\ &amp;= \\prod_{i=1}^n \\bigg( e^{-\\lambda_i} e^{t \\lambda_i} \\bigg) &amp; \\text{power series} \\\\ &amp;= \\prod_{i=1}^n \\bigg( e^{\\lambda_i(t - 1)} \\bigg) \\\\ &amp;= e^{\\sum_{i=1}^n \\lambda_i(t - 1)} \\\\ &amp;= e^{t \\sum_{i=1}^n \\lambda_i - \\sum_{i=1}^n \\lambda_i} \\\\ &amp;= e^{-\\sum_{i=1}^n \\lambda_i} \\sum_{j=0}^\\infty \\frac{(t \\sum_{i=1}^n \\lambda_i)^j}{j!}\\\\ &amp;= \\sum_{j=0}^\\infty \\frac{e^{-\\sum_{i=1}^n \\lambda_i} (t \\sum_{i=1}^n \\lambda_i)^j}{j!}\\\\ \\end{align}\\] The last term is the PGF of a Poisson random variable with parameter \\(\\sum_{i=1}^n \\lambda_i\\). Because the PGF is unique, \\(X\\) is a Poisson random variable. set.seed(1) library(tidyr) nsamps &lt;- 1000 samps &lt;- matrix(data = NA, nrow = nsamps, ncol = 4) samps[ ,1] &lt;- rpois(nsamps, 2) samps[ ,2] &lt;- rpois(nsamps, 5.2) samps[ ,3] &lt;- rpois(nsamps, 10) samps[ ,4] &lt;- samps[ ,1] + samps[ ,2] + samps[ ,3] colnames(samps) &lt;- c(2, 2.5, 10, &quot;sum&quot;) gsamps &lt;- as_tibble(samps) gsamps &lt;- gather(gsamps, key = &quot;dist&quot;, value = &quot;value&quot;) ggplot(gsamps, aes(x = value)) + geom_bar() + facet_wrap(~ dist) samps &lt;- cbind(samps, &quot;theoretical&quot; = rpois(nsamps, 2 + 5.2 + 10)) gsamps &lt;- as_tibble(samps[ ,4:5]) gsamps &lt;- gather(gsamps, key = &quot;dist&quot;, value = &quot;value&quot;) ggplot(gsamps, aes(x = value, fill = dist)) + geom_bar(position = &quot;dodge&quot;) Exercise 9.2 Find the expected value and variance of the negative binomial distribution. Hint: Find the Taylor series of \\((1 - y)^{-r}\\) at point 0. Solution. Let \\(X \\sim \\text{NB}(r, p)\\). \\[\\begin{align} \\alpha_X(t) &amp;= E[t^X] \\\\ &amp;= \\sum_{j=0}^\\infty t^j \\binom{j + r - 1}{j} (1 - p)^r p^j \\\\ &amp;= (1 - p)^r \\sum_{j=0}^\\infty \\binom{j + r - 1}{j} (tp)^j \\\\ &amp;= (1 - p)^r \\sum_{j=0}^\\infty \\frac{(j + r - 1)(j + r - 2)...r}{j!} (tp)^j. \\\\ \\end{align}\\] Let us look at the Taylor series of \\((1 - y)^{-r}\\) at 0 \\[\\begin{align} (1 - y)^{-r} = &amp;1 + \\frac{-r(-1)}{1!}y + \\frac{-r(-r - 1)(-1)^2}{2!}y^2 + \\\\ &amp;\\frac{-r(-r - 1)(-r - 2)(-1)^3}{3!}y^3 + ... \\\\ \\end{align}\\] How does the \\(k\\)-th term look like? We have \\(k\\) derivatives of our function so \\[\\begin{align} \\frac{d^k}{d^k y} (1 - y)^{-r} &amp;= \\frac{-r(-r - 1)...(-r - k + 1)(-1)^k}{k!}y^k \\\\ &amp;= \\frac{r(r + 1)...(r + k - 1)}{k!}y^k. \\end{align}\\] We observe that this equals to the \\(j\\)-th term in the sum of NB PGF. Therefore \\[\\begin{align} \\alpha_X(t) &amp;= (1 - p)^r (1 - tp)^{-r} \\\\ &amp;= \\Big(\\frac{1 - p}{1 - tp}\\Big)^r \\end{align}\\] To find the expected value, we need to differentiate \\[\\begin{align} \\frac{d}{dt} \\Big(\\frac{1 - p}{1 - tp}\\Big)^r &amp;= r \\Big(\\frac{1 - p}{1 - tp}\\Big)^{r-1} \\frac{d}{dt} \\frac{1 - p}{1 - tp} \\\\ &amp;= r \\Big(\\frac{1 - p}{1 - tp}\\Big)^{r-1} \\frac{p(1 - p)}{(1 - tp)^2}. \\\\ \\end{align}\\] Evaluating this at 1, we get: \\[\\begin{align} E[X] = \\frac{rp}{1 - p}. \\end{align}\\] For the variance we need the second derivative. \\[\\begin{align} \\frac{d^2}{d^2t} \\Big(\\frac{1 - p}{1 - tp}\\Big)^r &amp;= \\frac{p^2 r (r + 1) (\\frac{1 - p}{1 - tp})^r}{(tp - 1)^2} \\end{align}\\] Evaluating this at 1 and inserting the first derivatives, we get: \\[\\begin{align} Var[X] &amp;= \\frac{d^2}{dt^2} \\alpha_X(1) + \\frac{d}{dt}\\alpha_X(1) - \\Big(\\frac{d}{dt}\\alpha_X(t) \\Big)^2 \\\\ &amp;= \\frac{p^2 r (r + 1)}{(1 - p)^2} + \\frac{rp}{1 - p} - \\frac{r^2p^2}{(1 - p)^2} \\\\ &amp;= \\frac{rp}{(1 - p)^2}. \\end{align}\\] library(tidyr) set.seed(1) nsamps &lt;- 100000 find_p &lt;- function (mu, r) { return (10 / (r + 10)) } r &lt;- c(1,2,10,20) p &lt;- find_p(10, r) sigma &lt;- rep(sqrt(p*r / (1 - p)^2), each = nsamps) samps &lt;- cbind(&quot;r=1&quot; = rnbinom(nsamps, size = r[1], prob = 1 - p[1]), &quot;r=2&quot; = rnbinom(nsamps, size = r[2], prob = 1 - p[2]), &quot;r=4&quot; = rnbinom(nsamps, size = r[3], prob = 1 - p[3]), &quot;r=20&quot; = rnbinom(nsamps, size = r[4], prob = 1 - p[4])) gsamps &lt;- gather(as.data.frame(samps)) iw &lt;- (gsamps$value &gt; sigma + 10) | (gsamps$value &lt; sigma - 10) ggplot(gsamps, aes(x = value, fill = iw)) + geom_bar() + # geom_density() + facet_wrap(~ key) 9.2 Moment generating functions (MGFs) "]
]
