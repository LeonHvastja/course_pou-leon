[
["nhst.html", "Chapter 16 Null hypothesis significance testing", " Chapter 16 Null hypothesis significance testing This chapter deals with null hypothesis significance testing. The students are expected to acquire the following knowledge: Binomial test. t-test. Chi-squared test. .fold-btn { float: right; margin: 5px 5px 0 0; } .fold { border: 1px solid black; min-height: 40px; } Exercise 16.1 (Long-run guarantees of the t-test) Generate a sample of size \\(n = 10\\) from the standard normal. Use the two-sided t-test with \\(H0: \\mu = 0\\) and record the p-value. Can you reject H0 at 0.05 significance level? (before simulating) If we repeated (b) many times, what would be the relative frequency of false positives/Type I errors (rejecting the null that is true)? What would be the relative frequency of false negatives /Type II errors (retaining the null when the null is false)? (now simulate b and check if the simulation results match your answer in b) Similar to (a-c) but now we generate data from N(-0.5, 1). Similar to (a-c) but now we generate data from N(\\(\\mu\\), 1) where we every time pick a different \\(\\mu &lt; 0\\) and use a one-sided test \\(H0: \\mu &lt;= 0\\). n &lt;- 10 Exercise 16.2 (Chi-squared test) Show that the \\(\\chi^2 = \\sum_{i=1}^k \\frac{(O_i - E_i)^2}{E_i}\\) test statistic is approximately \\(\\chi^2\\) distributed when we have two categories. Compare the number of voters who voted for Trump or Hillary depending on their income (less or more than 100.000 dollars per year). Manually calculate the chi-squared statistic, compare to the chisq.test in R, and discuss the results. Visualize the test. Solution. Let \\(X_i\\) be binary variables, \\(i = 1,...,n\\). We can then express the test statistic as \\[\\begin{align} \\chi^2 = &amp;\\frac{(O_i - np)^2}{np} + \\frac{(n - O_i - n(1 - p))^2}{n(1 - p)} \\\\ &amp;= \\frac{(O_i - np)^2}{np(1 - p)} \\\\ &amp;= (\\frac{O_i - np}{\\sqrt{np(1 - p)}})^2. \\end{align}\\] When \\(n\\) is large, this distrbution is approximately normal with \\(\\mu = np\\) and \\(\\sigma^2 = np(1 - p)\\) (binomial converges in distribution to standard normal). By definition, the chi-squared distribution with \\(k\\) degrees of freedom is a sum of squares of \\(k\\) independent standard normal random variables. n &lt;- 24588 less100 &lt;- round(0.66 * n * c(0.49, 0.45, 0.06)) # some rounding, but it should not affect results more100 &lt;- round(0.34 * n * c(0.47, 0.47, 0.06)) x &lt;- rbind(less100, more100) colnames(x) &lt;- c(&quot;Clinton&quot;, &quot;Trump&quot;, &quot;other/no answer&quot;) print(x) ## Clinton Trump other/no answer ## less100 7952 7303 974 ## more100 3929 3929 502 chisq.test(x) ## ## Pearson&#39;s Chi-squared test ## ## data: x ## X-squared = 9.3945, df = 2, p-value = 0.00912 x ## Clinton Trump other/no answer ## less100 7952 7303 974 ## more100 3929 3929 502 csum &lt;- apply(x, 2, sum) rsum &lt;- apply(x, 1, sum) chi2 &lt;- (x[1,1] - csum[1] * rsum[1] / sum(x))^2 / (csum[1] * rsum[1] / sum(x)) + (x[1,2] - csum[2] * rsum[1] / sum(x))^2 / (csum[2] * rsum[1] / sum(x)) + (x[1,3] - csum[3] * rsum[1] / sum(x))^2 / (csum[3] * rsum[1] / sum(x)) + (x[2,1] - csum[1] * rsum[2] / sum(x))^2 / (csum[1] * rsum[2] / sum(x)) + (x[2,2] - csum[2] * rsum[2] / sum(x))^2 / (csum[2] * rsum[2] / sum(x)) + (x[2,3] - csum[3] * rsum[2] / sum(x))^2 / (csum[3] * rsum[2] / sum(x)) chi2 ## Clinton ## 9.394536 1 - pchisq(chi2, df = 2) ## Clinton ## 0.009120161 x &lt;- seq(0, 15, by = 0.01) df &lt;- data.frame(x = x) ggplot(data = df, aes(x = x)) + stat_function(fun = dchisq, args = list(df = 2)) + geom_segment(aes(x = chi2, y = 0, xend = chi2, yend = dchisq(chi2, df = 2))) + stat_function(fun = dchisq, args = list(df = 2), xlim = c(chi2, 15), geom = &quot;area&quot;, fill = &quot;red&quot;) "]
]
